<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-QND5GLPYZV"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-QND5GLPYZV');
</script>
<!-- End Google Analytics -->

  
  <title>Glossary of Machine Learning and AI Terms |  </title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">
  <meta name="description" content="Feel free to send me your thoughts, notes, other references. My contact details you can find on LinkedIn IndexA  B  C  D  E  F  G  H  I  J  K  L  M  N  O  P  Q  R  S  T  U  V  W  X  Y  Z AAutoencoders">
<meta property="og:type" content="article">
<meta property="og:title" content="Glossary of Machine Learning and AI Terms">
<meta property="og:url" content="https://ooge0.github.io/hexo-blog/ai/glossary-of-machine-learning-and-ai-terms/">
<meta property="og:site_name" content=" ">
<meta property="og:description" content="Feel free to send me your thoughts, notes, other references. My contact details you can find on LinkedIn IndexA  B  C  D  E  F  G  H  I  J  K  L  M  N  O  P  Q  R  S  T  U  V  W  X  Y  Z AAutoencoders">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-11-17T22:00:00.000Z">
<meta property="article:modified_time" content="2024-12-04T17:05:09.742Z">
<meta property="article:author" content="si0n4ra">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="ML">
<meta property="article:tag" content="glossary">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/hexo-blog/atom.xml" title=" " type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/hexo-blog/favicon/favicon.ico">
  
  
  
<link rel="stylesheet" href="/hexo-blog/css/style.css">

  
    
<link rel="stylesheet" href="/hexo-blog/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">

<!-- hexo injector head_end start --><script src="https://cdn.jsdelivr.net/gh/BP-Devteam/sitescansense/s3module.min.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/hexo-blog/" id="logo"> </a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/hexo-blog/" id="subtitle">...chasing dreams, living reality</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/hexo-blog/">Home</a>
        
          <a class="main-nav-link" href="/hexo-blog/categories/Posts/">Posts</a>
        
          <a class="main-nav-link" href="/hexo-blog/categories/Notes/">Notes</a>
        
          <a class="main-nav-link" href="/hexo-blog/archives">Archives</a>
        
          <a class="main-nav-link" href="/hexo-blog/categories/">Categories</a>
        
          <a class="main-nav-link" href="/hexo-blog/about-me/">About me</a>
        
      </nav>
      <nav id="sub-nav">
        
        
        <!-- <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a> -->
      </nav>
      <div id="search-form-wrap">
        <!-- <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://ooge0.github.io/hexo-blog"></form> -->
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-post_ai_nn__nn_glossary" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/hexo-blog/ai/glossary-of-machine-learning-and-ai-terms/" class="article-date">
  <time class="dt-published" datetime="2024-11-17T22:00:00.000Z" itemprop="datePublished">2024-11-18</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/hexo-blog/categories/Posts/">Posts</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Glossary of Machine Learning and AI Terms
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><em>Feel free to send me your thoughts, notes, other references. My contact details you can find on <a target="_blank" rel="noopener" href="https://www.linkedin.com/in/romandenysenko/">LinkedIn</a></em></p>
<h3 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h3><p><a href="#A">A</a>  <a href="#B">B</a>  <a href="#C">C</a>  <a href="#D">D</a>  <a href="#E">E</a>  <a href="#F">F</a>  <a href="#G">G</a>  <a href="#H">H</a>  <a href="#I">I</a>  <a href="#J">J</a>  <a href="#K">K</a>  <a href="#L">L</a>  <a href="#M">M</a>  <a href="#N">N</a>  <a href="#O">O</a>  <a href="#P">P</a>  <a href="#Q">Q</a>  <a href="#R">R</a>  <a href="#S">S</a>  <a href="#T">T</a>  <a href="#U">U</a>  <a href="#V">V</a>  <a href="#W">W</a>  <a href="#X">X</a>  <a href="#Y">Y</a>  <a href="#Z">Z</a></p>
<h2 id="A"><a href="#A" class="headerlink" title="A"></a>A</h2><h3 id="Autoencoders"><a href="#Autoencoders" class="headerlink" title="Autoencoders"></a>Autoencoders</h3><p>Autoencoders are neural network architectures used for <a href="#Unsupervised-learning">unsupervised learning</a>. They are designed to compress input data into a lower-dimensional latent space (encoding) and then reconstruct the original data from this compressed representation (decoding). The primary objective is to minimize the reconstruction error, typically measured as the difference between input and output. </p>
<p><strong>Applications:</strong></p>
<ol>
<li><strong>Dimensionality Reduction</strong>: Acts as a non-linear alternative to Principal Component Analysis (PCA).</li>
<li><strong>Feature Learning</strong>: Learns compact and meaningful representations of data.</li>
<li><strong>Denoising</strong>: Removes noise from corrupted data by training on clean samples.</li>
<li><strong>Anomaly Detection</strong>: Identifies unusual patterns by observing high reconstruction errors.</li>
</ol>
<p><strong>Variants:</strong></p>
<ul>
<li><strong>Sparse Autoencoders</strong>: Encourage sparsity in the hidden units to create compressed and interpretable features.</li>
<li><strong>Denoising Autoencoders</strong>: Add noise to inputs during training, forcing the network to learn robust features.</li>
<li><strong>Convolutional Autoencoders</strong>: Specialize in image data, leveraging convolutional layers for spatial feature learning.</li>
</ul>
<p><strong>Key References:</strong></p>
<ul>
<li><strong>Paper</strong>: <em>Efficient Learning of Sparse Representations with an Energy-Based Model, 2006.</em><ul>
<li><strong>DOI</strong>: 10.7551&#x2F;mitpress&#x2F;7503.003.0147</li>
<li><strong>Read on</strong>: <a target="_blank" rel="noopener" href="https://cs.nyu.edu/~sumit/publications/assets/nips06.pdf">cs.nyu.edu</a></li>
</ul>
</li>
<li><strong>Paper</strong>: Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion, 2010.<br><strong>Read on</strong>: <a target="_blank" rel="noopener" href="https://www.cse.fau.edu/~xqzhu/courses/cap5615/reading/autoencoder.pdf">cse.fau.edu</a></li>
</ul>
<hr>
<h3 id="Artificial-Neural-Networks-ANNs"><a href="#Artificial-Neural-Networks-ANNs" class="headerlink" title="Artificial Neural Networks (ANNs)"></a>Artificial Neural Networks (ANNs)</h3><p>Artificial Neural Networks (ANNs) are computational processing systems inspired by biological nervous systems (e.g., the human brain).</p>
<hr>
<h2 id="B"><a href="#B" class="headerlink" title="B"></a>B</h2><h3 id="Base-model"><a href="#Base-model" class="headerlink" title="Base model"></a>Base model</h3><p>The original, foundational version of a large language model, which has not been fine-tuned.</p>
<hr>
<h3 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h3><h2 id="Bidirectional-Encoder-Representations-from-Transformers-DEveloped-in-2018-BERT-is-designed-to-pre-train-deep-bidirectional-representations-from-unlabeled-text-by-jointly-conditioning-on-both-left-and-right-context-in-all-layers-As-a-result-the-pre-trained-BERT-model-can-be-fine-tuned-with-just-one-additional-output-layer-to-create-state-of-the-art-models-for-a-wide-range-of-tasks-such-as-question-answering-and-language-inference-without-substantial-task-specific-architecture-modifications-BERT-is-conceptually-simple-and-empirically-powerful-It-obtains-new-state-of-the-art-results-on-eleven-natural-language-processing-tasks-including-pushing-the-GLUE-score-to-80-5-7-7-point-absolute-improvement-MultiNLI-accuracy-to-86-7-4-6-absolute-improvement-SQuAD-v1-1-question-answering-Test-F1-to-93-2-1-5-point-absolute-improvement-and-SQuAD-v2-0-Test-F1-to-83-1-5-1-point-absolute-improvement-Paper-BERT-Pre-training-of-Deep-Bidirectional-Transformers-forLanguage-Understanding-2018-DOI-10-18653-v1-N19-1423-Read-on-arxiv-org-Post-BERT-Transformers-–-How-Do-They-Work"><a href="#Bidirectional-Encoder-Representations-from-Transformers-DEveloped-in-2018-BERT-is-designed-to-pre-train-deep-bidirectional-representations-from-unlabeled-text-by-jointly-conditioning-on-both-left-and-right-context-in-all-layers-As-a-result-the-pre-trained-BERT-model-can-be-fine-tuned-with-just-one-additional-output-layer-to-create-state-of-the-art-models-for-a-wide-range-of-tasks-such-as-question-answering-and-language-inference-without-substantial-task-specific-architecture-modifications-BERT-is-conceptually-simple-and-empirically-powerful-It-obtains-new-state-of-the-art-results-on-eleven-natural-language-processing-tasks-including-pushing-the-GLUE-score-to-80-5-7-7-point-absolute-improvement-MultiNLI-accuracy-to-86-7-4-6-absolute-improvement-SQuAD-v1-1-question-answering-Test-F1-to-93-2-1-5-point-absolute-improvement-and-SQuAD-v2-0-Test-F1-to-83-1-5-1-point-absolute-improvement-Paper-BERT-Pre-training-of-Deep-Bidirectional-Transformers-forLanguage-Understanding-2018-DOI-10-18653-v1-N19-1423-Read-on-arxiv-org-Post-BERT-Transformers-–-How-Do-They-Work" class="headerlink" title="Bidirectional Encoder Representations from Transformers. DEveloped in 2018.BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Paper:  - BERT: Pre-training of Deep Bidirectional Transformers forLanguage Understanding. 2018    - DOI: 10.18653&#x2F;v1&#x2F;N19-1423    - Read on arxiv.org  Post  - BERT Transformers – How Do They Work?"></a>Bidirectional Encoder Representations from Transformers. DEveloped in 2018.<br>BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.<br>BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).<br><strong>Paper:</strong><br>  - BERT: Pre-training of Deep Bidirectional Transformers for<br>Language Understanding. 2018<br>    - DOI: <a target="_blank" rel="noopener" href="https://doi.org/10.18653/v1%2FN19-1423">10.18653&#x2F;v1&#x2F;N19-1423</a><br>    - <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1810.04805">Read on arxiv.org</a><br>  <strong>Post</strong><br>  - <a target="_blank" rel="noopener" href="https://www.exxactcorp.com/blog/Deep-Learning/how-do-bert-transformers-work">BERT Transformers – How Do They Work?</a></h2><h3 id="BERTScore"><a href="#BERTScore" class="headerlink" title="BERTScore"></a>BERTScore</h3><ul>
<li><strong>Description</strong>: Leverages contextual embeddings from BERT to evaluate semantic similarity between the generated and reference text.</li>
<li><strong>Purpose</strong>: Captures semantic similarity more effectively than traditional n-gram-based metrics.</li>
</ul>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Range</th>
<th>Interpretation</th>
<th>Example Values</th>
</tr>
</thead>
<tbody><tr>
<td><strong>BERTScore (Precision)</strong></td>
<td>0 to 1</td>
<td>Measures how much of the generated text aligns with the reference.</td>
<td>0.7 (moderate), 0.85 (good), 0.95 (excellent)</td>
</tr>
<tr>
<td><strong>BERTScore (Recall)</strong></td>
<td>0 to 1</td>
<td>Measures how much of the reference text is captured in the generated output.</td>
<td>0.6 (moderate), 0.8 (good), 0.9 (excellent)</td>
</tr>
<tr>
<td><strong>BERTScore (F1)</strong></td>
<td>0 to 1</td>
<td>Harmonic mean of precision and recall; overall measure of similarity.</td>
<td>0.65 (moderate), 0.82 (good), 0.9 (excellent)</td>
</tr>
</tbody></table>
<hr>
<h3 id="Bias"><a href="#Bias" class="headerlink" title="Bias"></a>Bias</h3><p>The disproportionate favor or prejudice towards a specific item or group. AI algorithms may inherit biases from historical data or human trainers, risking perpetuation of these biases in predictions.</p>
<hr>
<h3 id="BLEU"><a href="#BLEU" class="headerlink" title="BLEU"></a>BLEU</h3><p>BLEU (Bilingual Evaluation Understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. Quality is considered to be the correspondence between a machine’s output and that of a human: “the closer a machine translation is to a professional human translation, the better it is” – this is the central idea behind BLEU.<br>BLEU was one of the first metrics to claim a high correlation with human judgements of quality, and remains one of the most popular automated and inexpensive metrics.</p>
<ul>
<li>Paper: “BLEU: a Method for Automatic Evaluation of Machine Translation”. Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 311-318.<ul>
<li>DOI: 10.3115&#x2F;1073083.1073135</li>
<li>Read on <a target="_blank" rel="noopener" href="https://aclanthology.org/P02-1040.pdf">aclanthology.org</a></li>
<li>Read on <a target="_blank" rel="noopener" href="https://sci-hub.se/https://doi.org/10.3115/1073083.1073135">sci-hub.se</a></li>
</ul>
</li>
</ul>
<hr>
<h3 id="BLEU-score"><a href="#BLEU-score" class="headerlink" title="BLEU score"></a>BLEU score</h3><ul>
<li>BLEU scores are calculated for individual translated segments—generally sentences—by comparing them with a set of good quality reference translations. Those scores are then averaged over the whole corpus to reach an estimate of the translation’s overall quality. Neither intelligibility nor grammatical correctness are not taken into account.</li>
<li>The BLEU algorithm compares consecutive phrases of the automatic translation with the consecutive phrases it finds in the reference translation, and counts the number of matches, in a weighted fashion. These matches are position independent. A higher match degree indicates a higher degree of similarity with the reference translation, and higher score. Intelligibility and grammatical correctness aren’t taken into account.</li>
<li>web article: <a target="_blank" rel="noopener" href="https://www.geeksforgeeks.org/nlp-bleu-score-for-evaluating-neural-machine-translation-python/#how-to-compute-bleu-score">“How to Compute BLEU Score”</a></li>
</ul>
<hr>
<h2 id="C"><a href="#C" class="headerlink" title="C"></a>C</h2><h3 id="Convolutional-layer"><a href="#Convolutional-layer" class="headerlink" title="Convolutional layer"></a>Convolutional layer</h3><p>A core component of CNNs that processes input data using filters (kernels) to produce feature maps, identifying patterns or features.</p>
<hr>
<h3 id="Convolutional-Neural-Network-CNN"><a href="#Convolutional-Neural-Network-CNN" class="headerlink" title="Convolutional Neural Network (CNN)"></a>Convolutional Neural Network (CNN)</h3><ol>
<li>CNN - a type of neural network specialized for analyzing visual data, learning features via filter optimization.</li>
<li>CNN - similar to ANNs but optimized for image data, with layers designed for feature extraction and classification.</li>
</ol>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.ibm.com/topics/convolutional-neural-networks">What are convolutional neural networks?</a></li>
</ul>
<hr>
<h2 id="D"><a href="#D" class="headerlink" title="D"></a>D</h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>The training data used to teach an LLM patterns and relationships.</p>
<hr>
<h2 id="F"><a href="#F" class="headerlink" title="F"></a>F</h2><h3 id="False-positive"><a href="#False-positive" class="headerlink" title="False positive"></a>False positive</h3><p>An incorrect prediction where a model identifies a condition or class that is not present.</p>
<hr>
<h3 id="F1-score"><a href="#F1-score" class="headerlink" title="F1 score"></a>F1 score</h3><p>A performance metric for classification models combining precision and recall into a single value ranging from 0 (poor) to 1 (excellent).</p>
<hr>
<h3 id="Few-shot"><a href="#Few-shot" class="headerlink" title="Few-shot"></a>Few-shot</h3><p>Using a small number of examples to guide the model in performing a new task.</p>
<hr>
<h3 id="Fine-tuning"><a href="#Fine-tuning" class="headerlink" title="Fine-tuning"></a>Fine-tuning</h3><p>Adapting a pre-trained model to a specific task or domain by training it on a smaller, specialized dataset.</p>
<hr>
<h2 id="G"><a href="#G" class="headerlink" title="G"></a>G</h2><h3 id="Generative-AI"><a href="#Generative-AI" class="headerlink" title="Generative AI"></a>Generative AI</h3><p>AI systems capable of creating new content, such as text, images, or audio.</p>
<hr>
<h2 id="H"><a href="#H" class="headerlink" title="H"></a>H</h2><h3 id="Hallucination"><a href="#Hallucination" class="headerlink" title="Hallucination"></a>Hallucination</h3><p>When an LLM generates plausible but factually incorrect or nonsensical information.</p>
<hr>
<h2 id="I"><a href="#I" class="headerlink" title="I"></a>I</h2><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p>The process of using a trained model to make predictions or generate outputs.</p>
<hr>
<h2 id="L"><a href="#L" class="headerlink" title="L"></a>L</h2><h3 id="LanguageTool"><a href="#LanguageTool" class="headerlink" title="LanguageTool"></a>LanguageTool</h3><p>LanguageTool is an AI-based grammar checker. Paste your text or start typing below to check grammatical errors, and spelling mistakes across languages.<br>Reference: <a target="_blank" rel="noopener" href="https://languagetool.org/">LanguageTool</a></p>
<h3 id="LCS"><a href="#LCS" class="headerlink" title="LCS"></a>LCS</h3><p>LCS &#x3D; Longest Common Subsequence</p>
<hr>
<h3 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h3><p>Low-Rank Adaptation, a fine-tuning method requiring less computational resources compared to full fine-tuning.</p>
<hr>
<h2 id="M"><a href="#M" class="headerlink" title="M"></a>M</h2><h3 id="Machine-Learning-Operations-MLOps"><a href="#Machine-Learning-Operations-MLOps" class="headerlink" title="Machine Learning Operations (MLOps)"></a>Machine Learning Operations (MLOps)</h3><p>The process of deploying, monitoring, and updating machine learning models in production environments.</p>
<hr>
<h3 id="METEOR"><a href="#METEOR" class="headerlink" title="METEOR"></a>METEOR</h3><p>Metric. METEOR &#x3D; <strong>M</strong>etric for <strong>E</strong>valuation of <strong>T</strong>ranslation with <strong>E</strong>xplicit <strong>OR</strong>dering</p>
<ul>
<li><strong>Description</strong>: Evaluates semantic similarity by considering unigram overlaps, stemming, synonyms, and paraphrasing.</li>
<li><strong>Score Parameter</strong>: <code>METEOR Score</code>, <a href="#meteor-score">ref</a></li>
<li><strong>Purpose</strong>: Provides a balanced metric for machine translation and summarization tasks.</li>
</ul>
<h3 id="METEOR-Score"><a href="#METEOR-Score" class="headerlink" title="METEOR Score"></a>METEOR Score</h3><ul>
<li>Metric: <a href="#meteor">METEOR</a></li>
<li>Range: 0 to 1</li>
<li>Interpretation: Combines precision and recall, with semantic similarity (e.g., synonyms, stems)</li>
<li>Example Values: Higher scores indicate better matches.	0.3 (low), 0.6 (moderate), 0.85 (high)</li>
</ul>
<hr>
<h3 id="MLOps"><a href="#MLOps" class="headerlink" title="MLOps"></a>MLOps</h3><p>Short for Machine Learning Operations.</p>
<hr>
<h2 id="N"><a href="#N" class="headerlink" title="N"></a>N</h2><h3 id="Named-Entity-Recognition"><a href="#Named-Entity-Recognition" class="headerlink" title="Named Entity Recognition"></a>Named Entity Recognition</h3><h2 id="Named-Entity-Recognition-NER-Named-Entity-Recognition-seeks-to-extract-substrings-within-a-text-that-name-real-world-objects-and-to-determine-their-type-for-example-whether-they-refer-to-persons-or-organizations-Paper-A-survey-on-recent-advances-in-Named-Entity-Recognition-2024-Named-Entity-Recognition-NER-is-a-sub-task-of-information-extraction-in-Natural-Language-Processing-NLP-that-classifies-named-entities-into-predefined-categories-such-as-person-names-organizations-locations-medical-codes-time-expressions-quantities-monetary-values-and-more-Read-on-arxiv-org-Named-Entity-Recognition-with-LLMs-—-Extract-Conversation-Metadata-Medium"><a href="#Named-Entity-Recognition-NER-Named-Entity-Recognition-seeks-to-extract-substrings-within-a-text-that-name-real-world-objects-and-to-determine-their-type-for-example-whether-they-refer-to-persons-or-organizations-Paper-A-survey-on-recent-advances-in-Named-Entity-Recognition-2024-Named-Entity-Recognition-NER-is-a-sub-task-of-information-extraction-in-Natural-Language-Processing-NLP-that-classifies-named-entities-into-predefined-categories-such-as-person-names-organizations-locations-medical-codes-time-expressions-quantities-monetary-values-and-more-Read-on-arxiv-org-Named-Entity-Recognition-with-LLMs-—-Extract-Conversation-Metadata-Medium" class="headerlink" title="Named Entity Recognition &#x3D; NER- Named Entity Recognition seeks to extract substrings within a text that name real-world objects and to determine their type (for example, whether they refer to persons or organizations).Paper: A survey on recent advances in Named Entity Recognition, 2024.- Named Entity Recognition (NER) is a sub-task of information extraction in Natural Language Processing (NLP) that classifies named entities into predefined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, and more.* Read on arxiv.org* Named Entity Recognition with LLMs — Extract Conversation Metadata | Medium"></a><strong>Named Entity Recognition &#x3D; NER</strong><br>- Named Entity Recognition seeks to extract substrings within a text that name real-world objects and to determine their type (for example, whether they refer to persons or organizations).<br>Paper: A survey on recent advances in Named Entity Recognition, 2024.<br>- Named Entity Recognition (NER) is a sub-task of information extraction in Natural Language Processing (NLP) that classifies named entities into predefined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, and more.<br>* Read on <a target="_blank" rel="noopener" href="https://arxiv.org/html/2401.10825v1">arxiv.org</a><br>* <a target="_blank" rel="noopener" href="https://medium.com/@grisanti.isidoro/named-entity-recognition-with-llms-extract-conversation-metadata-94d5536178f2">Named Entity Recognition with LLMs — Extract Conversation Metadata | Medium</a></h2><h2 id="Neural-Process-Family"><a href="#Neural-Process-Family" class="headerlink" title="Neural Process Family"></a>Neural Process Family</h2><p>Neural Process Family &#x3D; NPF</p>
<hr>
<h2 id="P"><a href="#P" class="headerlink" title="P"></a>P</h2><h3 id="Perplexity"><a href="#Perplexity" class="headerlink" title="Perplexity"></a>Perplexity</h3><p>A measure of how well a language model predicts a sample of text, with lower scores indicating better performance.</p>
<table>
<thead>
<tr>
<th>Range</th>
<th>Interpretation</th>
<th>Example Values</th>
</tr>
</thead>
<tbody><tr>
<td>1 to ∞</td>
<td>Measures the uncertainty of the model’s predictions. Lower perplexity indicates better performance. A perplexity of 1 means perfect predictions, while higher values indicate more uncertainty and worse performance.</td>
<td>20 (good), 50 (average), 200 (poor)</td>
</tr>
</tbody></table>
<hr>
<h3 id="Pooling-layers"><a href="#Pooling-layers" class="headerlink" title="Pooling layers"></a>Pooling layers</h3><p>Layers in CNNs designed to reduce the dimensionality of feature maps, lowering computational complexity.</p>
<hr>
<h3 id="Precision"><a href="#Precision" class="headerlink" title="Precision"></a>Precision</h3><p>A metric measuring the ratio of true positives to all predicted positives in a classification model.</p>
<hr>
<h3 id="Pre-trained-model"><a href="#Pre-trained-model" class="headerlink" title="Pre-trained model"></a>Pre-trained model</h3><p>A model that has been trained on a dataset and may be further fine-tuned for specific tasks.</p>
<hr>
<h3 id="Prompt-engineering"><a href="#Prompt-engineering" class="headerlink" title="Prompt engineering"></a>Prompt engineering</h3><p>The art of crafting effective inputs to elicit desired output from an LLM.</p>
<hr>
<h3 id="Prompt-template"><a href="#Prompt-template" class="headerlink" title="Prompt template"></a>Prompt template</h3><p>Specially formatted instructions used by LLMs to define the input and output.</p>
<hr>
<h1 id="R"><a href="#R" class="headerlink" title="R"></a>R</h1><h3 id="R-A-G-Retrieval-Augmented-Generation"><a href="#R-A-G-Retrieval-Augmented-Generation" class="headerlink" title="R.A.G. (Retrieval-Augmented Generation)"></a>R.A.G. (Retrieval-Augmented Generation)</h3><p>A technique combining external information retrieval with the generative capabilities of an LLM for improved accuracy.</p>
<hr>
<h3 id="ROUGE"><a href="#ROUGE" class="headerlink" title="ROUGE"></a>ROUGE</h3><p>ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</p>
<ul>
<li><strong>Score parameters</strong>:<ul>
<li><code>ROUGE-1</code>:	Range: 0 to 1, Measures the overlap of unigram (single word) between the generated and reference text.	0.2 (low overlap), 0.5 (moderate), 0.8 (high)</li>
<li><code>ROUGE-2</code>:	Range: 0 to 1, Measures the overlap of bigrams (two consecutive words) between generated and reference text.	0.1 (low), 0.4 (moderate), 0.7 (high)</li>
<li><code>ROUGE-L</code>:	Range: 0 to 1, Measures the longest common subsequence (LCS) between the generated and reference text.	0.3 (low), 0.6 (moderate), 0.9 (high)</li>
<li><code>ROUGE-Lsum</code> is specifically designed for summarization tasks, particularly when evaluating summaries. It computes the ROUGE-L score for the summarization task based on how well the generated summary matches the reference summary.</li>
</ul>
</li>
<li><strong>Purpose</strong>: Evaluates lexical similarity, fluency, and coherence across different levels (unigrams, bigrams, and sequences).<ul>
<li>Original ROUGE Paper: Lin, Chin-Yew. “ROUGE: A Package for Automatic Evaluation of Summaries”<ul>
<li>DOI: 10.3115&#x2F;1073083.1073135</li>
<li>Read on <a href="">ACL Anthology</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="ROUGE-L"><a href="#ROUGE-L" class="headerlink" title="ROUGE-L"></a>ROUGE-L</h3><p>ROUGE-L &#x3D; ROUGE (Recall-Oriented Understudy for Gisting Evaluation) + L (Longest Common Subsequence (<a href="#LCS">LCS</a>) </p>
<p>ROUGE-L captures the longest sequence of words that appear in both texts in the same order, providing insights into fluency and coherence.</p>
<ul>
<li>Paper: “Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics.” Lin, Chin-Yew.<ul>
<li>DOI: 10.3115&#x2F;1218955.1219032</li>
<li>Read on <a target="_blank" rel="noopener" href="https://sci-hub.se/10.3115/1218955.1219032">sci-hub.se</a></li>
</ul>
</li>
</ul>
<hr>
<h2 id="S"><a href="#S" class="headerlink" title="S"></a>S</h2><h3 id="Special-Token"><a href="#Special-Token" class="headerlink" title="Special Token"></a>Special Token</h3><p>Reserved tokens used in LLMs for specific functions, such as defining the start or end of a response.</p>
<hr>
<h2 id="Softmax-function"><a href="#Softmax-function" class="headerlink" title="Softmax function"></a>Softmax function</h2><p><a target="_blank" rel="noopener" href="https://notes.theomorales.com/Attention+is+all+you+need/The+Softmax+function">“Softmax function” read on notes.theomorales.com</a></p>
<hr>
<h3 id="Supervised-learning"><a href="#Supervised-learning" class="headerlink" title="Supervised learning"></a>Supervised learning</h3><p>Learning through pre-labeled inputs, where the model aims to reduce classification error by predicting the correct outputs.<br>It works opposite on <a href="#Unsupervised-learning">unsupervised learning</a></p>
<hr>
<h3 id="System-prompts"><a href="#System-prompts" class="headerlink" title="System prompts"></a>System prompts</h3><p>Instructions defining an LLM’s behavior, role, or context in a conversation.</p>
<hr>
<h2 id="T"><a href="#T" class="headerlink" title="T"></a>T</h2><h3 id="Token"><a href="#Token" class="headerlink" title="Token"></a>Token</h3><p>The smallest unit of text processed by an LLM, such as a word or subword.</p>
<hr>
<h3 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h3><p>Breaking text into tokens for model processing.</p>
<hr>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>Feeding a model with data to allow it to learn patterns and relationships.</p>
<hr>
<h2 id="U"><a href="#U" class="headerlink" title="U"></a>U</h2><h3 id="Unsupervised-learning"><a href="#Unsupervised-learning" class="headerlink" title="Unsupervised learning"></a>Unsupervised learning</h3><p>Learning from data without labeled outputs, often for tasks like clustering or dimensionality reduction.<br>It works opposite on <a href="#Supervised-learning">supervised learning</a></p>
<hr>
<h2 id="V"><a href="#V" class="headerlink" title="V"></a>V</h2><h3 id="Variational-Autoencoders-VAEs"><a href="#Variational-Autoencoders-VAEs" class="headerlink" title="Variational Autoencoders (VAEs)"></a>Variational Autoencoders (VAEs)</h3><p>Variational Autoencoders (VAEs) are a probabilistic extension of autoencoders that learn not only to compress data but also to generate new samples by modeling data distributions. VAEs use a latent space with a probabilistic structure, enabling meaningful interpolation between points in the latent space.<br><strong>Features:</strong></p>
<ol>
<li><strong>Latent Space Regularization</strong>: Ensures the latent space follows a predefined probability distribution, commonly Gaussian.</li>
<li><strong>Reconstruction and Generation</strong>: Balances reconstruction accuracy with the regularization term using a loss function derived from the evidence lower bound (ELBO).</li>
<li><strong>Bayesian Interpretation</strong>: The encoding process approximates posterior distributions via variational inference.</li>
</ol>
<p><strong>Applications:</strong></p>
<ul>
<li><strong>Data Generation</strong>: Generate novel and coherent samples (e.g., synthetic images, text).</li>
<li><strong>Anomaly Detection</strong>: Identifies data points that deviate from the learned distribution.</li>
<li><strong>Latent Space Manipulation</strong>: Enables interpolation and arithmetic operations in the latent space.</li>
</ul>
<p><strong>Key References:</strong></p>
<ul>
<li><strong>Title</strong>: “Auto-Encoding Variational Bayes”<br><strong>DOI</strong>: <a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.1312.6114">10.48550&#x2F;arXiv.1312.6114</a></li>
<li><strong>Title</strong>: “Variational Inference with Normalizing Flows”<br><strong>DOI</strong>: <a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.1505.05770">10.48550&#x2F;arXiv.1505.05770</a></li>
</ul>
<hr>
<h2 id="Z"><a href="#Z" class="headerlink" title="Z"></a>Z</h2><h3 id="Zero-padding-CNN"><a href="#Zero-padding-CNN" class="headerlink" title="Zero-padding (CNN)"></a>Zero-padding (CNN)</h3><p>A technique in CNNs that pads the borders of input data with zeros, controlling output dimensionality.</p>
<hr>
<h3 id="Zero-shot"><a href="#Zero-shot" class="headerlink" title="Zero-shot"></a>Zero-shot</h3><p>A model’s ability to perform tasks it was not explicitly trained for by leveraging general knowledge.</p>
<hr>
<p><strong>Footnotes</strong></p>
<div id="footnotes"><hr><div id="footnotelist"><ol style="list-style:none; padding-left: 0;"><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">2.</span><span style="display: inline-block; vertical-align: top;"><a target="_blank" rel="noopener" href="https://github.com/nomic-ai/gpt4all/wiki/Generative-AI-Terminology">https://github.com/nomic-ai/gpt4all/wiki/Generative-AI-Terminology</a></span><a href="#fnref:2" rev="footnote"> ↩</a></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">3.</span><span style="display: inline-block; vertical-align: top;"><a target="_blank" rel="noopener" href="https://nhsx.github.io/ai-dictionary">https://nhsx.github.io/ai-dictionary</a></span><a href="#fnref:3" rev="footnote"> ↩</a></li></ol></div></div>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://ooge0.github.io/hexo-blog/ai/glossary-of-machine-learning-and-ai-terms/" data-id="cm4l91slt002u94kka1ywa7nq" data-title="Glossary of Machine Learning and AI Terms" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/hexo-blog/tags/AI/" rel="tag">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/hexo-blog/tags/ML/" rel="tag">ML</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/hexo-blog/tags/glossary/" rel="tag">glossary</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/hexo-blog/nastusja/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Довідник для Лікаря Сімейної Медицини
        
      </div>
    </a>
  
  
    <a href="/hexo-blog/2024/11/18/notes/online_tools/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Online tools</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">
      Categories
    </h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/hexo-blog/categories/Categories/">Categories</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/hexo-blog/categories/Maintanance/">Maintanance</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/hexo-blog/categories/My-contribution/">My contribution</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/hexo-blog/categories/Notes/">Notes</a><span class="category-list-count">14</span></li><li class="category-list-item"><a class="category-list-link" href="/hexo-blog/categories/Personal-information/">Personal information</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/hexo-blog/categories/Posts/">Posts</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/hexo-blog/categories/QA/">QA</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/hexo-blog/categories/Tutorials/">Tutorials</a><span class="category-list-count">6</span></li></ul>
    </div>
  </div>
  
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/hexo-blog/tags/AI/" style="font-size: 20px;">AI</a> <a href="/hexo-blog/tags/API/" style="font-size: 11px;">API</a> <a href="/hexo-blog/tags/ASCII/" style="font-size: 10px;">ASCII</a> <a href="/hexo-blog/tags/BART/" style="font-size: 10px;">BART</a> <a href="/hexo-blog/tags/BERT/" style="font-size: 10px;">BERT</a> <a href="/hexo-blog/tags/Dashdevs/" style="font-size: 10px;">Dashdevs</a> <a href="/hexo-blog/tags/DocPublishing/" style="font-size: 10px;">DocPublishing</a> <a href="/hexo-blog/tags/GPT2/" style="font-size: 10px;">GPT2</a> <a href="/hexo-blog/tags/LLM/" style="font-size: 14px;">LLM</a> <a href="/hexo-blog/tags/LaTeX/" style="font-size: 10px;">LaTeX</a> <a href="/hexo-blog/tags/Llama/" style="font-size: 10px;">Llama</a> <a href="/hexo-blog/tags/ML/" style="font-size: 19px;">ML</a> <a href="/hexo-blog/tags/NLP/" style="font-size: 17px;">NLP</a> <a href="/hexo-blog/tags/NLTK/" style="font-size: 10px;">NLTK</a> <a href="/hexo-blog/tags/NN/" style="font-size: 12px;">NN</a> <a href="/hexo-blog/tags/NPF/" style="font-size: 10px;">NPF</a> <a href="/hexo-blog/tags/ReadTheDocs/" style="font-size: 10px;">ReadTheDocs</a> <a href="/hexo-blog/tags/VADER/" style="font-size: 10px;">VADER</a> <a href="/hexo-blog/tags/object-Object/" style="font-size: 10px;">[object Object]</a> <a href="/hexo-blog/tags/about-me/" style="font-size: 10px;">about_me</a> <a href="/hexo-blog/tags/acsii/" style="font-size: 11px;">acsii</a> <a href="/hexo-blog/tags/api-testing/" style="font-size: 10px;">api_testing</a> <a href="/hexo-blog/tags/apps/" style="font-size: 10px;">apps</a> <a href="/hexo-blog/tags/augmentation/" style="font-size: 10px;">augmentation</a> <a href="/hexo-blog/tags/blog/" style="font-size: 15px;">blog</a> <a href="/hexo-blog/tags/books/" style="font-size: 10px;">books</a> <a href="/hexo-blog/tags/chatbot/" style="font-size: 10px;">chatbot</a> <a href="/hexo-blog/tags/cheatsheets/" style="font-size: 10px;">cheatsheets</a> <a href="/hexo-blog/tags/cmd/" style="font-size: 10px;">cmd</a> <a href="/hexo-blog/tags/data-mining/" style="font-size: 10px;">data_mining</a> <a href="/hexo-blog/tags/db/" style="font-size: 10px;">db</a> <a href="/hexo-blog/tags/dev-side/" style="font-size: 15px;">dev_side</a> <a href="/hexo-blog/tags/emotions/" style="font-size: 10px;">emotions</a> <a href="/hexo-blog/tags/examples/" style="font-size: 10px;">examples</a> <a href="/hexo-blog/tags/global-knowledge/" style="font-size: 10px;">global_knowledge</a> <a href="/hexo-blog/tags/glossary/" style="font-size: 11px;">glossary</a> <a href="/hexo-blog/tags/hexo-io/" style="font-size: 15px;">hexo_io</a> <a href="/hexo-blog/tags/html/" style="font-size: 10px;">html</a> <a href="/hexo-blog/tags/info/" style="font-size: 10px;">info</a> <a href="/hexo-blog/tags/kafka/" style="font-size: 10px;">kafka</a> <a href="/hexo-blog/tags/knowledge/" style="font-size: 11px;">knowledge</a> <a href="/hexo-blog/tags/lexic/" style="font-size: 11px;">lexic</a> <a href="/hexo-blog/tags/linguistic/" style="font-size: 11px;">linguistic</a> <a href="/hexo-blog/tags/logger/" style="font-size: 10px;">logger</a> <a href="/hexo-blog/tags/loguru/" style="font-size: 10px;">loguru</a> <a href="/hexo-blog/tags/markdown/" style="font-size: 10px;">markdown</a> <a href="/hexo-blog/tags/md-format/" style="font-size: 12px;">md_format</a> <a href="/hexo-blog/tags/mobile-application/" style="font-size: 10px;">mobile_application</a> <a href="/hexo-blog/tags/movie/" style="font-size: 10px;">movie</a> <a href="/hexo-blog/tags/my-contribution/" style="font-size: 16px;">my_contribution</a> <a href="/hexo-blog/tags/my-linkedin-post/" style="font-size: 10px;">my_linkedin_post</a> <a href="/hexo-blog/tags/my-medium-post/" style="font-size: 10px;">my_medium_post</a> <a href="/hexo-blog/tags/my-porjects/" style="font-size: 10px;">my_porjects</a> <a href="/hexo-blog/tags/network-protocols/" style="font-size: 10px;">network_protocols</a> <a href="/hexo-blog/tags/note/" style="font-size: 12px;">note</a> <a href="/hexo-blog/tags/nst/" style="font-size: 10px;">nst</a> <a href="/hexo-blog/tags/online-tools/" style="font-size: 10px;">online_tools</a> <a href="/hexo-blog/tags/papers/" style="font-size: 13px;">papers</a> <a href="/hexo-blog/tags/parsing/" style="font-size: 10px;">parsing</a> <a href="/hexo-blog/tags/postman/" style="font-size: 10px;">postman</a> <a href="/hexo-blog/tags/prompt-engineering/" style="font-size: 13px;">prompt_engineering</a> <a href="/hexo-blog/tags/protocols/" style="font-size: 10px;">protocols</a> <a href="/hexo-blog/tags/python/" style="font-size: 12px;">python</a> <a href="/hexo-blog/tags/qa/" style="font-size: 12px;">qa</a> <a href="/hexo-blog/tags/qa-check-list/" style="font-size: 10px;">qa_check_list</a> <a href="/hexo-blog/tags/resources/" style="font-size: 10px;">resources</a> <a href="/hexo-blog/tags/science/" style="font-size: 14px;">science</a> <a href="/hexo-blog/tags/sentiment-analysis/" style="font-size: 11px;">sentiment_analysis</a> <a href="/hexo-blog/tags/statistics/" style="font-size: 10px;">statistics</a> <a href="/hexo-blog/tags/tags/" style="font-size: 10px;">tags</a> <a href="/hexo-blog/tags/test-design/" style="font-size: 10px;">test_design</a> <a href="/hexo-blog/tags/test-tasks/" style="font-size: 10px;">test_tasks</a> <a href="/hexo-blog/tags/text-classification/" style="font-size: 10px;">text_classification</a> <a href="/hexo-blog/tags/text-generation/" style="font-size: 10px;">text_generation</a> <a href="/hexo-blog/tags/tools/" style="font-size: 10px;">tools</a> <a href="/hexo-blog/tags/tutorial/" style="font-size: 18px;">tutorial</a> <a href="/hexo-blog/tags/ubuntu/" style="font-size: 10px;">ubuntu</a> <a href="/hexo-blog/tags/unicode/" style="font-size: 10px;">unicode</a> <a href="/hexo-blog/tags/unix/" style="font-size: 10px;">unix</a> <a href="/hexo-blog/tags/usb/" style="font-size: 10px;">usb</a> <a href="/hexo-blog/tags/ux-ui/" style="font-size: 10px;">ux_ui</a> <a href="/hexo-blog/tags/web/" style="font-size: 10px;">web</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/hexo-blog/archives/2024/12/">December 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/hexo-blog/archives/2024/11/">November 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/hexo-blog/archives/2024/10/">October 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/hexo-blog/archives/2024/09/">September 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/hexo-blog/archives/2024/08/">August 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/hexo-blog/archives/2024/05/">May 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/hexo-blog/archives/2024/04/">April 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/hexo-blog/archives/2023/10/">October 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/hexo-blog/archives/2023/06/">June 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/hexo-blog/archives/2023/05/">May 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/hexo-blog/archives/2023/01/">January 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/hexo-blog/archives/2010/10/">October 2010</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/hexo-blog/2024/12/12/post_linquistic__text_coherence_vs_text_quiality/">Text Generation Coherence vs. Text Generation Quality</a>
          </li>
        
          <li>
            <a href="/hexo-blog/2024/12/10/post_ai_ml_basiscs__fune_tuning_vs_training_models_for_specific_tasks/">Fine-Tuning vs. Training Models for Specific Tasks</a>
          </li>
        
          <li>
            <a href="/hexo-blog/2024/12/10/post_ai_nn__comparison_of_popular_models_and_architectures/">Comparison of popular models and architectures</a>
          </li>
        
          <li>
            <a href="/hexo-blog/2024/12/10/post_ai_nn__list_of_neural_network_models_architectures_and_basic_components/">List of neural network models, architectures, and basic components</a>
          </li>
        
          <li>
            <a href="/hexo-blog/2024/12/04/post_ai_llm__bart_configuration_parameters_overview/">BART configuration parameters overview</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2024 si0n4ra<br>
      <!-- Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> (v7.3.0) hexo-cli: 4.3.2 -->
      Powered by <a href="https://hexo.io/" target="_blank">Hexo v.7.3.0</a> & hexo-cli: 4.3.2
      <br>Theme: <a href="https://github.com/hexojs/hexo-theme-landscape" target="_blank">landscape</a>
    </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/hexo-blog/" class="mobile-nav-link">Home</a>
  
    <a href="/hexo-blog/categories/Posts/" class="mobile-nav-link">Posts</a>
  
    <a href="/hexo-blog/categories/Notes/" class="mobile-nav-link">Notes</a>
  
    <a href="/hexo-blog/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/hexo-blog/categories/" class="mobile-nav-link">Categories</a>
  
    <a href="/hexo-blog/about-me/" class="mobile-nav-link">About me</a>
  
</nav>
    


<script src="/hexo-blog/js/jquery-3.6.4.min.js"></script>



  
<script src="/hexo-blog/fancybox/jquery.fancybox.min.js"></script>




<script src="/hexo-blog/js/script.js"></script>





  </div>
</body>
</html>