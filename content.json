{"meta":{"title":" ","subtitle":"...chasing dreams, living reality","description":"Personal blog where you can find a lot of information about me and my projects","author":"si0n4ra","url":"https://ooge0.github.io/hexo-blog","root":"/hexo-blog/"},"pages":[{"title":"","date":"2024-12-11T13:07:13.407Z","updated":"2024-12-11T13:07:13.407Z","comments":true,"path":"draft/AI.html","permalink":"https://ooge0.github.io/hexo-blog/draft/AI","excerpt":"","text":"Make table with content for Autoencoders, CNN, RNN, FNN, LSTM, GNN, BERT, BART, T5, LLAMA, GPT Table columns: 1 - model&#x2F;architecture 2 - basic components of model&#x2F;architecture 3 -open-source &#x2F; forbiden for modifications 4 - criteria of mesuring model productivity 5 - model’s parameters (changable&#x2F;non-changable) 6 - criteria of mesuring parameter’s productivity 7 - model’s hyper parameters (changable&#x2F;non-changable) 8 - criteria of mesuring hyper parameter’s productivity 9 - basic components of the hyper parameter’s productivity"},{"title":"","date":"2024-12-11T09:39:56.639Z","updated":"2024-12-11T09:39:56.639Z","comments":true,"path":"draft/python.html","permalink":"https://ooge0.github.io/hexo-blog/draft/python","excerpt":"","text":"Python Rest Api Framework’s documentation [Python Rest Api Framework’s documentation project ()]https://python-rest-framework.readthedocs.io/en/latest/index.html Description: Python REST API framework is a set of utilities based on werkzeug to easily build Restful API with a MVC pattern. Main features includes: Pagination, Authentication, Authorization, Filters, Partials Response, Error handling, data validators, data formaters… and more… Sections: Working with Pagination"},{"title":"","date":"2024-12-11T09:59:53.733Z","updated":"2024-12-11T09:59:53.733Z","comments":true,"path":"draft/понимание текстов.html","permalink":"https://ooge0.github.io/hexo-blog/draft/%D0%BF%D0%BE%D0%BD%D0%B8%D0%BC%D0%B0%D0%BD%D0%B8%D0%B5%20%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%BE%D0%B2","excerpt":"","text":"Сюжет Понимание Восприятие Внимание Мышление Мышление - учат через логику Понимание - учат через текст"},{"title":"","date":"2024-10-28T10:18:33.000Z","updated":"2024-11-18T15:10:12.171Z","comments":true,"path":"/index.html","permalink":"https://ooge0.github.io/hexo-blog/","excerpt":"","text":"Hello and welcome! Hereregarding to this blog I can say that here you’ll find content on various web testing techniques, QA/QC technical skills, hands-on projects, plus a bit about my journey and achievements in software quality assurance. Also here you will find information related to topics related to the artificial intelligence, neural networks, natural language processing, data processing, mashine learning, crawling data and web parsing Feel free to explore: Navigation Menu: Use the menu on the left to browse posts by topic or tags. Mobile-Friendly features: Whether on mobile or desktop, HEXO.IO’s blog design adapts for readability. Projects: Discover my projects in the GitHub projects section, showcasing real-world applications of my work and posts that I decided add to my collection. Archives: For a complete collection of my posts, head to My Archives Blog Stats: Check out some blog statistics on the dedicated page of my statistics. You can find more information about me, my achievements and contributions. Let’s connect, learn, and grow together in the world of software quality and testing. My latest CV you can see here. Happy reading! Total views:"},{"title":"Sitemap","date":"2024-11-01T10:00:00.000Z","updated":"2024-11-01T09:31:07.263Z","comments":true,"path":"sitemap/index.html","permalink":"https://ooge0.github.io/hexo-blog/sitemap/","excerpt":"","text":"Sitemap Posts Categories"}],"posts":[{"title":"Windows OS apps","slug":"post_os__windows app","date":"2024-12-13T07:18:22.000Z","updated":"2024-12-13T07:33:16.529Z","comments":true,"path":"2024/12/13/post_os__windows app/","permalink":"https://ooge0.github.io/hexo-blog/2024/12/13/post_os__windows%20app/","excerpt":"","text":"Monitorian Monitorian app description: Monitorian is a desktop tool to adjust the brightness of multiple monitors with ease. The user can change the brightness of monitors, including external ones, either individually or in unison. In addition, the user can change the adjustable range of brightness and contrast for each monitor seamlessly. To control an external monitor, the monitor must be DDC/CI compatible and the function enabled. If a monitor is connected through an converter, docking station or other device, such a device must be also compatible. Monitorian home page Monitorian Microsoft store download page","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"apps","slug":"apps","permalink":"https://ooge0.github.io/hexo-blog/tags/apps/"},{"name":"windows_os","slug":"windows-os","permalink":"https://ooge0.github.io/hexo-blog/tags/windows-os/"}]},{"title":"Text Generation Coherence vs. Text Generation Quality","slug":"post_linquistic__text_coherence_vs_text_quiality","date":"2024-12-12T09:21:11.000Z","updated":"2024-12-12T10:54:20.189Z","comments":true,"path":"2024/12/12/post_linquistic__text_coherence_vs_text_quiality/","permalink":"https://ooge0.github.io/hexo-blog/2024/12/12/post_linquistic__text_coherence_vs_text_quiality/","excerpt":"","text":"1. Text Generation CoherenceCoherence refers to the logical and structural flow of the text. A coherent text feels connected and makes sense as a whole. It ensures that: Logical Progression: Sentences and paragraphs follow each other in a natural order. Topic Consistency: The generated text remains focused on a single theme or idea. Contextual Relevance: Each part of the text relates appropriately to the previous and following parts. Absence of Contradictions: There are no logical inconsistencies or contradictory statements. Example: Coherent Text: “The sun was shining brightly in the clear blue sky. Birds chirped as children played in the park. It was a perfect day for a picnic.” This is coherent because the sentences are connected and describe a single scene. Incoherent Text: “The sun was shining. Suddenly, a spaceship landed in the park. Watermelons are tasty.” This lacks coherence due to abrupt topic shifts and lack of logical flow. Key Challenge: Ensuring the generated text flows smoothly across different parts without jumping topics or introducing unrelated ideas. 2. Text Generation QualityQuality refers to how well the generated text meets overall standards of good writing. It is a broader measure that includes: Grammar and Syntax: Free of grammatical errors and awkward sentence structures. Vocabulary Use: Appropriate word choice, richness of language, and precision. Creativity and Style: Ability to generate text that is engaging, varied, and stylistically appropriate for the context. Correctness: Facts and information presented are accurate. Relevance: Addresses the input or prompt directly and completely. Example: High-Quality Text: “Artificial intelligence has revolutionized many industries, offering unparalleled efficiency and innovation. From healthcare to finance, its impact is profound.” This text is grammatically correct, well-structured, and relevant. Low-Quality Text: “Artificial intelligance revolution in industries many, efficiently innovation offering. Healthcare finance profound impact is.” This text is grammatically incorrect and poorly structured. Key Challenge: Balancing creativity, accuracy, and linguistic correctness to produce engaging and relevant content. Comparison Aspect Text Generation Coherence Text Generation Quality Focus Logical flow and connection between parts of the text. Overall writing standards, including grammar, style, and relevance. Scope Specific to structural and contextual alignment. Broader, encompassing coherence, grammar, creativity, etc. Evaluation Assesses transitions, logical progression, and focus. Evaluates overall effectiveness, correctness, and engagement. Common Issues Topic shifts, contradictions, lack of context. Grammatical errors, awkward phrasing, irrelevance. Real-World Importance Coherence is critical for tasks requiring deep contextual understanding, such as writing long essays or technical documents. Quality is essential for creating polished and professional output, such as marketing content or customer communications. While coherence is a subset of quality, high-quality text must always be coherent. However, coherent text might not necessarily be high-quality if it lacks creativity, relevance, or stylistic finesse.","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://ooge0.github.io/hexo-blog/tags/NLP/"},{"name":"linguistic","slug":"linguistic","permalink":"https://ooge0.github.io/hexo-blog/tags/linguistic/"}]},{"title":"Fine-Tuning vs. Training Models for Specific Tasks","slug":"post_ai_ml_basiscs__fune_tuning_vs_training_models_for_specific_tasks","date":"2024-12-10T17:30:33.000Z","updated":"2024-12-10T17:30:22.717Z","comments":true,"path":"2024/12/10/post_ai_ml_basiscs__fune_tuning_vs_training_models_for_specific_tasks/","permalink":"https://ooge0.github.io/hexo-blog/2024/12/10/post_ai_ml_basiscs__fune_tuning_vs_training_models_for_specific_tasks/","excerpt":"","text":"Fine-tuning and training are two common approaches to adapting machine learning models for specific tasks. While training involves building a model from scratch or pre-trained weights, fine-tuning adapts an already trained model to perform well on a particular task. Below, we explore the nuances of these approaches for various popular architectures, including FFNs, CNNs, LSTMs, and Transformers. Key differences between fine-tuning and training Aspect Fine-Tuning Training Starting Point Pre-trained model weights from a related task (e.g., ImageNet for vision, GPT for text). Randomly initialized weights. Data Requirements Requires less data; leverages knowledge from the pre-trained model. Requires large amounts of labeled data. Training Duration Generally shorter; only adjusts some layers. Longer; all parameters are learned from scratch. Resource Needs Less compute-intensive due to reduced training scope. High resource demands for extensive training. Use Case When data is limited or computational resources are constrained. When custom architecture or large task-specific datasets are available. Baselines for fine-tuning&#x2F;training1. Feedforward Networks (FFNs) Baseline: Fully connected layers with non-linear activations. Fine-Tuning: Modify output layers to match the target task and optionally freeze earlier layers. Training: Start with random initialization, requiring carefully tuned learning rates and weight initialization methods. 2. Convolutional Neural Networks (CNNs) Baseline: Architectures like VGG, ResNet, or EfficientNet, pre-trained on ImageNet. Fine-Tuning: Replace the classification head to match target classes, often freezing earlier convolutional layers. Training: Train from scratch if the task involves entirely different visual domains (e.g., medical imaging). 3. Recurrent Neural Networks (RNNs) and Variants (LSTMs, GRUs) Baseline: Pre-trained word embeddings (e.g., GloVe, Word2Vec) or models like ELMo. Fine-Tuning: Use pre-trained embeddings, fine-tune the LSTM layers on sequential tasks. Training: Train an LSTM network from scratch for language modeling or time-series tasks. 4. Transformers Baseline: Models like BERT, GPT, or T5. Fine-Tuning: Modify the decoder head, adjust hyperparameters like learning rate and layer freezing. Training: Start with pre-trained embeddings; for unique tasks, initialize the Transformer from scratch (high compute). Technical details of popular architectures Model Baseline Task Fine-Tuning Scope Training Considerations VGG16 Image Classification (ImageNet) Replace final dense layer, freeze initial layers. High memory usage, simpler architecture to optimize. ResNet50 Image Classification (ImageNet) Adjust classification head; fine-tune deeper layers as needed. Skip connections improve gradient flow. BERT Masked Language Model (MLM) Modify for classification, QA, or summarization. Pre-training requires MLM objectives. GPT-3 Text Generation Fine-tune specific tasks by updating decoder head. Requires extensive GPU resources for full training. LSTM Sequential Data Modeling Fine-tune on embeddings, adjust for target sequence length. Long training times due to sequential processing. EfficientNet Image Classification Replace head; scale input resolution for task-specific datasets. Compound scaling optimizes trade-offs in performance. SummaryFine-tuning and training models depend heavily on the task, available data, and computational resources. Fine-tuning is generally faster and resource-efficient, making it ideal for adapting large pre-trained models to specific tasks. On the other hand, training from scratch offers flexibility when creating custom architectures but demands extensive data and compute. By understanding these approaches, practitioners can select the most suitable method for their applications.","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"ML","slug":"ML","permalink":"https://ooge0.github.io/hexo-blog/tags/ML/"}]},{"title":"Comparison of popular models and architectures","slug":"post_ai_nn__comparison_of_popular_models_and_architectures","date":"2024-12-10T17:30:33.000Z","updated":"2024-12-13T07:38:16.744Z","comments":true,"path":"2024/12/10/post_ai_nn__comparison_of_popular_models_and_architectures/","permalink":"https://ooge0.github.io/hexo-blog/2024/12/10/post_ai_nn__comparison_of_popular_models_and_architectures/","excerpt":"","text":"Related to: List of neural network models, architectures, and basic components Table - Single models with optionsDetailed Breakdown of Popular Models and ArchitecturesAutoencoders Category Details Basic Components Encoder, Decoder, Latent Space Open-Source&#x2F; Forbidden Open-Source Criteria of Measuring Productivity Reconstruction Loss Model’s Parameters Weights (Changeable), Latent Dimensions (Fixed) Criteria of Measuring Parameter’s Productivity Reconstruction Accuracy Model’s Hyperparameters Learning Rate, Latent Dimension Size (Changeable) Criteria of Measuring Hyperparameter’s Productivity Lower Reconstruction Error Basic Components of Hyperparameter’s Productivity Effective Latent Space Size, Training Convergence Rate CNN (Convolutional Neural Networks) Category Details Basic Components Convolution Layers, Pooling, Fully Connected Layers Open-Source&#x2F; Forbidden Open-Source Criteria of Measuring Productivity Accuracy, Precision, Recall, F1-Score Model’s Parameters Filter Weights (Changeable), Input Channels (Fixed) Criteria of Measuring Parameter’s Productivity Detection Accuracy Model’s Hyperparameters Kernel Size, Stride, Number of Filters (Changeable) Criteria of Measuring Hyperparameter’s Productivity Higher Feature Extraction Quality Basic Components of Hyperparameter’s Productivity Filter Efficiency, Computational Cost RNN (Recurrent Neural Networks) Category Details Basic Components Recurrent Layers, Activation Functions Open-Source&#x2F; Forbidden Open-Source Criteria of Measuring Productivity Perplexity, Accuracy, BLEU Score (NLP) Model’s Parameters Hidden State (Changeable), Sequence Length (Fixed) Criteria of Measuring Parameter’s Productivity Temporal Pattern Capture Efficiency Model’s Hyperparameters Learning Rate, Hidden State Size (Changeable) Criteria of Measuring Hyperparameter’s Productivity Sequence Learning Performance Basic Components of Hyperparameter’s Productivity Effective Sequence Memory Size LSTM (Long Short-Term Memory Networks) Category Details Basic Components LSTM Cells (Input, Forget, Output Gates) Open-Source&#x2F; Forbidden Open-Source Criteria of Measuring Productivity Perplexity, Accuracy (Time-Series, NLP) Model’s Parameters Cell Weights (Changeable), Memory Cell (Fixed) Criteria of Measuring Parameter’s Productivity Long-Term Dependency Capture Efficiency Model’s Hyperparameters Learning Rate, Number of Layers (Changeable) Criteria of Measuring Hyperparameter’s Productivity Retention of Long-Term Dependencies Basic Components of Hyperparameter’s Productivity Sequence Retention and Gradient Stability GNN (Graph Neural Networks) Category Details Basic Components Node Embeddings, Edge Features, Graph Convolutions Open-Source&#x2F; Forbidden Open-Source Criteria of Measuring Productivity Node Classification Accuracy, Link Prediction Model’s Parameters Edge Weights (Changeable), Node Attributes (Fixed) Criteria of Measuring Parameter’s Productivity Graph Feature Capture Efficiency Model’s Hyperparameters Number of Layers, Embedding Size (Changeable) Criteria of Measuring Hyperparameter’s Productivity Graph-Level Feature Generalization Basic Components of Hyperparameter’s Productivity Graph Topology Learning BERT (Bidirectional Encoder Representations from Transformers) Category Details Basic Components Encoder, Multi-Head Attention, Feedforward Layers Open-Source&#x2F; Forbidden Open-Source Criteria of Measuring Productivity F1-Score, Exact Match (QA), Perplexity Model’s Parameters Token Embeddings (Changeable), Vocabulary (Fixed) Criteria of Measuring Parameter’s Productivity Contextual Understanding Quality Model’s Hyperparameters Learning Rate, Batch Size, Sequence Length (Changeable) Criteria of Measuring Hyperparameter’s Productivity Contextual Embedding Accuracy Basic Components of Hyperparameter’s Productivity Attention Mechanism, Positional Encoding BART (Bidirectional and Auto-Regressive Transformers) Category Details Basic Components Encoder-Decoder, Multi-Head Attention, Feedforward Open-Source&#x2F; Forbidden Open-Source Criteria of Measuring Productivity Rouge Score, BLEU Score Model’s Parameters Attention Weights (Changeable), Vocabulary (Fixed) Criteria of Measuring Parameter’s Productivity Summarization and Translation Accuracy Model’s Hyperparameters Learning Rate, Number of Heads (Changeable) Criteria of Measuring Hyperparameter’s Productivity Text Generation Quality Basic Components of Hyperparameter’s Productivity Encoder-Decoder Consistency T5 (Text-to-Text Transfer Transformer) Category Details Basic Components Encoder-Decoder, Attention Mechanisms, Feedforward Open-Source&#x2F; Forbidden Open-Source Criteria of Measuring Productivity Rouge Score, BLEU Score Model’s Parameters Token Embeddings (Changeable), Vocabulary (Fixed) Criteria of Measuring Parameter’s Productivity Text-to-Text Conversion Accuracy Model’s Hyperparameters Sequence Length, Beam Width (Changeable) Criteria of Measuring Hyperparameter’s Productivity Text Generation Coherence Basic Components of Hyperparameter’s Productivity Attention Span, Latent Representation Quality LLAMA Category Details Basic Components Transformer Layers, Feedforward Layers, Attention Open-Source&#x2F; Forbidden Restricted for Modifications Criteria of Measuring Productivity F1-Score, Rouge Score Model’s Parameters Attention Weights (Changeable), Vocabulary (Fixed) Criteria of Measuring Parameter’s Productivity Latent Representation Consistency Model’s Hyperparameters Number of Layers, Head Size (Changeable) Criteria of Measuring Hyperparameter’s Productivity Layer-to-Layer Weight Propagation Basic Components of Hyperparameter’s Productivity Transformer Block Efficiency GPT (Generative Pre-trained Transformer) Category Details Basic Components Transformer Decoder, Feedforward Layers, Attention Open-Source&#x2F; Forbidden Open-Source Criteria of Measuring Productivity Perplexity, BLEU Score Model’s Parameters Attention Weights (Changeable), Vocabulary (Fixed) Criteria of Measuring Parameter’s Productivity Generative Text Coherence Model’s Hyperparameters Learning Rate, Model Depth, Token Limit (Changeable) Criteria of Measuring Hyperparameter’s Productivity Generative Text Quality Basic Components of Hyperparameter’s Productivity Token Context Understanding ViT (Vision Transformer) Category Details Basic Components Patch Embedding, Transformer Layers, Attention Open-Source&#x2F; Forbidden Open-Source Criteria of Measuring Productivity Accuracy, Precision, Recall, F1-Score Model’s Parameters Patch Embeddings (Changeable), Image Size (Fixed) Criteria of Measuring Parameter’s Productivity Visual Feature Generalization Model’s Hyperparameters Patch Size, Attention Heads (Changeable) Criteria of Measuring Hyperparameter’s Productivity Patch Extraction Accuracy, Attention Span Basic Components of Hyperparameter’s Productivity Image Feature Learning Efficiency","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"NN","slug":"NN","permalink":"https://ooge0.github.io/hexo-blog/tags/NN/"}]},{"title":"List of neural network models, architectures, and basic components","slug":"post_ai_nn__list_of_neural_network_models_architectures_and_basic_components","date":"2024-12-10T17:30:33.000Z","updated":"2024-12-12T11:40:09.755Z","comments":true,"path":"2024/12/10/post_ai_nn__list_of_neural_network_models_architectures_and_basic_components/","permalink":"https://ooge0.github.io/hexo-blog/2024/12/10/post_ai_nn__list_of_neural_network_models_architectures_and_basic_components/","excerpt":"","text":"Related to Comparison of Popular Models and Architectures This document provides a categorized list of common neural network (NN) models and architectures. It also outlines their basic components and how they fit into larger systems. Neural network models and architectures Architecture Model Examples Purpose Feedforward Neural Network (FNN) Basic MLP (Multi-Layer Perceptron) General-purpose model for regression and classification tasks. Convolutional Neural Networks (CNN) VGG, ResNet, AlexNet, EfficientNet Designed for image processing tasks like classification, object detection, and segmentation. Recurrent Neural Networks (RNNs) Vanilla RNN, LSTM, GRU Sequential data processing for tasks like language modeling and time-series prediction. Transformers BERT, GPT, T5, Vision Transformer (ViT) State-of-the-art architecture for text, sequential, and image tasks. Autoencoders Variational Autoencoder (VAE), Denoising Autoencoder Dimensionality reduction, feature extraction, and generative tasks. Generative Adversarial Networks (GANs) DCGAN, StyleGAN, CycleGAN Generative tasks such as image synthesis and domain transfer. Graph Neural Networks (GNNs) GCN, GraphSAGE, GAT Structured data learning tasks, e.g., on graphs or social networks. Basic Components of Neural Networks Component Description Applications Neuron Basic computation unit applying a weighted sum followed by an activation function. Foundational unit in all neural networks. Layer A collection of neurons; can be input, hidden, or output. Used in all neural architectures. Activation Function Non-linear function applied to neurons, e.g., ReLU, Sigmoid, Tanh. Enables learning of complex patterns. Dropout Regularization technique randomly dropping neurons during training. Reduces overfitting in models. Encoder Part of the model that converts input data into a latent representation. Used in Transformers, Autoencoders, BERT, and more. Decoder Converts latent representations back to an output format. Used in Transformers, Autoencoders, and Seq2Seq models. Attention Mechanism Focuses on important parts of the input data, e.g., Self-Attention. Essential in Transformers and attention-based architectures. Residual Block A module that adds shortcut connections to mitigate vanishing gradients. Found in ResNet, Transformer architectures. Convolution Layer Applies convolutional operations to extract spatial features. Used in CNNs for tasks like image and video analysis. Pooling Layer Reduces spatial dimensions using techniques like max-pooling or average pooling. Used in CNNs to downsample feature maps. Recurrent Cell Core unit of RNNs, capable of maintaining temporal dependencies. Used in RNNs, LSTMs, and GRUs for time-series and sequential data. Self-Attention Layer Computes relationships between all input tokens to capture global dependencies. Core of Transformers. Feedforward Layer Dense layer applied after attention mechanisms in Transformers. Processes token-wise transformations. Embedding Layer Converts categorical data or tokens into dense vectors. Used in NLP, graph embeddings, and more. Latent Space Compressed representation of data, typically learned by encoders. Found in Autoencoders, VAEs, and GANs. How components relate to models Architecture Key Components FNN Neurons, Layers, Activation Functions, Dropout. CNN Convolution Layers, Pooling Layers, Fully Connected Layers, Activation Functions. RNN (Vanilla) Recurrent Cells, Layers, Activation Functions. LSTM LSTM Cells (with Forget, Input, Output gates), Layers. Transformers Encoder, Decoder, Self-Attention, Multi-Head Attention, Feedforward Layers, Positional Embeddings. Autoencoders Encoder, Decoder, Latent Space, Reconstruction Loss. GANs Generator, Discriminator, Adversarial Loss. GNNs Node Embeddings, Edge Features, Graph Convolutions. This table serves as a foundation for understanding how modern deep learning architectures are structured and utilized across a wide range of applications.","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"NN","slug":"NN","permalink":"https://ooge0.github.io/hexo-blog/tags/NN/"}]},{"title":"BART configuration parameters overview","slug":"post_ai_llm__bart_configuration_parameters_overview","date":"2024-12-04T20:08:12.000Z","updated":"2024-12-04T20:39:12.658Z","comments":true,"path":"2024/12/04/post_ai_llm__bart_configuration_parameters_overview/","permalink":"https://ooge0.github.io/hexo-blog/2024/12/04/post_ai_llm__bart_configuration_parameters_overview/","excerpt":"","text":"This post related to: Techniques for Handling Context in AI Models Parameters list and descriptions Parameter Description Data Type&#x2F;Options max_position_embeddings Maximum number of positions for input tokens. Integer, e.g., 1024. d_model Dimensionality of the model’s embeddings and hidden states. Integer, e.g., 768, 1024. encoder_layers Number of layers in the encoder. Integer, e.g., 6, 12. decoder_layers Number of layers in the decoder. Integer, e.g., 6, 12. encoder_attention_heads Number of attention heads in the encoder. Integer, e.g., 12. decoder_attention_heads Number of attention heads in the decoder. Integer, e.g., 12. vocab_size Vocabulary size of the tokenizer. Represents the range of token IDs. Integer, e.g., 50265. activation_function Activation function used in feedforward layers. String: relu, gelu, tanh, etc. dropout Dropout probability applied to various layers. Float between 0.0 and 1.0, typically 0.1. attention_dropout Dropout probability in the attention mechanism. Float between 0.0 and 1.0, typically 0.1. init_std Standard deviation for weight initialization. Float, e.g., 0.02. encoder_ffn_dim Dimensionality of the encoder feedforward layers. Integer, e.g., 3072. decoder_ffn_dim Dimensionality of the decoder feedforward layers. Integer, e.g., 3072. scale_embedding Whether to scale the embeddings by sqrt(d_model). Boolean, true or false. use_cache Whether to use cached key&#x2F;values for faster decoding. Boolean, true or false. pad_token_id Token ID used for padding. Integer, typically 0. bos_token_id Token ID for the beginning-of-sequence token. Integer, typically 0. eos_token_id Token ID for the end-of-sequence token. Integer, typically 2. Summary of parameter impactHow Changes Reflect on Model Behavior: Model Complexity: Increasing encoder_layers, decoder_layers, d_model, or attention heads enhances model capacity but increases computational requirements. Regularization: Dropout parameters (dropout, attention_dropout) control overfitting risks but may reduce performance if too high. Encoder-Decoder interactions: encoder_ffn_dim and decoder_ffn_dim directly influence the learning ability of the model for complex patterns. Efficiency: Enabling use_cache improves inference time for autoregressive tasks. References Paper: BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension Read on arxiv.org DOI:10.48550&#x2F;arXiv.1910.13461 Paper: Improving Sharpness-Aware Minimization with Fisher Mask for Better Generalization on Language Models DOI:10.48550&#x2F;arXiv.2210.05497 Read on arxiv.org Hugging Face BART Documentation Web article: BART Model Architecture | Medium Web article: Transformers BART Model Explained for Text Summarization | projectpro.io Google colab notebook: BART Learns to Rap - Medium.ipynb Challenges and reports on configuration Report: Paper: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Raffel et al., 2020. DOI: 10.48550&#x2F;arXiv.1910.10683 Read on arxiv.org Challenges: Balancing fine-tuning for generative and discriminative tasks in sequence-to-sequence models. Paper: Unified Generative and Discriminative Training for Multi-modal Large Language Models Read on arxiv.org Playgrounds to experiment Hugging Face Spaces: https://huggingface.co/spaces Google Colab with Transformers: https://colab.research.google.com/ OpenAI Playground for Generative Tasks: https://platform.openai.com/playground","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"LLM","slug":"LLM","permalink":"https://ooge0.github.io/hexo-blog/tags/LLM/"},{"name":"BART","slug":"BART","permalink":"https://ooge0.github.io/hexo-blog/tags/BART/"}]},{"title":"BERT configuration parameters overview","slug":"post_ai_llm__bert_configuration_parameters_overview","date":"2024-12-04T20:08:12.000Z","updated":"2024-12-04T20:38:06.482Z","comments":true,"path":"2024/12/04/post_ai_llm__bert_configuration_parameters_overview/","permalink":"https://ooge0.github.io/hexo-blog/2024/12/04/post_ai_llm__bert_configuration_parameters_overview/","excerpt":"","text":"This post related to:1. Techniques for Handling Context in AI Models2. GPT2 configuration parameters overview3. BART configuration parameters overviewParameters list and descriptions Parameter Description Data Type&#x2F;Options hidden_size Dimensionality of the hidden states and embeddings. Integer, e.g., 768, 1024. num_hidden_layers Number of hidden layers in the transformer encoder. Integer, e.g., 12, 24. num_attention_heads Number of attention heads per transformer layer. Integer, typically a divisor of hidden_size. vocab_size Vocabulary size of the tokenizer. Represents the range of token IDs. Integer, e.g., 30522. intermediate_size Dimensionality of the feedforward layers. Integer, e.g., 3072. hidden_dropout_prob Dropout probability for fully connected layers in the encoder. Float between 0.0 and 1.0, typically 0.1. attention_probs_dropout_prob Dropout probability in the attention mechanism. Float between 0.0 and 1.0, typically 0.1. max_position_embeddings Maximum number of positions for input tokens. Integer, e.g., 512. type_vocab_size Size of the token type vocabulary for segment embeddings. Integer, typically 2. initializer_range Standard deviation for weight initialization. Float, e.g., 0.02. layer_norm_eps A small value added for numerical stability in layer normalization. Float, typically 1e-5. output_hidden_states Whether to output all hidden states from each layer. Boolean, true or false. output_attentions Whether to output the attention weights. Boolean, true or false. Summary of parameter impactHow changes reflect on model behavior: Capacity: Increasing hidden_size, num_hidden_layers, or num_attention_heads allows the model to capture more complex patterns but increases resource usage. Regularization: Dropout probabilities (hidden_dropout_prob, attention_probs_dropout_prob) control overfitting risks but can hinder learning if set too high. Pretraining vs. Fine-tuning: type_vocab_size is essential for tasks requiring segment embeddings (e.g., sentence pairs). Stability and Efficiency: layer_norm_eps ensures stable training, while initializer_range affects convergence. References Paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding DOI: 10.48550&#x2F;arXiv.1810.04805 Read on Hugging Face BERT Documentation Challenges and reports on configuration Report: “On the Structural Properties of BERT Models” (Kovaleva et al., 2019). Challenges: Over-parameterization and inefficiency in fine-tuning for domain-specific tasks. Playgrounds to experiment Hugging Face Spaces: https://huggingface.co/spaces Google Colab with Transformers: https://colab.research.google.com/ OpenAI Playground for Text Understanding: https://platform.openai.com/playground","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"LLM","slug":"LLM","permalink":"https://ooge0.github.io/hexo-blog/tags/LLM/"},{"name":"BERT","slug":"BERT","permalink":"https://ooge0.github.io/hexo-blog/tags/BERT/"}]},{"title":"GPT-2 configuration parameters overview","slug":"post_ai_llm__gpt2_configuration_parameters_overview","date":"2024-12-04T18:08:12.000Z","updated":"2024-12-12T10:55:43.394Z","comments":true,"path":"2024/12/04/post_ai_llm__gpt2_configuration_parameters_overview/","permalink":"https://ooge0.github.io/hexo-blog/2024/12/04/post_ai_llm__gpt2_configuration_parameters_overview/","excerpt":"","text":"This post related to: Techniques for Handling Context in AI Models BART configuration parameters overview Parameters List and Descriptions Parameter Description Data Type&#x2F;Options n_ctx The context size or maximum length of input tokens the model can process. Integer, e.g., 1024, 2048. n_embd Dimensionality of the embeddings. Determines the size of the word and positional embeddings. Integer, typically 768, 1024, or 1280. n_layer Number of transformer layers in the model. Dictates depth of the network. Integer, e.g., 12, 24, 48. n_head Number of attention heads in each transformer layer. Reflects parallel attention mechanisms. Integer, typically a divisor of n_embd such as 12 or 16. vocab_size Size of the vocabulary for the tokenizer. Represents the range of token IDs the model can handle. Integer, e.g., 50257. activation_function The activation function used in feedforward layers (e.g., “gelu”). Affects non-linearity. String: relu, gelu, tanh, sigmoid, etc. resid_pdrop Dropout probability for residual connections. Adds regularization. Float between 0.0 and 1.0. Typically 0.1. embd_pdrop Dropout probability for embeddings. Helps prevent overfitting. Float between 0.0 and 1.0. Typically 0.1. attn_pdrop Dropout probability in the attention mechanism. Ensures robustness. Float between 0.0 and 1.0. Typically 0.1. initializer_range The range of the uniform distribution for weight initialization. Float, e.g., 0.02. layer_norm_epsilon A small value added for numerical stability in layer normalization. Float, typically 1e-5. use_cache Whether the model uses cached key&#x2F;values for faster generation during inference. Boolean, true or false. bos_token_id Token ID for the beginning-of-sequence token. Integer, e.g., 50256. eos_token_id Token ID for the end-of-sequence token. Integer, e.g., 50256. scale_attn_weights Whether to scale the attention weights. Boolean, true or false. output_hidden_states Whether to return all hidden states from each layer during inference. Boolean, true or false. output_attentions Whether to return the attention weights during inference. Boolean, true or false. tie_word_embeddings Whether to tie the input and output word embeddings. Boolean, true or false. Summary of Parameter ImpactHow changes reflect on model behavior: Model Size and Computation: Increasing n_layer, n_embd, or n_head leads to larger and more computationally intensive models but potentially improves learning capacity. Reducing n_ctx limits the model’s ability to process long inputs. Regularization: Dropout parameters (resid_pdrop, embd_pdrop, attn_pdrop) mitigate overfitting but may hinder performance if too high. Non-linearity: The choice of activation_function (e.g., gelu vs. relu) affects gradient behavior and optimization efficiency. Stability: Small layer_norm_epsilon values ensure numerical stability during normalization but may require tuning based on the architecture. Flexibility: Enabling output_hidden_states or output_attentions increases interpretability but may slow inference. References GPT-2 Paper Hugging Face Documentation Challenges and reports on configuration Report: “Language Models are Few-Shot Learners” (Brown et al., 2020) discusses scalability challenges in transformer-based architectures. Challenges: Balancing model depth and breadth while maintaining computational efficiency. Playgrounds to experiment OpenAI Playground: https://platform.openai.com/playground Hugging Face Spaces: https://huggingface.co/spaces Google Colab with Transformers: https://colab.research.google.com/","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"LLM","slug":"LLM","permalink":"https://ooge0.github.io/hexo-blog/tags/LLM/"},{"name":"GPT2","slug":"GPT2","permalink":"https://ooge0.github.io/hexo-blog/tags/GPT2/"},{"name":"NN","slug":"NN","permalink":"https://ooge0.github.io/hexo-blog/tags/NN/"}]},{"title":"NLTK general","slug":"post_ai_nltk__general","date":"2024-12-04T09:21:11.000Z","updated":"2024-12-04T10:38:11.781Z","comments":true,"path":"2024/12/04/post_ai_nltk__general/","permalink":"https://ooge0.github.io/hexo-blog/2024/12/04/post_ai_nltk__general/","excerpt":"","text":"NLTK - Natural Language Toolkit for Natural Language Text Processing (NLTP) General Example usage of NLTK modules NLTK WIKI NLTK WIKI Projects NLTK FAQ NLTK Interactive online demos NLTK booksNatural Language Processing with Python – Analyzing Text with the Natural Language Toolkit. Steven Bird, Ewan Klein, and Edward Loper NLTK coursesNLTK courses About ‘Natural Language Text Processing’Natural Language Text Processing encompasses a range of techniques and tools to analyze, manipulate, and understand human language in text form. Below is a detailed explanation of key terms, their technical details, and implementation options in NLP. Natural Language Text Processing include but not limited by sentiment analysis, which uses text classification to determine sentiment polarity, word tokenization , stemming text, speech tagging using speech taggers, chunk extraction and named entity recognition. Sentiment AnalysisDefinition: Sentiment Analysis determines the emotional tone behind a body of text. It classifies text as positive, negative, or neutral. Approaches: Lexicon-Based Methods: Use predefined dictionaries of positive and negative words. Examples: SentiWordNet, VADER. Machine Learning-Based Methods: Train a model on labeled datasets to classify sentiments. Examples: Naive Bayes, Support Vector Machines (SVM). Deep Learning Methods: Utilize neural networks like RNNs, LSTMs, or transformers. Examples: BERT, RoBERTa, DistilBERT. Word TokenizationDefinition: The process of splitting a sentence or paragraph into individual words or tokens. Options: Rule-Based Tokenization: Uses language-specific rules to split text. Example Tools: NLTK, SpaCy. Statistical Tokenization: Employs probabilistic models for token boundaries. Examples: Punkt tokenizer. Subword Tokenization: Splits text into subwords to handle rare words. Examples: Byte Pair Encoding (BPE), WordPiece (used in BERT). StemmingDefinition: Reduces words to their base or root form (e.g., “running” → “run”). Methods: Porter Stemmer: Algorithmic and rule-based. Lancaster Stemmer: Faster but more aggressive. Snowball Stemmer: Improved version of Porter. Usage: Common in search engines and text indexing. Speech TaggingDefinition: Assigning parts of speech (POS) tags (e.g., noun, verb) to each word in a text. Taggers: Rule-Based POS Tagging: Uses manually crafted rules. Statistical POS Tagging: Relies on probabilistic models (e.g., Hidden Markov Models). Neural POS Tagging: Utilizes neural networks for higher accuracy. Example Tools: NLTK POS Tagger, SpaCy. Chunk ExtractionDefinition: Identifies and groups related words (e.g., noun phrases, verb phrases). Types: Shallow Parsing: Focuses on high-level phrase detection. Dependency Parsing: Analyzes grammatical structure by identifying relationships between words. Example Tools: OpenNLP, CoreNLP. Named Entity Recognition (NER)Definition: Identifies and categorizes entities in text (e.g., names, organizations, dates). NER Types: Rule-Based NER: Uses pattern-matching rules. Examples: Regular Expressions. Statistical NER: Trains models on labeled entity datasets. Examples: Conditional Random Fields (CRF). Neural NER: Deep learning-based methods for context understanding. Examples: SpaCy, Flair, Hugging Face. Implementation in NLP Libraries and Frameworks: NLTK: A foundational library for tokenization, stemming, and POS tagging. SpaCy: Industrial-strength NLP with support for tokenization, POS tagging, NER, etc. Transformers (Hugging Face): Pre-trained models for sentiment analysis, NER, and more. CoreNLP: Comprehensive NLP suite by Stanford. Use Cases: Sentiment analysis in social media monitoring. Tokenization in machine translation. NER for information extraction from documents. Related to this topic Natural Language Processing | Stanford NLP Understanding NLP Techniques | Hugging Face NLTK Official Documentation","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"NLTK","slug":"NLTK","permalink":"https://ooge0.github.io/hexo-blog/tags/NLTK/"}]},{"title":"Neural Process Family","slug":"post_ai_ml_npf__general","date":"2024-11-30T14:45:33.000Z","updated":"2024-11-30T15:18:33.089Z","comments":true,"path":"2024/11/30/post_ai_ml_npf__general/","permalink":"https://ooge0.github.io/hexo-blog/2024/11/30/post_ai_ml_npf__general/","excerpt":"","text":"The Neural Process Family refers to a class of models designed to learn distributions over functions, offering a blend of the expressiveness of deep learning and the flexibility of probabilistic models. These models are particularly useful for tasks requiring uncertainty quantification, few-shot learning, and function estimation. Key members of this family include Neural Processes (NPs), Conditional Neural Processes (CNPs), Attentive Neural Processes (ANPs), and extensions like ConvCNPs and Variational NPs (VNPs). Core Concepts Probabilistic Nature:Neural Processes learn distributions over functions. Given a set of input-output pairs, they can predict the probability distribution of outputs for new inputs, making them suitable for uncertainty estimation. Few-Shot Learning:These models can make predictions given only a few examples, making them ideal for problems where data is scarce. Model Components: Encoder: Maps input-output pairs to a latent representation. Decoder: Takes the latent representation and generates outputs for given inputs. Latent Space: Captures the uncertainty and variability in the function space. Meta-Learning:NPs can generalize across tasks by learning a distribution over tasks, enabling them to perform well on unseen tasks after being trained on related ones. Variants of Neural Processes Conditional Neural Processes (CNPs): A deterministic model that learns to map a context set of input-output pairs to predictions for new inputs. Simple and efficient but limited in capturing uncertainty in the underlying function. Neural Processes (NPs): Adds a latent variable to model uncertainty explicitly, making it a probabilistic counterpart to CNPs. Balances flexibility and computational efficiency. Attentive Neural Processes (ANPs): Introduces attention mechanisms to improve modeling of relationships between context points and query points. Addresses issues with poor extrapolation and oversmoothing in standard NPs. Convolutional Neural Processes (ConvCNPs): Leverages convolutional architectures for tasks like image generation, capturing local correlations more effectively. Variational Neural Processes (VNPs): Focuses on improved variational inference techniques to better approximate the posterior distribution over functions. Applications Regression:Modeling functions with uncertainty, e.g., Bayesian regression tasks. Few-Shot Classification:Classifying data with limited examples by modeling task distributions. Spatio-Temporal Data:Applications in time-series forecasting and spatial predictions. Reinforcement Learning:Modeling uncertainty in reward functions or dynamics. Image Completion:Predicting missing pixels in images. Strengths and ChallengesStrengths: Scalability due to neural networks. Probabilistic outputs allow uncertainty estimation. Adaptable across domains with minimal changes. Challenges: Trade-off between computational cost and flexibility. Dependence on good representation learning. Overcoming limitations of context aggregation in high-dimensional tasks. The Neural Process Family continues to evolve, with active research aimed at improving its scalability, expressiveness, and applications to real-world problems. About: yanndubs.github.io | The Neural Process Family notes.theomorales.com | The Neural Process Family Papers: Papers: Title: The Neural Process Family: Survey, Applicationsand Perspectives DOI: 10.48550&#x2F;arXiv.2209.00517 Read on arxiv.org","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"ML","slug":"ML","permalink":"https://ooge0.github.io/hexo-blog/tags/ML/"},{"name":"NPF","slug":"NPF","permalink":"https://ooge0.github.io/hexo-blog/tags/NPF/"}]},{"title":"Machine Learning.Teach by Doing(LinkedIn post)","slug":"post_ai_ml__machine_learning_teach_by_doing","date":"2024-11-30T14:33:33.000Z","updated":"2024-11-30T14:38:34.347Z","comments":true,"path":"2024/11/30/post_ai_ml__machine_learning_teach_by_doing/","permalink":"https://ooge0.github.io/hexo-blog/2024/11/30/post_ai_ml__machine_learning_teach_by_doing/","excerpt":"","text":"This is reopst of LinkedIN post. Current post contains list of references and some additional detilas I(Author) started the Machine Learning: Teach by Doing series to transfer my learnings to those who want to transition to Machine Learning. I(Author) have recorded 37 videos in the past 6 months. Here are the links for you to learn: Introduction to Machine Learning Teach by Doing: https://lnkd.in/gqN2PMX5 What is Machine Learning? History of Machine Learning: https://lnkd.in/gvpNSAKh Types of ML Models: https://lnkd.in/gSy2mChM 6 steps of any ML project: https://lnkd.in/ggCGchPQ Install Python and VSCode and run your first code: https://lnkd.in/gyic7J7b Linear Classifiers Part 1: https://lnkd.in/gYdfD97D Linear Classifiers Part 2: https://lnkd.in/gac_z-G8 Jupyter Notebook, Numpy and Scikit-Learn: https://lnkd.in/gWRaC_tB Running the Random Linear Classifier Algorithm in Python: https://lnkd.in/g5HacbFC The oldest ML model - Perceptron: https://lnkd.in/gpce6uFt Coding the Perceptron: https://lnkd.in/gmz-XjNK Perceptron Convergence Theorem: https://lnkd.in/gmz-XjNK Magic of features in Machine Learning: https://lnkd.in/gCeDRb3g One hot encoding: https://lnkd.in/g3WfRQGQ Logistic Regression Part 1: https://lnkd.in/gTgZAAZn Cross Entropy Loss: https://lnkd.in/g3Ywg_2p How gradient descent works: https://lnkd.in/gKBAsazF Logistic Regression from scratch in Python: https://lnkd.in/g8iZh27P Introduction to Regularization: https://lnkd.in/gjM9pVw2 Implementing Regularization in Python: https://lnkd.in/gRnSK4v4 Linear Regression Introduction: https://lnkd.in/gPYtSPJ9 Ordinary Least Squares step by step implementation: https://lnkd.in/gnWQdgNy Ridge regression fundamentals and intuition: https://lnkd.in/gE5M-CSM Regression recap for interviews: https://lnkd.in/gNBWzzWv Neural network architecture in 30 minutes: https://lnkd.in/g7qSrkxG Backpropagation intuition: https://lnkd.in/gAmBARHm Neural network activation functions: https://lnkd.in/gqrC3zDP Momentum in gradient descent: https://lnkd.in/g3M4qhbP Hands on neural network training in Python: https://lnkd.in/gz-fTBxs Introduction to Convolutional Neural Networks (CNNs.: https://lnkd.in/gpmuBm3j Filters in 1D and the Convolution Operation: https://lnkd.in/gEDaKHDU Filters in 2D, Channels and Feature Identification: https://lnkd.in/g3Gf_4ia Filtering Layers in Convolutional Neural Networks: https://lnkd.in/gUaiBkTu What is Max Pooling in Convolutional Neural Networks?: https://lnkd.in/gGRGy6wq CNN Architecture explained: https://lnkd.in/gPQvRh9i Backpropagation in Convolutional Neural Networks: https://lnkd.in/g942G6zv Build your own brain tumor classification CNN application in Python: https://lnkd.in/gQB5zRGk Join our AI live lectures waitlist here: https://lnkd.in/gDcHZdHg","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"ML","slug":"ML","permalink":"https://ooge0.github.io/hexo-blog/tags/ML/"}]},{"title":"Parsing web site for job offers","slug":"post_data_mining__parsing_web_site_for_job_offers","date":"2024-11-27T22:49:11.000Z","updated":"2024-11-27T23:20:32.939Z","comments":true,"path":"2024/11/28/post_data_mining__parsing_web_site_for_job_offers/","permalink":"https://ooge0.github.io/hexo-blog/2024/11/28/post_data_mining__parsing_web_site_for_job_offers/","excerpt":"","text":"Given The website with posted offers.Goal: to get information from the website using python, BeautifulSoup and save it in JSON and markdown files. Python scirpt Install and import required packages 12import jsonfrom bs4 import BeautifulSoup 123456789101112131415161718192021222324252627282930313233343536373839def execute_requests(base_url, amount_of_pages): &quot;&quot;&quot; Executes GET requests for the specified number of pages and returns the responses. Args: base_url (str): The base URL for requests. amount_of_pages (int): The number of pages to fetch. Returns: list: A list of dictionaries containing the request number, page counter, and response content. &quot;&quot;&quot; payload = &#123;&#125; headers = &#123; &#x27;Cookie&#x27;: &#x27;_jobboard_session=895b7b35b6493519c3ad686923d8cc1d; __cf_bm=BrUISN3QZXOQ1BPeJXOvNpqBAcT8YXS6XqIr7jlW.4M-1732742386-1.0.1.1-1hk8BgPrEcPEO6ZLFO6W6W6e8MNhcmQswlF6K2dUhchBp0px3reiDJPOuXnzX6z.etyzZq.Q9BIUEYJ1dHZqP75g&#x27; &#125; responses_data = [] # Initialize an empty list to store response data for counter in range(1, amount_of_pages + 1): # Construct the URL with the current page counter url = f&quot;&#123;base_url&#125;&amp;page=&#123;counter&#125;&quot; print(f&quot;Fetching data from: &#123;url&#125;&quot;) try: # Send GET request response = requests.get(url, headers=headers, data=payload) # Append the response data to the list responses_data.append(&#123; &quot;request_key&quot;: f&quot;request_&#123;counter&#125;&quot;, &quot;counter&quot;: counter, &quot;response_content&quot;: response.text &#125;) print(f&quot;counter: &#123;counter&#125; | &#x27;status_code:&#x27; &#123;response.status_code&#125;&quot;) except requests.RequestException as e: print(f&quot;Error fetching data for page &#123;counter&#125;: &#123;e&#125;&quot;) return responses_data Parse data from json 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667def parse_job_data_from_json(response_data, output_json_file, output_markdown_file): &quot;&quot;&quot; Parse job data from a list of responses and extract job listings using BeautifulSoup. Save results to both a JSON file and a Markdown file. Args: response_data (list): List of dictionaries containing the response data. output_json_file (str): Path to save the parsed job data in JSON format. output_markdown_file (str): Path to save the parsed job data in Markdown format. &quot;&quot;&quot; try: job_data = [] # List to store extracted job data markdown_content = [] # List to store Markdown entries # Loop through each request in the list for request in response_data: counter = request.get(&quot;counter&quot;, &quot;unknown&quot;) response_content = request.get(&quot;response_content&quot;, &quot;&quot;) # Parse the HTML content using BeautifulSoup soup = BeautifulSoup(response_content, &#x27;html.parser&#x27;) # Find all job listings using the locator job_listings = soup.find_all(&#x27;li&#x27;, class_=&#x27;job-listing&#x27;) # Extract data from each job listing for job in job_listings: # Safely find required elements, fallback to &#x27;N/A&#x27; if not present job_title = job.find(&#x27;a&#x27;, class_=&#x27;jobList-title zip-backfill-link&#x27;) job_description = job.find(&#x27;div&#x27;, class_=&#x27;jobList-description&#x27;) salary = job.find(&#x27;div&#x27;, class_=&#x27;jobList-salary&#x27;) job_info = &#123; &#x27;title&#x27;: job_title.text.strip() if job_title else &#x27;N/A&#x27;, &#x27;href&#x27;: job_title[&#x27;href&#x27;] if job_title else &#x27;N/A&#x27;, &#x27;description&#x27;: job_description.text.strip() if job_description else &#x27;N/A&#x27;, &#x27;salary&#x27;: salary.text.strip() if salary else &#x27;N/A&#x27;, &#x27;page&#x27;: counter &#125; job_data.append(job_info) # Prepare entry for Markdown markdown_entry = f&quot;&quot;&quot;### Job Title: &#123;job_info[&#x27;title&#x27;]&#125;- **Link**: [&#123;job_info[&#x27;title&#x27;]&#125;](&#123;job_info[&#x27;href&#x27;]&#125;)- **Description**: &#123;job_info[&#x27;description&#x27;]&#125;- **Salary**: &#123;job_info[&#x27;salary&#x27;]&#125;- **Page**: &#123;job_info[&#x27;page&#x27;]&#125; &quot;&quot;&quot; markdown_content.append(markdown_entry.strip()) # Save extracted job data to a new JSON file with open(output_json_file, &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as f: json.dump(job_data, f, ensure_ascii=False, indent=4) print(f&quot;Job data successfully parsed and saved to &#123;output_json_file&#125;&quot;) # Save Markdown content to a file with open(output_markdown_file, &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as f: f.write(&quot;\\n\\n&quot;.join(markdown_content)) print(f&quot;Job data successfully saved to &#123;output_markdown_file&#125;&quot;) except Exception as e: print(f&quot;Error processing file: &#123;e&#125;&quot;) IMPORTANT ! Provide valid references for saving retrieved data. Make sure that you copied valid url from the browser and manage pagination properly. 1234567891011if __name__ == &#x27;__main__&#x27;: base_url = &quot;https://www.ziprecruiter.co.uk/jobs/search?l=Remote&amp;q=qa+software+engineer&amp;remote=full&quot; amount_of_pages = 100 # Or any number that you wish to check responses_data = execute_requests(base_url, amount_of_pages) # Output files for parsed data output_json_file = &#x27;parsed_job_data.json&#x27; output_markdown_file = &#x27;parsed_job_data.md&#x27; # Parse and save the job data parse_job_data_from_json(responses_data, output_json_file, output_markdown_file) Script works fine for several executions. After that cookies expired and new one should be regenerated.","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"data_mining","slug":"data-mining","permalink":"https://ooge0.github.io/hexo-blog/tags/data-mining/"},{"name":"parsing","slug":"parsing","permalink":"https://ooge0.github.io/hexo-blog/tags/parsing/"},{"name":"python","slug":"python","permalink":"https://ooge0.github.io/hexo-blog/tags/python/"}]},{"title":"VADER - intro","slug":"post_ai__vader_intro","date":"2024-11-26T19:37:30.000Z","updated":"2024-11-26T20:01:47.805Z","comments":true,"path":"2024/11/26/post_ai__vader_intro/","permalink":"https://ooge0.github.io/hexo-blog/2024/11/26/post_ai__vader_intro/","excerpt":"","text":"VADERVADER (Valence Aware Dictionary and sEntiment Reasoner) is a widely used sentiment analysis tool tailored for understanding emotions in text, especially in social media contexts. Developed by C.J. Hutto and Eric Gilbert, it combines lexicon-based methods with grammatical and syntactical rules, offering precise sentiment analysis. VADER excels at capturing sentiment intensity, polarity (positive, negative, neutral), and even nuances like sarcasm, thanks to empirically validated linguistic rules and datasets. Originally presented at the Eighth International Conference on Weblogs and Social Media in 2014, VADER was designed for scalability and ease of use. Its open-source implementation in Python is accessible for various applications, from marketing analysis to social media monitoring. The tool incorporates features like emoticons, slang, and acronyms, making it uniquely adept at analyzing informal text. Users can install it via Python’s pip command, and the source code is freely available under the MIT License. The tool has been validated rigorously with human raters to ensure accuracy. Datasets like tweets, movie reviews, and editorial snippets were used for its development, enabling a robust understanding of diverse text formats. For official details, you can visit VADER’s documentation: VADER Sentiment. Releated resources Medium post : A Short Introduction to VADER Paper: VADER: A Parsimonious Rule-Based Model for Sentiment Analysis of Social Media Text. 2014 DOI: 10.1609&#x2F;icwsm.v8i1.14550 Read on ojs.aaai.org Article: VADER sentiment analysis NLTK module: nltk.sentiment.vader module","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"VADER","slug":"VADER","permalink":"https://ooge0.github.io/hexo-blog/tags/VADER/"},{"name":"sentiment_analysis","slug":"sentiment-analysis","permalink":"https://ooge0.github.io/hexo-blog/tags/sentiment-analysis/"}]},{"title":"Evolution of Text Augmentation in NLP","slug":"post_ai_nlp__evolution_of_text_augmentation_in_nlp","date":"2024-11-24T23:03:11.000Z","updated":"2024-11-25T10:28:07.976Z","comments":true,"path":"2024/11/25/post_ai_nlp__evolution_of_text_augmentation_in_nlp/","permalink":"https://ooge0.github.io/hexo-blog/2024/11/25/post_ai_nlp__evolution_of_text_augmentation_in_nlp/","excerpt":"","text":"Text augmentation has evolved alongside advancements in natural language processing (NLP), enabling robust data generation and model improvement. Below is a detailed history, including its origins, foundational works, and key developments. Origins of Development: Pre-Digital Era (1940s–1960s)Discovery: The foundations of text augmentation trace back to linguistic research and early computational experiments. Theoretical frameworks like Noam Chomsky’s generative grammar established the principles of sentence structure and transformation. Significance: These linguistic theories formed the basis for later computational methods for generating diverse text variations. Book: Syntactic Structures. Noam Chomsky. 1957. Read on MIT Press Syntactic Structures. Noam Chomsky. 2nd edition. 2022 (with introduction by David Lightfoot) Read on tallinzen.net Papers: A Mathematical Theory of Communication. Shannon, C. E. (1948). Bell System Technical Journal, 27(3), 379–423. DOI: 10.1002&#x2F;j.1538-7305.1948.tb01338.x Read on sci-hub.se Three models for the description of language. Chomsky, N. (1956). IEEE Transactions on Information Theory, 2(3), 113–124. DOI: 10.1109&#x2F;TIT.1956.1056813 Read on sci-hub.se Syntactic Structures. Language, Lees, R. B., &amp; Chomsky, N. (1957). 33(3), 375. DOI:10.2307&#x2F;411160 Read on sci-hub.se Early Rule-Based Methods (1960s–1980s)Discovery: Rule-based systems emerged as the first computational attempt to augment text. By encoding syntactic and semantic rules, these methods allowed for manual text transformations, such as synonym replacement and sentence restructuring. Significance: These approaches demonstrated how structured transformations could enrich NLP tasks like translation and summarization. Paper: Computational Semantics for Natural Language Processing. Yorick Wilks. 1972. DOI: 10.1145&#x2F;1234567 Read on ACM Emergence of Statistical Methods (1990s)Discovery: Statistical NLP introduced probabilistic models such as n-grams and Hidden Markov Models (HMMs), enabling dynamic text generation. Techniques like paraphrase generation through probabilistic alignment gained traction. Significance: The shift to statistical methods increased scalability and adaptability, marking a transition from deterministic rules to data-driven approaches. Paper: A Statistical Approach to Machine Translation. Brown et al. 1990. DOI: 10.1162&#x2F;089120100750105975 Read on aclanthology.org Fundamental Work: Foundations of Statistical Natural Language Processing. Manning &amp; Schütze. 1999. DOI: N&#x2F;A Read on web.stanford.edu Word Embeddings and Neural Networks (2000s–2010s)Discovery: Embedding-based models like Word2Vec and GloVe enabled semantic-aware text augmentation, where words with similar meanings were mapped closer in vector space. Neural networks introduced deeper, context-aware text manipulation. Significance: Word embeddings made synonym substitution and paraphrasing more semantically relevant, while neural networks added contextual depth. Paper: Distributed Representations of Words and Phrases and Their Compositionality. Mikolov et al. 2013. DOI: 10.1162&#x2F;153244303322533223 Read on arXiv Transformer Revolution (2017–Present)Discovery: Transformers like BERT, GPT, and T5 redefined NLP, introducing powerful models for context-aware text augmentation. Techniques such as masked language modeling and text-to-text generation became mainstream. Significance: The transformer architecture allowed for high-quality, large-scale text augmentation, driving state-of-the-art performance in multiple NLP tasks. Paper: Attention Is All You Need. Vaswani et al. 2017. DOI: 10.48550&#x2F;arXiv.1706.03762 Read on arXiv Paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Devlin et al. 2018. DOI: 10.48550&#x2F;arXiv.1810.04805 Read on arXiv Modern NLP Data Augmentation Libraries (2020s)Discovery: The development of augmentation libraries such as nlpaug, TextAttack, and EDA (Easy Data Augmentation) simplified access to advanced techniques like back-translation, synonym replacement, and adversarial generation. Significance: These tools democratized text augmentation, making sophisticated methods accessible for both research and industry. Paper: TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Model Training. Morris et al. 2020. DOI: 10.48550&#x2F;arXiv.2005.05909 Read on arXiv Paper: EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks. Wei &amp; Zou. 2019. DOI: 10.48550&#x2F;arXiv.1901.11196 Read on arXiv ConclusionText augmentation has evolved from manual rules to cutting-edge neural models and accessible libraries. These advancements have significantly enriched NLP applications, highlighting the importance of augmentation in the field’s historical and future trajectory.","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"NLP","slug":"NLP","permalink":"https://ooge0.github.io/hexo-blog/tags/NLP/"},{"name":"lexic","slug":"lexic","permalink":"https://ooge0.github.io/hexo-blog/tags/lexic/"}]},{"title":"NLP lexical resources","slug":"post_ai_nlp__nlp_lexical_resources","date":"2024-11-24T23:03:11.000Z","updated":"2024-11-24T23:15:07.116Z","comments":true,"path":"2024/11/25/post_ai_nlp__nlp_lexical_resources/","permalink":"https://ooge0.github.io/hexo-blog/2024/11/25/post_ai_nlp__nlp_lexical_resources/","excerpt":"","text":"WordNetResource: https://wordnet.princeton.edu/WordNet® is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations. The resulting network of meaningfully related words and concepts can be navigated with the browser(Link is external). WordNet is also freely and publicly available for download. WordNet’s structure makes it a useful tool for computational linguistics and natural language processing. Glitch Text GeneratorResource: https://glyphy.io/font-generator/glitch-textUse our glitch text generator to design creepy text for your social media accounts. Copy and paste these cursed fonts to add some weirdness to your profiles! Corrupted-Text Python LibraryA python library to generate out-of-distribution text datasets. Specifically, the library applies model-independent, commonplace corruptions (not model-specific, worst-case adversarial corruptions). We thus aim to allow benchmark-studies regarding robustness against realistic outliers.PIPpip install corrupted-textArticle: Text Augmentation Using Corrupted-Text Python Library TensorFlow Data Augmentation APIGuide: Text and natural language processing with TensorFlow","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"NLP","slug":"NLP","permalink":"https://ooge0.github.io/hexo-blog/tags/NLP/"},{"name":"lexic","slug":"lexic","permalink":"https://ooge0.github.io/hexo-blog/tags/lexic/"}]},{"title":"Sentiment analysis framework","slug":"post_ai_nlp__sentiment_analysis_framework","date":"2024-11-24T23:03:11.000Z","updated":"2024-11-27T23:26:07.554Z","comments":true,"path":"2024/11/25/post_ai_nlp__sentiment_analysis_framework/","permalink":"https://ooge0.github.io/hexo-blog/2024/11/25/post_ai_nlp__sentiment_analysis_framework/","excerpt":"","text":"INTRO Here is a draft structure of Python-based project with an OOP design. It focuses on sentiment analysis for text files stored in a nested directory. The design incorporates multiple sentiment analysis frameworks and flexible configurations for each, while storing results in a JSON file. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175import osfrom abc import ABC, abstractmethodclass ConfigManager: &quot;&quot;&quot; Responsible for loading and managing configuration files for different sentiment analysis frameworks. &quot;&quot;&quot; def __init__(self, config_dir: str): self.config_dir = config_dir def load_config(self, framework_name: str) -&gt; dict: &quot;&quot;&quot; Loads configuration for the specified framework. :param framework_name: Name of the sentiment analysis framework. :return: Dictionary with configuration parameters. &quot;&quot;&quot; # Placeholder for loading configuration logic passclass TextFileProcessor: &quot;&quot;&quot; Handles discovery and reading of text files from the nested directory. &quot;&quot;&quot; def __init__(self, base_dir: str): self.base_dir = base_dir def get_text_files(self) -&gt; list: &quot;&quot;&quot; Recursively fetches all text files in the nested directory. :return: List of file paths. &quot;&quot;&quot; # Placeholder for file discovery logic pass def read_file(self, file_path: str) -&gt; str: &quot;&quot;&quot; Reads the content of a text file. :param file_path: Path to the text file. :return: File content as a string. &quot;&quot;&quot; # Placeholder for file reading logic passclass SentimentAnalyzer(ABC): &quot;&quot;&quot; Abstract base class for all sentiment analysis frameworks. &quot;&quot;&quot; def __init__(self, config: dict): self.config = config @abstractmethod def analyze(self, text: str) -&gt; dict: &quot;&quot;&quot; Analyzes the sentiment of the provided text. :param text: Input text for sentiment analysis. :return: Dictionary containing analysis metrics. &quot;&quot;&quot; passclass FrameworkAAnalyzer(SentimentAnalyzer): &quot;&quot;&quot; Implements sentiment analysis using Framework A. &quot;&quot;&quot; def analyze(self, text: str) -&gt; dict: &quot;&quot;&quot; Uses Framework A to analyze sentiment. :param text: Input text for sentiment analysis. :return: Dictionary with metrics from Framework A. &quot;&quot;&quot; # Placeholder for sentiment analysis logic passclass FrameworkBAnalyzer(SentimentAnalyzer): &quot;&quot;&quot; Implements sentiment analysis using Framework B. &quot;&quot;&quot; def analyze(self, text: str) -&gt; dict: &quot;&quot;&quot; Uses Framework B to analyze sentiment. :param text: Input text for sentiment analysis. :return: Dictionary with metrics from Framework B. &quot;&quot;&quot; # Placeholder for sentiment analysis logic passclass AnalysisManager: &quot;&quot;&quot; Coordinates the sentiment analysis process. &quot;&quot;&quot; def __init__(self, config_manager: ConfigManager, text_processor: TextFileProcessor): self.config_manager = config_manager self.text_processor = text_processor def analyze_files(self, frameworks: list) -&gt; list: &quot;&quot;&quot; Analyzes all text files using specified frameworks. :param frameworks: List of framework analyzer instances. :return: List of results containing file names and analysis metrics. &quot;&quot;&quot; results = [] text_files = self.text_processor.get_text_files() for file_path in text_files: content = self.text_processor.read_file(file_path) metrics = [] for framework in frameworks: metrics.append(framework.analyze(content)) results.append(&#123; &quot;file_name&quot;: os.path.basename(file_path), &quot;description&quot;: &quot;Sentiment analysis results&quot;, &quot;metrics&quot;: metrics &#125;) return resultsclass ResultSaver: &quot;&quot;&quot; Saves the analysis results to a JSON file. &quot;&quot;&quot; def save_to_json(self, results: list, output_file: str): &quot;&quot;&quot; Writes results to a JSON file. :param results: List of analysis results. :param output_file: Path to the output JSON file. &quot;&quot;&quot; # Placeholder for JSON writing logic passclass Main: &quot;&quot;&quot; Entry point for the sentiment analysis project. &quot;&quot;&quot; def __init__(self, base_dir: str, config_dir: str, output_file: str): self.base_dir = base_dir self.config_dir = config_dir self.output_file = output_file def run(self): &quot;&quot;&quot; Executes the sentiment analysis pipeline. &quot;&quot;&quot; # Initialize components config_manager = ConfigManager(self.config_dir) text_processor = TextFileProcessor(self.base_dir) # Load configurations and instantiate frameworks frameworks = [ FrameworkAAnalyzer(config_manager.load_config(&quot;FrameworkA&quot;)), FrameworkBAnalyzer(config_manager.load_config(&quot;FrameworkB&quot;)) ] # Perform analysis analysis_manager = AnalysisManager(config_manager, text_processor) results = analysis_manager.analyze_files(frameworks) # Save results saver = ResultSaver() saver.save_to_json(results, self.output_file) All implementations and improvements will be presented in separate posts.","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://ooge0.github.io/hexo-blog/tags/NLP/"},{"name":"sentiment_analysis","slug":"sentiment-analysis","permalink":"https://ooge0.github.io/hexo-blog/tags/sentiment-analysis/"}]},{"title":"Types of sentiment analysis techniques in NLP","slug":"post_ai_nlp__types_of_sentiment_analysis_techniques_in_nlp","date":"2024-11-21T18:01:11.000Z","updated":"2024-11-26T20:11:53.601Z","comments":true,"path":"2024/11/21/post_ai_nlp__types_of_sentiment_analysis_techniques_in_nlp/","permalink":"https://ooge0.github.io/hexo-blog/2024/11/21/post_ai_nlp__types_of_sentiment_analysis_techniques_in_nlp/","excerpt":"","text":"related to : LLM system prompts Types of Sentiment Analysis Techniques for NLP (with DOI Papers)This post summarizes various sentiment analysis techniques, from lexicon-based methods to advanced deep learning approaches, along with DOI references to explore these concepts further. 1. Lexicon-Based Sentiment Analysis Description: Relies on predefined sentiment lexicons (dictionaries of positive, negative, and neutral words). Sentiment scores are calculated based on the presence and frequency of words. Applications: Basic polarity detection, customer feedback analysis. Key Paper:Liu, B. (2012). Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies, 5(1), 1–167.DOI: 10.2200&#x2F;S00416ED1V01Y201204HLT016Read on www.cs.uic.edu 2. Rule-Based Sentiment Analysis Description: Combines sentiment lexicons with linguistic rules (e.g., negation handling, intensifiers). Example: “not good” &#x3D; negative, “very bad” &#x3D; strongly negative. Applications: Sentiment classification for rule-governed domains. Key Paper:Hutto, C., &amp; Gilbert, E. (2014). VADER: A Parsimonious Rule-Based Model for Sentiment Analysis of Social Media Text. Proceedings of the Eighth International AAAI Conference on Weblogs and Social Media.DOI: 10.1609&#x2F;icwsm.v8i1.14550Read on ojs.aaai.org 3. Machine Learning-Based Sentiment Analysis Description: Uses ML models like Naive Bayes, SVM, or Decision Trees trained on labeled sentiment datasets. Applications: News sentiment analysis, customer reviews, product recommendations. Key Paper:Pang, B., &amp; Lee, L. (2002). Thumbs Up? Sentiment Classification Using Machine Learning Techniques. Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing (EMNLP), 10, 79–86.DOI: 10.3115&#x2F;1118693.1118704Read on dl.acm.org 4. Deep Learning-Based Sentiment Analysis Description: Uses neural networks (CNNs, RNNs, LSTMs, Transformers) to analyze sentiment from raw text. Applications: Social media analysis, multilingual sentiment detection. Key Papers: Socher, R., et al. (2013). Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. Proceedings of EMNLP 2013.Read on nlp.stanford.edu Vaswani, A., et al. (2017). Attention Is All You Need. Advances in Neural Information Processing Systems (NeurIPS).DOI: 10.48550&#x2F;arXiv.1706.03762Read on arxiv.org 5. Aspect-Based Sentiment Analysis (ABSA) Description: Focuses on sentiment specific to aspects of a product or service (e.g., food, service in restaurant reviews). Applications: E-commerce reviews, detailed product feedback. Key Paper:Pontiki, M., et al. (2014). SemEval-2014 Task 4: Aspect-Based Sentiment Analysis. Proceedings of SemEval 2014.DOI: 10.3115&#x2F;v1&#x2F;S14-2004Read on aclanthology.org 6. Emotion Detection Description: Detects specific emotions (e.g., happiness, anger, fear) rather than just positive or negative sentiment. Applications: Crisis management, psychological studies. Key Paper:Mohammad, S. M., &amp; Turney, P. D. (2013). Crowdsourcing a Word–Emotion Association Lexicon. Computational Intelligence, 29(3), 436–465.DOI: 10.1111&#x2F;j.1467-8640.2012.00460.xRead on sci-hub.se 7. Multimodal Sentiment Analysis Description: Combines multiple data sources (e.g., text, audio, video) for sentiment analysis. Applications: Video sentiment analysis, call center analytics. Key Paper:Zadeh, A., et al. (2017). Tensor Fusion Network for Multimodal Sentiment Analysis. Proceedings of EMNLP 2017.DOI: 10.48550&#x2F;arXiv.1707.07250Read on arxiv.org 8. Hybrid Sentiment Analysis Description: Combines lexicon-based, rule-based, and machine learning techniques for robust and accurate sentiment detection. Applications: Industry-specific sentiment analysis. Key Paper:Cambria, E., et al. (2013). New Avenues in Opinion Mining and Sentiment Analysis. IEEE Intelligent Systems, 28(2), 15–21.DOI: 10.1109&#x2F;MIS.2013.30Read on sci-hub.se 9. Transfer Learning for Sentiment Analysis Description: Fine-tunes pre-trained models (e.g., BERT, RoBERTa, GPT) for sentiment classification tasks. Applications: Multilingual sentiment analysis, specialized domains. Key Paper:Devlin, J., et al. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Proceedings of NAACL 2019.DOI: 10.48550&#x2F;arXiv.1810.04805Read on arxiv.org 10. Fine-Grained Sentiment Analysis Description: Assigns sentiment scores on a fine-grained scale (e.g., star ratings from 1 to 5). Applications: Detailed product reviews, star-rating predictions. Key Paper:Yang, B., et al. (2016). Hierarchical Attention Networks for Document Classification. Proceedings of NAACL 2016.DOI: 10.18653&#x2F;v1&#x2F;N16-1174 Other resources https://www.nice.com/info/top-sentiment-analysis-tools-and-techniques Article: Opinion Mining and Sentiment Analysis. January 2008. Foundations and Trends® in Information Retrieval 2(1–2):1-135 DOI:10.1561&#x2F;1500000011 Read on cs.cornell.edu","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"NLP","slug":"NLP","permalink":"https://ooge0.github.io/hexo-blog/tags/NLP/"}]},{"title":"NLP in pictures","slug":"post_ai_nlp__nlp_in_pictures_1","date":"2024-11-21T17:21:11.000Z","updated":"2024-11-21T17:52:01.459Z","comments":true,"path":"2024/11/21/post_ai_nlp__nlp_in_pictures_1/","permalink":"https://ooge0.github.io/hexo-blog/2024/11/21/post_ai_nlp__nlp_in_pictures_1/","excerpt":"","text":"NLP extracting information flowDesired (logical processes)Morphological analysis &gt;&gt; Syntax analysis &gt;&gt; Semantic analysis &gt;&gt; Extracting information NLP text processing pipeline (imagination in some AI&#x2F;ML engineers heads) Megaputer representationMegaputer YouTube: Большая языковая модель MegaGPT + лингвистические правила: гибридный подход для анализа текстов Презентация доступна по ссылке. Сайт Мегапьютер Галерея проектов, разработанных в PolyAnalyst","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"ML","slug":"ML","permalink":"https://ooge0.github.io/hexo-blog/tags/ML/"},{"name":"NLP","slug":"NLP","permalink":"https://ooge0.github.io/hexo-blog/tags/NLP/"}]},{"title":"Linguistic basics","slug":"post_linquistic_basics","date":"2024-11-21T17:21:11.000Z","updated":"2024-12-16T08:20:18.808Z","comments":true,"path":"2024/11/21/post_linquistic_basics/","permalink":"https://ooge0.github.io/hexo-blog/2024/11/21/post_linquistic_basics/","excerpt":"","text":"1. Linguistic Domains Involved Syntax: Deals with sentence structure and grammatical correctness, which is crucial for text quality. Semantics: Ensures that the meaning of words and sentences aligns logically, contributing to coherence and quality. Pragmatics: Focuses on the context and the intended meaning, which impacts both coherence and quality. Discourse Analysis: Examines how sentences and paragraphs are connected to form a meaningful and coherent whole, a critical aspect of text coherence. 2. Theoretical LinguisticsIn theoretical linguistics, these concepts relate to: Text Linguistics: Studies how text is structured to make it meaningful and coherent as a whole. Cohesion and Coherence: Examines linguistic devices (e.g., conjunctions, pronouns, references) and logical connections between text elements. Cognitive Linguistics: Explores how readers perceive and interpret generated text, contributing to the perception of coherence and quality. 3. Applied LinguisticsIn applied contexts, text coherence and quality are central to: Natural Language Processing (NLP): Ensuring generated text by machines aligns with human linguistic expectations. Language Teaching: Teaching coherent and high-quality writing skills. Translation Studies: Maintaining coherence and quality when translating between languages. 4. Computational LinguisticsText generation and evaluation are significant in computational linguistics, where algorithms are developed to: Mimic human-like coherence and quality in machine-generated text. Optimize coherence through logical flow and topic consistency. Enhance quality with grammatically accurate and stylistically appropriate output. References Paper: Cohesion and Coherence in Text: An Introduction by Teun A. van Dijk (1997) DOI: 10.1016&#x2F;j.pragma.2010.04.028 Read Cohesion and Coherence in Text: An Introduction by Teun A. van Dijk (1997) on library.lgaki.info Paper: Cognitive Linguistics and Second Language Learning by Peter Robinson (2007) DOI: 10.1017&#x2F;CBO9781139524780 Read Cognitive Linguistics and Second Language Learning by Peter Robinson (2007) on api.pageplace.de Book: Speech and Language Processing by Daniel Jurafsky &amp; James H. Martin DOI: XYZ Read Speech and Language Processing by Daniel Jurafsky &amp; James H. Martin (1999) on us.archive.org Read Speech and Language Processing by Daniel Jurafsky &amp; James H. Martin (2000) on ipwatchdog.com Paper: Coherence in natural language: Data structures and applications by Florian Wolf (2000) DOI: XYZ Read Coherence in natural language: Data structures and applications by Florian Wolf (2000) on Paper: Natural Language Processing and Automated Text Categorization by Alessandro Moschitti (2003) DOI: XYZ Read Natural Language Processing and Automated Text Categorization by Alessandro Moschitti (2003) on Book: The Cambridge Handbook of Computational Linguistics by R. Dale, H. Moisl &amp; H. Somers DOI: 10.1017&#x2F;9781108755610","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://ooge0.github.io/hexo-blog/tags/NLP/"},{"name":"linguistic","slug":"linguistic","permalink":"https://ooge0.github.io/hexo-blog/tags/linguistic/"}]},{"title":"AI - Machine Learning + emotions","slug":"post_ai_ml__emotions","date":"2024-11-21T00:08:12.000Z","updated":"2024-11-21T22:00:11.516Z","comments":true,"path":"2024/11/21/post_ai_ml__emotions/","permalink":"https://ooge0.github.io/hexo-blog/2024/11/21/post_ai_ml__emotions/","excerpt":"","text":"Paper: InstructERC: Reforming Emotion Recognition in Conversation with a Multi-task Retrieval-based LLMs Framework DOI:10.48550&#x2F;arXiv.2406.18088 Read on openreview.net Paper: InstructERC: Reforming Emotion Recognition in Conversation with Multi-task Retrieval-Augmented Large Language Models, 2024 DOI: 10.48550&#x2F;arXiv.2309.11911 Read on arxiv.org Other resources&#x2F;papers&#x2F;books Book: Zinker J. Creative process in gestalt therapy. Vintage books. Random House, 1978 Book: The Expressions of the Emotions in Man and Animals. Darwin. 1872 Read book on darwin-online.org.uk ✰✰✰✰✰ Read book on archive.org ✰ Read book on web.seducoahuila.gob.mx Paper: PyPlutchik: Visualising and comparing emotion-annotated corpora. 2021. DOI:10.1371&#x2F;journal.pone.0256503 ✰✰✰✰✰ Read on researchgate.net Paper: The Feeling Wheel.Willcox,G.(1982).Transactional Analysis Journal, 12(4), 274–276. DOI:10.1177&#x2F;036215378201200411 Read on sc-hub.se","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"ML","slug":"ML","permalink":"https://ooge0.github.io/hexo-blog/tags/ML/"},{"name":"emotions","slug":"emotions","permalink":"https://ooge0.github.io/hexo-blog/tags/emotions/"}]},{"title":"Fine-tuning vs. Feature-based Approaches","slug":"post_ai_ml__finetuning_vs_feature_based_approaches","date":"2024-11-21T00:08:12.000Z","updated":"2024-12-04T09:35:50.869Z","comments":true,"path":"2024/11/21/post_ai_ml__finetuning_vs_feature_based_approaches/","permalink":"https://ooge0.github.io/hexo-blog/2024/11/21/post_ai_ml__finetuning_vs_feature_based_approaches/","excerpt":"","text":"Fine-tuning and feature-based approaches are two common techniques in transfer learning, a machine learning method where a pre-trained model is reused as a starting point for a new task. Fine-tuning involves adjusting the weights of a pre-trained model on a new dataset. This method is more computationally intensive but can lead to better performance, especially when the new task is similar to the original task. Feature-based approaches, on the other hand, extract features from a pre-trained model and use them as input for a new model. This method is less computationally intensive and can be effective when the new task is different from the original task. Key Differences: Feature Fine-tuning Feature-based Model Modification Adjusts weights of pre-trained model Extracts features, trains new model Computational Cost Higher Lower Task Similarity Best for similar tasks Can be used for diverse tasks Data Requirements More data needed Less data needed","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"ML","slug":"ML","permalink":"https://ooge0.github.io/hexo-blog/tags/ML/"}]},{"title":"Prompt Engineering - task_1.","slug":"post_ai_promt_engineer_task_1","date":"2024-11-20T23:56:12.000Z","updated":"2024-12-12T11:12:16.777Z","comments":true,"path":"2024/11/21/post_ai_promt_engineer_task_1/","permalink":"https://ooge0.github.io/hexo-blog/2024/11/21/post_ai_promt_engineer_task_1/","excerpt":"","text":"Prompt Evaluation Guide: Assessing Prompt and Response Quality 1. IntroductionIn this guide, we will evaluate the quality of prompts and their corresponding responses using a machine learning model. The goal is to determine whether the outputs align with specified criteria, improving the model’s prompt-handling capability through fine-tuning or adjustments. We’ll use OpenAI’s GPT-based models as our foundation, showcasing how to configure, evaluate, and visualize results locally. This guide includes a step-by-step walkthrough for local deployment, fine-tuning, and performance assessment, complete with visualization of results to understand the quality and impact of changes. 2. Required ToolsTools and Libraries Hugging Face Transformers: For model training and configuration. Datasets Library: To load and preprocess prompt-response datasets. PyTorch or TensorFlow: Backend for model execution. Matplotlib and Seaborn: For data visualization. Python 3.8+: Required for compatibility with libraries. Evaluation Metrics: BLEU Score ROUGE-L Perplexity Resources Model: Pretrained GPT-2 or similar transformer-based model. Download from Hugging Face Model Hub. Dataset: Use datasets like squad_v2 or create a custom prompt-response dataset. Environment: A local Python environment or virtual environment for isolation. 3. Installation GuideClone the repository for local setup. 12git clone https://github.com/huggingface/transformers.gitcd transformers Create virtual environment.To create a virtual environment, execute the following commands in the command line: 1pip install virtualenv Activate the virtual environment: 1venv\\Scripts\\activate Create requirements.txt in the project root directory.Add there list of Python libraries as 12345transformersdatasetstorchmatplotlib seaborn Install required Python libraries from requirements.txt: 1pip install -r requirements.txt or if you are not using virtual env, execute 1pip install transformers datasets torch matplotlib seaborn 4. Configuration Guide Prepare Configuration File Create a config.json with the following parameters:123456789&#123; &quot;model_name&quot;: &quot;gpt-2&quot;, &quot;dataset_name&quot;: &quot;custom_dataset.json&quot;, &quot;max_length&quot;: 256, &quot;batch_size&quot;: 16, &quot;learning_rate&quot;: 5e-5, &quot;num_epochs&quot;: 3, &quot;evaluation_metrics&quot;: [&quot;bleu&quot;, &quot;rouge-l&quot;, &quot;perplexity&quot;]&#125; Dataset PreparationEnsure your dataset is in JSONL format:12&#123;&quot;prompt&quot;: &quot;What is AI?&quot;, &quot;response&quot;: &quot;Artificial Intelligence is...&quot;&#125;&#123;&quot;prompt&quot;: &quot;Define Machine Learning&quot;, &quot;response&quot;: &quot;Machine Learning is...&quot;&#125; 5. Core for Evaluation TaskDefine the evaluation process: Load Dataset: Preprocess prompts and responses. Fine-Tune Model: Train on specific tasks to enhance response relevance. Evaluate metrics, measure: BLEU, ROUGE-L, perplexity scores for outputs. 6. Guidelines for Prompt EvaluationKey Evaluation Areas: Relevance: Does the response match the expected answer? Clarity: Is the response clear and concise? Adaptability: Does the model adjust to different prompt complexities? Consistency: Are responses uniform in quality across test cases? Complexity Consideration: Simple prompts: Direct, factual queries. Complex prompts: Context-based or multi-turn questions. 7. Main ScriptsTraining Script Save as train.py: 12345678910111213141516171819202122232425262728293031from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArgumentsfrom datasets import load_dataset# Load model and tokenizermodel_name = &quot;gpt-2&quot;model = GPT2LMHeadModel.from_pretrained(model_name)tokenizer = GPT2Tokenizer.from_pretrained(model_name)# Load and preprocess datasetdataset = load_dataset(&quot;json&quot;, data_files=&quot;custom_dataset.json&quot;)def tokenize(batch): return tokenizer(batch[&quot;prompt&quot;], padding=&quot;max_length&quot;, truncation=True)tokenized_data = dataset.map(tokenize, batched=True)# Training argumentstraining_args = TrainingArguments( output_dir=&quot;./results&quot;, evaluation_strategy=&quot;epoch&quot;, learning_rate=5e-5, per_device_train_batch_size=16, num_train_epochs=3)# Trainertrainer = Trainer( model=model, args=training_args, train_dataset=tokenized_data[&quot;train&quot;])trainer.train() Evaluation Script Save as evaluate.py: 123456789101112131415from transformers import pipelinefrom datasets import load_dataset# Load modelmodel_name = &quot;./results&quot;evaluator = pipeline(&quot;text-generation&quot;, model=model_name)# Evaluate promptsprompts = [&quot;What is AI?&quot;, &quot;Define Machine Learning&quot;]responses = [evaluator(prompt, max_length=50) for prompt in prompts]# Metricsmetric = load_metric(&quot;bleu&quot;)metric_score = metric.compute(predictions=responses, references=[&quot;Artificial Intelligence is...&quot;, &quot;Machine Learning is...&quot;])print(&quot;BLEU Score:&quot;, metric_score) 8. Visualization and Explanation of ResultsVisualization Script Save as visualize.py: 12345678910111213import matplotlib.pyplot as pltimport seaborn as sns# Example metric scoresmetrics = &#123;&quot;BLEU&quot;: 0.85, &quot;ROUGE-L&quot;: 0.87, &quot;Perplexity&quot;: 15.2&#125;# Plotplt.figure(figsize=(8, 5))sns.barplot(x=list(metrics.keys()), y=list(metrics.values()))plt.title(&quot;Evaluation Metrics&quot;)plt.ylabel(&quot;Scores&quot;)plt.xlabel(&quot;Metric&quot;)plt.show() Analysis BLEU &amp; ROUGE-L: Higher scores indicate better text generation quality. Perplexity: Lower scores indicate improved language model comprehension. To improve the model’s performance here is possible to focus on the following activities based on the evaluation metrics and the provided GPT-2 configuration: 1. Data Preprocessing and Augmentation Data Cleaning: Ensure the training data is clean, well-structured, and consistent. Remove noisy or irrelevant content that could negatively affect performance. Augment Data: Introduce more varied examples, especially for underrepresented topics. Adding more diverse sentence structures, word choices, and contexts can help improve model robustness. 2. Prompt Optimization Refining Prompts: Work on crafting more precise and detailed prompts to guide the model towards generating more accurate responses. Incorporate Context: Provide context-rich prompts (e.g., multi-turn conversations) or detailed instructions to ensure the model outputs relevant and coherent responses. Temperature and Sampling: Adjust the do_sample setting and modify top_k or top_p parameters to control the randomness and creativity of the model’s output. A lower temperature (e.g., 0.7) can reduce randomness and produce more deterministic outputs. 3. Model Hyperparameters Adjustment Increase Layers or Heads: If you’re able to fine-tune, consider experimenting with increasing the number of layers or attention heads to help the model learn more complex patterns. Experiment with n_inner: Fine-tuning the n_inner parameter (which controls the size of the intermediate layer in the transformer) may yield better results for more complex tasks. 4. Fine-Tuning GPT-2 Fine-Tuning with Task-Specific Data: Fine-tune the model on your specific domain or task using high-quality, labeled datasets. Fine-tuning will allow the model to learn task-specific patterns. Transfer Learning: Use transfer learning techniques by starting with a pre-trained GPT-2 model, and then train it on your task-specific corpus to improve the output quality. 5. Evaluation Metric-Specific Adjustments BLEU: Since BLEU is currently 0.0, which indicates poor overlap with reference texts, consider focusing on improving the lexical similarity by training on text data with high-quality references. ROUGE: Improve the recall and precision for ROUGE scores by providing more informative prompts that encourage the model to capture key content and key phrases. METEOR: Since METEOR considers synonyms and paraphrases, increasing the model’s understanding of semantic equivalence might improve this score. Use data augmentation or adversarial training to enhance this aspect. BERTScore: BERTScore evaluates embeddings, so improving model embeddings can significantly help. You can experiment with fine-tuning GPT-2 using BERT-based models (like bert-base-uncased) for better contextual word representations. 6. Regularization Techniques Dropout Regularization: Experiment with adjusting attn_pdrop, embd_pdrop, and other dropout parameters to control overfitting and improve generalization. Layer Normalization: Ensure that layer normalization parameters (layer_norm_epsilon) are tuned properly to stabilize learning and avoid vanishing&#x2F;exploding gradients. 7. Model Size and Parameters Larger Models: If feasible, switch to larger models (e.g., GPT-2 Medium, GPT-2 Large, or GPT-3) for more capacity and better performance in complex tasks. Learning Rate and Optimizer Tuning: Adjust the learning rate for better convergence. Use learning rate schedulers to optimize training over time and avoid issues like vanishing gradients or poor local minima. 8. Loss Function Adjustments Loss Function Tweaks: Investigate the loss function (cross-entropy in GPT-2) to ensure it’s optimized for your specific task. Sometimes, switching the loss function can help improve performance in tasks like summarization or question-answering. 9. Sampling Strategies Top-k Sampling: Adjust the top_k parameter during text generation to sample from the top K most likely words. This can prevent repetitive or irrelevant generation. Nucleus Sampling: Adjust the top_p value to sample words from the cumulative probability distribution of the top P words, ensuring more diversity in the outputs. 10. Model Evaluation and Iteration Cross-validation: Use cross-validation to evaluate different configurations and fine-tuned models to find the optimal setup. Hyperparameter Search: Perform a hyperparameter search (e.g., grid search, random search) to find the best set of hyperparameters for improving performance metrics. Example Implementation:from transformers import GPT2LMHeadModel, GPT2Tokenizer import torch # Load pre-trained model and tokenizer model = GPT2LMHeadModel.from_pretrained(&#39;gpt2&#39;) tokenizer = GPT2Tokenizer.from_pretrained(&#39;gpt2&#39;) # Refine the prompt to improve results prompt = &quot;Describe the importance of artificial intelligence in healthcare.&quot; # Generate text using refined prompt inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;) outputs = model.generate(**inputs, max_length=100, do_sample=True, top_p=0.95, top_k=60) # Decode and print the output generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True) print(generated_text)","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"ML","slug":"ML","permalink":"https://ooge0.github.io/hexo-blog/tags/ML/"},{"name":"prompt_engineering","slug":"prompt-engineering","permalink":"https://ooge0.github.io/hexo-blog/tags/prompt-engineering/"}]},{"title":"Web data handling","slug":"notes/notes_web_data_handling","date":"2024-11-20T19:38:30.000Z","updated":"2024-12-03T07:34:55.040Z","comments":true,"path":"2024/11/20/notes/notes_web_data_handling/","permalink":"https://ooge0.github.io/hexo-blog/2024/11/20/notes/notes_web_data_handling/","excerpt":"","text":"Get text from DOM by specific locator via DevTools Task: Retrieve text from DOM for elements that have locator “.gfg-similar-read-item-heading” using devtoolsSolution:1234567891011121314// Retrieve all elements matching the locatorconst elements = document.querySelectorAll(&quot;.gfg-similar-read-item-heading&quot;);// Extract text content from each elementconst texts = Array.from(elements).map(element =&gt; element.textContent.trim());// Output the text content as an arrayconsole.log(texts);// Optional: Display the texts in format &#123;order_number&#125;&#123;text&#125;texts.forEach((text, index) =&gt; console.log(`$&#123;index + 1&#125;: $&#123;text&#125;`));// Optional: Display the texts in format &#123;text&#125;texts.forEach((text, index) =&gt; console.log(`$&#123;text&#125;`));","categories":[{"name":"Notes","slug":"Notes","permalink":"https://ooge0.github.io/hexo-blog/categories/Notes/"}],"tags":[{"name":"web","slug":"web","permalink":"https://ooge0.github.io/hexo-blog/tags/web/"}]},{"title":"Windows services and VM usage","slug":"notes/notes_windows_services_and_vm_usage","date":"2024-11-20T19:38:30.000Z","updated":"2024-12-03T07:27:09.503Z","comments":true,"path":"2024/11/20/notes/notes_windows_services_and_vm_usage/","permalink":"https://ooge0.github.io/hexo-blog/2024/11/20/notes/notes_windows_services_and_vm_usage/","excerpt":"","text":"Virtual Machine SetupSteps to create a virtual machine: Download a hypervisor: Go to VirtualBox or VMware. Download and install the software on your machine. Obtain an OS image: Download the required ISO file for the operating system. Create the virtual machine: Open VirtualBox&#x2F;VMware app. Create a new virtual machine according to requested specification.&#96;&#96;&#96;&#96; Configure name, OS type, hardware settings like RAM, CPU, and disk space. Install the OS Start the virtual machine. Follow the installation steps in the OS setup wizard. Connect to the VM: For Linux VMs: Enable SSH in the VM during installation or after setup. Use SSH (on Linux&#x2F;Mac) or tools like PuTTY (on Windows) to connect. Additional steps: Windows VM Remote Desktop: Enable Remote Desktop in the Windows VM settings. Use the RDP client, configuration described here &gt;&gt; Remote Desktop clients for Remote Desktop Services and remote PCs on your host machine to connect. Connect RDP client to remote machine. Windows Services Management in PowerShell. Task: Print list of services Run specific service Stop specific service Restart specific service Check status for specific service To get list of running services I will use : 1Get-Service | Where-Object &#123;$_.Status -eq &#x27;Running&#x27;&#125; Assuming that I want to run &quot;WSLService&quot; To start &quot;WSLService&quot; service I’m using : 1Start-Service -Name &quot;WSLService&quot; To stop the service: 1Stop-Service -Name &quot;WSLService&quot; For restarting service: 1Restart-Service -Name &quot;WSLService&quot; For checking the status of monitored service: 1Get-Service -Name &quot;WSLService&quot; SSH connection to the Linux machine actions are below. I added more information in advance because the original task does not contain information about the client machine from which the connection to the Linux machine will be made. There is 3 different cases: Case 1: From another Linux client to Linux host Case 2: From a Windows client to Linux host Case 3: From macOS client to Linux host For Linux&#x2F;Mac: SSH is usually pre-installed but if it’s missing install an SSH client (e.g. on Ubuntu): 1sudo apt install openssh-client -yAfter installation, start the service on Linux machine: 1sudo systemctl start sshIf SSH service not started , checked its status on Linux machine: 1sudo systemctl status ssh Generate SSH keys (on the host machine): 1ssh-keygen -t rsa -b 2048 The keys will be stored in ~&#x2F;.ssh&#x2F;id_rsa (private) and ~&#x2F;.ssh&#x2F;id_rsa.pub (public). Copy the public key to the Linux server: 1ssh-copy-id user@server_ip Or manually append the content of id_rsa.pub to ~&#x2F;.ssh&#x2F;authorized_keys on the server. Connection via SSH for Linux: 1ssh user@server_ip Optional Configurations: Edit the SSH config file (~&#x2F;.ssh&#x2F;config) for aliases: 1234Host server_alias HostName server_ip User username IdentityFile ~/.ssh/id_rsa This will simplify the connection: 1ssh server_alias Case 1: From another Linux client to Linux host Verify SSH Client on the Local Machine: Most Linux distributions come with the ssh client pre-installed. Confirm by running:1ssh -V If it’s not installed, install it: 1sudo apt install openssh-client Replace &lt;username&gt; with the remote machine’s username. Replace &lt;remote_ip&gt; with the target machine’s IP address. Use Public Key Authentication (Optional): 1ssh-keygen Copy the public key to the remote machine:1ssh-copy-id &lt;username&gt;@&lt;remote_ip&gt; Now, log in without entering a password:1ssh &lt;username&gt;@&lt;remote_ip&gt; Case 2: From a Windows client to Linux host Install an SSH Client: Use the built-in OpenSSH client on Windows 10+. Open PowerShell or Command Prompt and check if SSH is installed: 1ssh -V For Windows alternatively install OpenSSH client from “Optional Features” or download PuTTY( third-party SSH client). Connect Using OpenSSH (Built-in). Open PowerShell or Command Prompt and run: 1ssh &lt;username&gt;@&lt;remote_ip&gt; Connect Using PuTTY: Download and install PuTTY. Open PuTTY and enter the hostname or IP address. Set the Port to 22 and click Open. Log in with your credentials. Enable Key Authentication (Optional): Use puttygen to generate a private&#x2F;public key pair. Copy the public key to the Linux machine’s ~&#x2F;.ssh&#x2F;authorized_keys. In PuTTY, configure the private key in Connection &gt; SSH &gt; Auth &gt; Browse Private Key. Case 3: From macOS client to Linux host Verify SSH Client: macOS includes an SSH client by default. Confirm it by running: 1ssh -V Connect to the Linux Machine: Open Terminal and run: 1ssh &lt;username&gt;@&lt;remote_ip&gt; Use Public Key Authentication (Optional): 1ssh-keygen Copy the public key to the remote machine: 1ssh-copy-id &lt;username&gt;@&lt;remote_ip&gt; Now, I can log in without entering a password: 1ssh &lt;username&gt;@&lt;remote_ip&gt;","categories":[{"name":"Notes","slug":"Notes","permalink":"https://ooge0.github.io/hexo-blog/categories/Notes/"}],"tags":[{"name":"Dashdevs","slug":"Dashdevs","permalink":"https://ooge0.github.io/hexo-blog/tags/Dashdevs/"}]},{"title":"Text Classification. Historical overview, tools, and techniques.","slug":"post_ai_nlp__text_classification_intro_1","date":"2024-11-19T10:05:30.000Z","updated":"2024-11-19T10:06:14.446Z","comments":true,"path":"2024/11/19/post_ai_nlp__text_classification_intro_1/","permalink":"https://ooge0.github.io/hexo-blog/2024/11/19/post_ai_nlp__text_classification_intro_1/","excerpt":"","text":"Text classification is a fundamental task in Natural Language Processing (NLP) that involves categorizing text into predefined categories. Its applications range from sentiment analysis to spam detection, and news categorization to intent recognition in conversational AI. This document provides a historical overview, a comparative analysis of tools, and details about modern approaches like Word2Vec, FastText, GloVe, and deep learning. Historical Overview of Text Classification Traditional Methods (1950s - 2000s) Bag of Words (BoW): Represents text as a frequency vector of words. Pros: Simple and interpretable. Cons: Ignores word order and semantics. TF-IDF (Term Frequency-Inverse Document Frequency): Improves BoW by weighting rare words higher. Pros: Better at distinguishing important terms. Cons: Still ignores context and word relationships. Naive Bayes: Commonly used with BoW or TF-IDF for classification. Pros: Fast and robust for small datasets. Cons: Assumes word independence, which rarely holds. Emergence of Distributed Representations (2010s)The advent of distributed word representations marked a significant leap, addressing the limitations of sparse representations. Word2Vec (2013, Mikolov et al.): Generates dense vector embeddings using Skip-gram or CBOW. Paper: Efficient Estimation of Word Representations in Vector Space DOI: 10.48550&#x2F;arXiv.1301.3781 Pros: Captures semantic relationships and is computationally efficient. Cons: Fixed-size embeddings and lacks out-of-vocabulary word handling. GloVe (2014, Pennington et al.): Combines global word co-occurrence statistics with local context to produce embeddings. Paper: GloVe: Global Vectors for Word Representation DOI: 10.3115&#x2F;v1&#x2F;D14-1162 Pros: Effective at capturing statistical information. Cons: Pre-trained on fixed corpora; inflexible for dynamic contexts. FastText (2016, Bojanowski et al.): Extends Word2Vec by representing words as n-grams of characters. Paper:Enriching Word Vectors with Subword Information, 2016 DOI: 10.48550&#x2F;arXiv.1607.04606 Pros: Handles rare and out-of-vocabulary words better. Cons: Increased computational cost compared to Word2Vec. Deep Learning Era (Late 2010s - Present)The rise of neural networks transformed text classification. Key innovations include: Recurrent Neural Networks (RNNs):Captures sequential dependencies but suffers from vanishing gradients. Convolutional Neural Networks (CNNs):Effective for capturing local patterns in text. Paper: An Introduction to Convolutional Neural Networks DOI: 10.48550&#x2F;arXiv.1511.08458 Papper: A review of convolutional neural networks in computer Read the doc &gt;&gt;&gt; A review of convolutional neural networks in computer Transformers and Pre-trained Models (2018 - Present):Models like: BERT Paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2018 DOI:10.48550&#x2F;arXiv.1810.04805 GPT GPT-2 Paper: Release Strategies and the Social Impacts of Language Models, 2019 DOI:10.48550&#x2F;arXiv.1908.09203 GPT-3.5 Paper: Language Models are Few-Shot Learners, 2020 DOI:10.48550&#x2F;arXiv.2005.14165 GPT-4 Paper: GPT-4 Technical Report, 2023 DOI:10.48550&#x2F;arXiv.2303.08774 and T5 revolutionized NLP by leveraging attention mechanisms and transfer learning. Paper: T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, 2019 DOI:10.48550&#x2F;arXiv.1910.10683 Footnotes1.GLUE - The General Language Understanding Evaluation (GLUE) benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems. ↩2.IMDB Dataset - Large Movie Review Dataset for Sentiment Analysis by Stanford. ↩3.CoNLL-2003 - Dataset for Named Entity Recognition and other sequence modeling tasks. ↩4.OntoNotes - Annotated dataset covering various linguistic phenomena. ↩5.WMT - Workshop on Machine Translation datasets for translation tasks. ↩6.OpenSubtitles - Large corpus of subtitles for multilingual tasks. ↩7.CNN/DailyMail - Dataset for abstractive summarization tasks. ↩8.XSum - Dataset for extreme summarization of news articles. ↩9.AG News - News topic classification dataset. ↩10.Reuters-21578 - Text categorization benchmark dataset. ↩11.SQuAD - Stanford Question Answering Dataset for reading comprehension tasks. ↩12.TriviaQA - Dataset containing trivia questions and evidence passages. ↩13.WikiText - Dataset for language modeling based on Wikipedia articles. ↩14.Penn Treebank - Corpus for linguistic annotation and modeling. ↩15.Universal Dependencies - Framework for consistent grammatical annotation across languages. ↩16.WSJ Corpus - Dataset from Wall Street Journal articles for POS tagging. ↩17.Coreference resolution (CR) is the task of finding all linguistic expressions (called mentions) in a given text that refer to the same real-world entity. ↩18.CoNLL-2012 - Shared task on coreference resolution and other NLP challenges. ↩19.Annotated dataset created to evaluate RoBERTa’s performance on coreference tasks, with a focus on contextual embeddings. ↩","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"ML","slug":"ML","permalink":"https://ooge0.github.io/hexo-blog/tags/ML/"},{"name":"NLP","slug":"NLP","permalink":"https://ooge0.github.io/hexo-blog/tags/NLP/"},{"name":"text_classification","slug":"text-classification","permalink":"https://ooge0.github.io/hexo-blog/tags/text-classification/"}]},{"title":"Довідник для Лікаря Сімейної Медицини","slug":"notes/nst_notes","date":"2024-11-18T09:28:32.625Z","updated":"2024-11-18T09:52:44.381Z","comments":true,"path":"/nastusja/","permalink":"https://ooge0.github.io/hexo-blog/nastusja/","excerpt":"","text":"Цей довідник створений власними силами та за допомогою відкритих джерел для допомоги лікарям сімейної медицини, надаючи ключові ресурси, офіційні українські портали, посилання на довідники, глосарії та інші корисні інструменти. Офіційні Українські Ресурси Міністерство охорони здоров’я України (МОЗ) Офіційний сайт: moz.gov.ua Новини, нормативна база, протоколи лікування. Національна служба здоров’я України (НСЗУ) Офіційний сайт: nszu.gov.ua Інформація про медичні послуги, звіти, та декларації з лікарями. Центр громадського здоров’я МОЗ України Сайт: phc.org.ua Вакцинація, профілактика хвороб, епідеміологія. Медичні Довідники та Компедіуми Компендіум Лікарських Засобів Сайт: compendium.com.ua Інформація про ліки, інструкції, дозування. Реєстр Ліків України Сайт: drlz.com.ua Офіційний перелік зареєстрованих ліків. Клінічні протоколи лікування МОЗ регулярно оновлює протоколи: Протоколи МОЗ. Глосарії та Довідкові Матеріали Медичний Глосарій Глосарій МОЗ: moz.gov.ua. МОЗ Про затвердження Єдиного термінологічного словника (Глосарій) з питань управління якості медичної допомоги: zakon.rada.gov.ua Довідники для лікарів первинної ланки Рекомендації для первинної допомоги: phc.org.ua. Кишеньковий довідник лікаря первинної медичної допомоги для роботи з дітьми та підлітками: настанови щодо зміцнення здоров’я, профілактики та лікування захворювань від народження до підліткового віку: who.int Що входить до обов’язків лікаря первинної ланки?: moz.gov.ua Лабораторії та Діагностика Сінево Україна synevo.ua Діла Лабораторія dila.ua Меділаб medilab.com.ua Глобальні Медичні Ресурси Всесвітня організація охорони здоров’я (ВООЗ) Сайт: who.int Міжнародні рекомендації та дослідження. Медична Бібліотека США (PubMed) Сайт: pubmed.ncbi.nlm.nih.gov Наукові статті з медицини. CDC (Центри контролю та профілактики захворювань, США) Сайт: cdc.gov Профілактика, контроль хвороб. Корисні Інструменти Клінічні калькулятори: msdmanuals.com Медичні калькулятори: medcalc.com.ua Калькулятор маси тіла: cardioprostir.com.ua Калькулятор Калькулятор оцінки прихильності пацієнта: cardioprostir.com.ua Розрахунок дозувань: clincalc.com Медичний калькулятор: Дозування таблеток в залежності від маси тіла: testresult.org Перевірка взаємодії препаратів Після переходу на сайт для перекладу натисніть правою кнопкою миші та оберіть “Перекласти”: cardioprostir.com.ua Завантаження та КонтактиЗберігайте цей список для швидкого доступу до корисних ресурсів! Якщо у вас є запитання чи пропозиції, будь ласка, зв’яжіться з автором через офіційні канали МОЗ або НСЗУ.","categories":[{"name":"Notes","slug":"Notes","permalink":"https://ooge0.github.io/hexo-blog/categories/Notes/"}],"tags":[{"name":"nst","slug":"nst","permalink":"https://ooge0.github.io/hexo-blog/tags/nst/"}]},{"title":"Glossary of Machine Learning and AI Terms","slug":"post_ai_nn__nn_glossary","date":"2024-11-17T22:00:00.000Z","updated":"2024-12-14T19:52:46.964Z","comments":true,"path":"/ai/glossary-of-machine-learning-and-ai-terms/","permalink":"https://ooge0.github.io/hexo-blog/ai/glossary-of-machine-learning-and-ai-terms/","excerpt":"","text":"Feel free to send me your thoughts, notes, other references. My contact details you can find on LinkedIn IndexA B C D E F G H I J K L M N O P Q R S T U V W X Y Z AAutoencodersAutoencoders are neural network architectures used for unsupervised learning. They are designed to compress input data into a lower-dimensional latent space (encoding) and then reconstruct the original data from this compressed representation (decoding). The primary objective is to minimize the reconstruction error, typically measured as the difference between input and output. Applications: Dimensionality Reduction: Acts as a non-linear alternative to Principal Component Analysis (PCA). Feature Learning: Learns compact and meaningful representations of data. Denoising: Removes noise from corrupted data by training on clean samples. Anomaly Detection: Identifies unusual patterns by observing high reconstruction errors. Variants: Sparse Autoencoders: Encourage sparsity in the hidden units to create compressed and interpretable features. Denoising Autoencoders: Add noise to inputs during training, forcing the network to learn robust features. Convolutional Autoencoders: Specialize in image data, leveraging convolutional layers for spatial feature learning. Key References: Paper: Efficient Learning of Sparse Representations with an Energy-Based Model, 2006. DOI: 10.7551&#x2F;mitpress&#x2F;7503.003.0147 Read on: cs.nyu.edu Paper: Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion, 2010.Read on: cse.fau.edu Artificial Neural Networks (ANNs)Artificial Neural Networks (ANNs) are computational processing systems inspired by biological nervous systems (e.g., the human brain). BBase modelThe original, foundational version of a large language model, which has not been fine-tuned. BERTBidirectional Encoder Representations from Transformers. DEveloped in 2018.BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Paper: - BERT: Pre-training of Deep Bidirectional Transformers forLanguage Understanding. 2018 - DOI: 10.18653&#x2F;v1&#x2F;N19-1423 - Read on arxiv.org Post - BERT Transformers – How Do They Work? BERTScore Description: Leverages contextual embeddings from BERT to evaluate semantic similarity between the generated and reference text. Purpose: Captures semantic similarity more effectively than traditional n-gram-based metrics. Metric Range Interpretation Example Values BERTScore (Precision) 0 to 1 Measures how much of the generated text aligns with the reference. 0.7 (moderate), 0.85 (good), 0.95 (excellent) BERTScore (Recall) 0 to 1 Measures how much of the reference text is captured in the generated output. 0.6 (moderate), 0.8 (good), 0.9 (excellent) BERTScore (F1) 0 to 1 Harmonic mean of precision and recall; overall measure of similarity. 0.65 (moderate), 0.82 (good), 0.9 (excellent) BiasThe disproportionate favor or prejudice towards a specific item or group. AI algorithms may inherit biases from historical data or human trainers, risking perpetuation of these biases in predictions. BLEUBLEU (Bilingual Evaluation Understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. Quality is considered to be the correspondence between a machine’s output and that of a human: “the closer a machine translation is to a professional human translation, the better it is” – this is the central idea behind BLEU.BLEU was one of the first metrics to claim a high correlation with human judgements of quality, and remains one of the most popular automated and inexpensive metrics. Paper: “BLEU: a Method for Automatic Evaluation of Machine Translation”. Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 311-318. DOI: 10.3115&#x2F;1073083.1073135 Read on aclanthology.org Read on sci-hub.se BLEU score BLEU scores are calculated for individual translated segments—generally sentences—by comparing them with a set of good quality reference translations. Those scores are then averaged over the whole corpus to reach an estimate of the translation’s overall quality. Neither intelligibility nor grammatical correctness are not taken into account. The BLEU algorithm compares consecutive phrases of the automatic translation with the consecutive phrases it finds in the reference translation, and counts the number of matches, in a weighted fashion. These matches are position independent. A higher match degree indicates a higher degree of similarity with the reference translation, and higher score. Intelligibility and grammatical correctness aren’t taken into account. web article: “How to Compute BLEU Score” CConvolutional layerA core component of CNNs that processes input data using filters (kernels) to produce feature maps, identifying patterns or features. Convolutional Neural Network (CNN) CNN - a type of neural network specialized for analyzing visual data, learning features via filter optimization. CNN - similar to ANNs but optimized for image data, with layers designed for feature extraction and classification. What are convolutional neural networks? DDatasetThe training data used to teach an LLM patterns and relationships. FFalse positiveAn incorrect prediction where a model identifies a condition or class that is not present. F1 scoreA performance metric for classification models combining precision and recall into a single value ranging from 0 (poor) to 1 (excellent). Few-shotUsing a small number of examples to guide the model in performing a new task. Fine-tuningAdapting a pre-trained model to a specific task or domain by training it on a smaller, specialized dataset. GGenerative AIAI systems capable of creating new content, such as text, images, or audio. HHallucinationWhen an LLM generates plausible but factually incorrect or nonsensical information. IInferenceThe process of using a trained model to make predictions or generate outputs. LLanguageToolLanguageTool is an AI-based grammar checker. Paste your text or start typing below to check grammatical errors, and spelling mistakes across languages.Reference: LanguageTool LCSLCS &#x3D; Longest Common Subsequence LoRALow-Rank Adaptation, a fine-tuning method requiring less computational resources compared to full fine-tuning. MMachine Learning Operations (MLOps)The process of deploying, monitoring, and updating machine learning models in production environments. METEORMetric. METEOR &#x3D; Metric for Evaluation of Translation with Explicit ORdering Description: Evaluates semantic similarity by considering unigram overlaps, stemming, synonyms, and paraphrasing. Score Parameter: METEOR Score, ref Purpose: Provides a balanced metric for machine translation and summarization tasks. METEOR Score Metric: METEOR Range: 0 to 1 Interpretation: Combines precision and recall, with semantic similarity (e.g., synonyms, stems) Example Values: Higher scores indicate better matches. 0.3 (low), 0.6 (moderate), 0.85 (high) MLOpsShort for Machine Learning Operations. NNamed Entity RecognitionNamed Entity Recognition &#x3D; NER- Named Entity Recognition seeks to extract substrings within a text that name real-world objects and to determine their type (for example, whether they refer to persons or organizations).Paper: A survey on recent advances in Named Entity Recognition, 2024.- Named Entity Recognition (NER) is a sub-task of information extraction in Natural Language Processing (NLP) that classifies named entities into predefined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, and more.* Read on arxiv.org* Named Entity Recognition with LLMs — Extract Conversation Metadata | MediumNeural Process FamilyNeural Process Family &#x3D; NPF PPerplexityA measure of how well a language model predicts a sample of text, with lower scores indicating better performance. Range Interpretation Example Values 1 to ∞ Measures the uncertainty of the model’s predictions. Lower perplexity indicates better performance. A perplexity of 1 means perfect predictions, while higher values indicate more uncertainty and worse performance. 20 (good), 50 (average), 200 (poor) Pooling layersLayers in CNNs designed to reduce the dimensionality of feature maps, lowering computational complexity. PrecisionA metric measuring the ratio of true positives to all predicted positives in a classification model. Pre-trained modelA model that has been trained on a dataset and may be further fine-tuned for specific tasks. Prompt engineeringThe art of crafting effective inputs to elicit desired output from an LLM. Prompt templateSpecially formatted instructions used by LLMs to define the input and output. RR.A.G. (Retrieval-Augmented Generation)A technique combining external information retrieval with the generative capabilities of an LLM for improved accuracy. ROUGEROUGE (Recall-Oriented Understudy for Gisting Evaluation) Score parameters: ROUGE-1: Range: 0 to 1, Measures the overlap of unigram (single word) between the generated and reference text. 0.2 (low overlap), 0.5 (moderate), 0.8 (high) ROUGE-2: Range: 0 to 1, Measures the overlap of bigrams (two consecutive words) between generated and reference text. 0.1 (low), 0.4 (moderate), 0.7 (high) ROUGE-L: Range: 0 to 1, Measures the longest common subsequence (LCS) between the generated and reference text. 0.3 (low), 0.6 (moderate), 0.9 (high) ROUGE-Lsum is specifically designed for summarization tasks, particularly when evaluating summaries. It computes the ROUGE-L score for the summarization task based on how well the generated summary matches the reference summary. Purpose: Evaluates lexical similarity, fluency, and coherence across different levels (unigrams, bigrams, and sequences). Original ROUGE Paper: Lin, Chin-Yew. “ROUGE: A Package for Automatic Evaluation of Summaries” DOI: 10.3115&#x2F;1073083.1073135 Read on ACL Anthology ROUGE-LROUGE-L &#x3D; ROUGE (Recall-Oriented Understudy for Gisting Evaluation) + L (Longest Common Subsequence (LCS) ROUGE-L captures the longest sequence of words that appear in both texts in the same order, providing insights into fluency and coherence. Paper: “Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics.” Lin, Chin-Yew. DOI: 10.3115&#x2F;1218955.1219032 Read on sci-hub.se SSpecial TokenReserved tokens used in LLMs for specific functions, such as defining the start or end of a response. Softmax function“Softmax function” read on notes.theomorales.com Supervised learningLearning through pre-labeled inputs, where the model aims to reduce classification error by predicting the correct outputs.It works opposite on unsupervised learning System promptsInstructions defining an LLM’s behavior, role, or context in a conversation. TTokenThe smallest unit of text processed by an LLM, such as a word or subword. TokenizationBreaking text into tokens for model processing. TrainingFeeding a model with data to allow it to learn patterns and relationships. UUnsupervised learningLearning from data without labeled outputs, often for tasks like clustering or dimensionality reduction.It works opposite on supervised learning VVariational Autoencoders (VAEs)Variational Autoencoders (VAEs) are a probabilistic extension of autoencoders that learn not only to compress data but also to generate new samples by modeling data distributions. VAEs use a latent space with a probabilistic structure, enabling meaningful interpolation between points in the latent space.Features: Latent Space Regularization: Ensures the latent space follows a predefined probability distribution, commonly Gaussian. Reconstruction and Generation: Balances reconstruction accuracy with the regularization term using a loss function derived from the evidence lower bound (ELBO). Bayesian Interpretation: The encoding process approximates posterior distributions via variational inference. Applications: Data Generation: Generate novel and coherent samples (e.g., synthetic images, text). Anomaly Detection: Identifies data points that deviate from the learned distribution. Latent Space Manipulation: Enables interpolation and arithmetic operations in the latent space. Key References: Title: “Auto-Encoding Variational Bayes”DOI: 10.48550&#x2F;arXiv.1312.6114 Title: “Variational Inference with Normalizing Flows”DOI: 10.48550&#x2F;arXiv.1505.05770 ZZero-padding (CNN)A technique in CNNs that pads the borders of input data with zeros, controlling output dimensionality. Zero-shotA model’s ability to perform tasks it was not explicitly trained for by leveraging general knowledge. Footnotes 2.https://github.com/nomic-ai/gpt4all/wiki/Generative-AI-Terminology ↩3.https://nhsx.github.io/ai-dictionary ↩","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"ML","slug":"ML","permalink":"https://ooge0.github.io/hexo-blog/tags/ML/"},{"name":"glossary","slug":"glossary","permalink":"https://ooge0.github.io/hexo-blog/tags/glossary/"}]},{"title":"Online tools","slug":"notes/online_tools","date":"2024-11-17T22:00:00.000Z","updated":"2024-11-18T17:52:11.042Z","comments":true,"path":"2024/11/18/notes/online_tools/","permalink":"https://ooge0.github.io/hexo-blog/2024/11/18/notes/online_tools/","excerpt":"","text":"TextText editing https://convertcase.net/ Text comparison https://text-compare.com/ OCR Text from boofer&#x2F;pictures: http://www.structurise.com/screenshot-ocr/","categories":[{"name":"Notes","slug":"Notes","permalink":"https://ooge0.github.io/hexo-blog/categories/Notes/"}],"tags":[{"name":"my_contribution","slug":"my-contribution","permalink":"https://ooge0.github.io/hexo-blog/tags/my-contribution/"},{"name":"tools","slug":"tools","permalink":"https://ooge0.github.io/hexo-blog/tags/tools/"},{"name":"online_tools","slug":"online-tools","permalink":"https://ooge0.github.io/hexo-blog/tags/online-tools/"}]},{"title":"Comparing Efficiency of NLP Models. Methods and Metrics","slug":"post_ai__comparing_nlp_models_methods_metrix","date":"2024-11-17T19:39:11.000Z","updated":"2024-12-04T20:06:36.610Z","comments":true,"path":"/techniques-for-handling-context-in-ai-models/","permalink":"https://ooge0.github.io/hexo-blog/techniques-for-handling-context-in-ai-models/","excerpt":"","text":"When comparing the efficiency of NLP models, it is essential to use standardized approaches, metrics, and parameters to ensure a fair and comprehensive evaluation. Below is a structured guide. 1. Approaches for Evaluating NLP Models Task-Specific Evaluation: Measure performance on specific NLP tasks (e.g., sentiment analysis, named entity recognition, machine translation). Benchmark Datasets: Use well-known datasets like GLUE1, SuperGLUE2, SQuAD3, or WMT4 for standardized comparisons. Ablation Studies: Analyze the impact of model components by systematically removing or modifying parts of the model. Scalability and Efficiency Testing: Test for performance across different dataset sizes. Evaluate computational efficiency (e.g., inference speed, training time). Generalization and Robustness: Test on out-of-distribution data or adversarial examples. Use cross-lingual or domain-specific datasets. 2. Relevant MetricsA. Task-Specific Metrics Classification Tasks: Accuracy Precision Recall F1-Score Area Under the Curve (AUC) for ROC&#x2F;PR curves Text Generation Tasks: BLEU (Bilingual Evaluation Understudy) ROUGE (Recall-Oriented Understudy for Gisting Evaluation) METEOR (Metric for Evaluation of Translation with Explicit Ordering) Question Answering: Exact Match (EM) F1-Score Language Modeling: Perplexity Bits-per-character (BPC) Named Entity Recognition (NER): F1-Score for Entity-Level Precision&#x2F;Recall Text Summarization: ROUGE-1, ROUGE-2, ROUGE-L B. Efficiency Metrics Computational Efficiency: FLOPs (Floating Point Operations per Second) Latency (time per inference) Training Time Memory Usage: GPU&#x2F;CPU memory requirements Model size (in MB or parameters) Energy Consumption: Energy usage during training&#x2F;inference 3. Parameters for Testing Model Parameters: Number of layers Hidden size Attention heads Dataset Characteristics: Dataset size Distribution (balanced vs. imbalanced classes) Language&#x2F;domain Hyperparameters: Learning rate Batch size Dropout rate Optimization algorithm (e.g., AdamW, SGD) Infrastructure: Hardware (e.g., GPU, TPU, or CPU) Software (e.g., TensorFlow, PyTorch) 4. Reusability for Variable Changes Modular Code: Ensure model code allows for easy swapping of components (e.g., tokenizer, embedding layer). Parameterization: Use configuration files (YAML&#x2F;JSON) to define model parameters and settings. Reproducibility: Log training runs using tools like TensorBoard, Weights &amp; Biases, or MLflow. Fix random seeds for deterministic results. Automated Testing: Implement pipelines to rerun experiments with new variables. ConclusionThe comparison of NLP models requires a multifaceted approach, evaluating both task performance and computational efficiency. Selecting appropriate metrics and parameters ensures comprehensive insights into model strengths and weaknesses. By maintaining modularity and automation, reusability across experiments is simplified, enabling iterative improvements and robust testing. 1.GLUE - The General Language Understanding Evaluation (GLUE) benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems. ↩2.SuperGLUE is a benchmark dataset designed to pose a more rigorous test of language understanding than GLUE. ↩3.SQuAD - Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. ↩4.WMT: Workshop on Statistical Machine Translation focuses on news text translation. It includes language pairs such as English to/from various languages like Chinese, Czech, German, Hausa, Icelandic, Japanese, Russian, and more. ↩","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"ML","slug":"ML","permalink":"https://ooge0.github.io/hexo-blog/tags/ML/"},{"name":"NLP","slug":"NLP","permalink":"https://ooge0.github.io/hexo-blog/tags/NLP/"}]},{"title":"NLP tasks","slug":"post_ai_nlp__nlp_tasks_intro_1","date":"2024-11-17T19:39:11.000Z","updated":"2024-11-27T11:10:24.687Z","comments":true,"path":"2024/11/17/post_ai_nlp__nlp_tasks_intro_1/","permalink":"https://ooge0.github.io/hexo-blog/2024/11/17/post_ai_nlp__nlp_tasks_intro_1/","excerpt":"","text":"Natural Language Processing (NLP) encompasses a variety of tasks that enable machines to understand, interpret, and generate human language. Below is an overview of some of the most important NLP tasks and their applications. 1. Sentiment AnalysisDescription: Determines the sentiment or emotion expressed in text (positive, negative, neutral).Applications: Social media monitoring, product reviews, customer feedback.Datasets: IMDB Reviews, Sentiment140.References: GLUE1 IMDB Dataset2 Docs: Paper: Lexicon-Based Methods for Sentiment Analysis.2011 DOI: 10.1162&#x2F;COLI_a_00049 Read on aclanthology.org Read on sci-hub.se 2. Named Entity Recognition (NER)Description: Identifies entities in text such as names of people, organizations, locations, etc.Applications: Information extraction, legal document analysis, bioinformatics.Datasets: CoNLL-2003, OntoNotes.References: CoNLL-20033 OntoNotes4 3. Machine TranslationDescription: Automatically translates text from one language to another.Applications: Cross-lingual communication, global business, localization.Datasets: WMT, OpenSubtitles.References: WMT Dataset5 OpenSubtitles6 4. Text SummarizationDescription: Produces a concise summary of a longer text while retaining essential information.Applications: News summarization, legal briefings, academic research.Datasets: CNN&#x2F;DailyMail, XSum.References: CNN&#x2F;DailyMail7 XSum8 5. Text ClassificationDescription: Categorizes text into predefined classes (e.g., spam detection, topic classification).Applications: Email filtering, topic modeling, document organization.Datasets: AG News, Reuters-21578.References: AG News9 Reuters-2157810 6. Question Answering (QA)Description: Answers questions based on a given text passage or context.Applications: Chatbots, search engines, educational tools.Datasets: SQuAD, TriviaQA.References: SQuAD11 TriviaQA12 7. Language ModelingDescription: Predicts the next word or sequence of words in a text.Applications: Autocomplete, text generation, conversational AI.Datasets: WikiText, Penn Treebank.References: WikiText13 Penn Treebank14 8. Part-of-Speech (POS) TaggingDescription: Assigns grammatical categories (e.g., noun, verb, adjective) to words in a sentence.Applications: Grammar correction, syntactic analysis, linguistic research.Datasets: Universal Dependencies, WSJ Corpus.References: Universal Dependencies15 WSJ Corpus16 9. Coreference Resolution (CR)Description: CR17 Identifies and links all expressions in a text that refer to the same entity.Applications: Dialogue systems, summarization, information extraction.Datasets: OntoNotes, CoNLL-2012, TED Talks Dataset, Europarl DatasetReferences: OntoNotes4 CoNLL-201218 CorefRoBERTa19 Footnotes1.GLUE - The General Language Understanding Evaluation (GLUE) benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems. ↩2.IMDB Dataset - Large Movie Review Dataset for Sentiment Analysis by Stanford. ↩3.CoNLL-2003 - Dataset for Named Entity Recognition and other sequence modeling tasks. ↩4.OntoNotes - Annotated dataset covering various linguistic phenomena. ↩5.WMT - Workshop on Machine Translation datasets for translation tasks. ↩6.OpenSubtitles - Large corpus of subtitles for multilingual tasks. ↩7.CNN/DailyMail - Dataset for abstractive summarization tasks. ↩8.XSum - Dataset for extreme summarization of news articles. ↩9.AG News - News topic classification dataset. ↩10.Reuters-21578 - Text categorization benchmark dataset. ↩11.SQuAD - Stanford Question Answering Dataset for reading comprehension tasks. ↩12.TriviaQA - Dataset containing trivia questions and evidence passages. ↩13.WikiText - Dataset for language modeling based on Wikipedia articles. ↩14.Penn Treebank - Corpus for linguistic annotation and modeling. ↩15.Universal Dependencies - Framework for consistent grammatical annotation across languages. ↩16.WSJ Corpus - Dataset from Wall Street Journal articles for POS tagging. ↩17.Coreference resolution (CR) is the task of finding all linguistic expressions (called mentions) in a given text that refer to the same real-world entity. ↩18.CoNLL-2012 - Shared task on coreference resolution and other NLP challenges. ↩19.Annotated dataset created to evaluate RoBERTa’s performance on coreference tasks, with a focus on contextual embeddings. ↩","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"ML","slug":"ML","permalink":"https://ooge0.github.io/hexo-blog/tags/ML/"},{"name":"NLP","slug":"NLP","permalink":"https://ooge0.github.io/hexo-blog/tags/NLP/"}]},{"title":"ML - colorizing plain text dynamically in HTML","slug":"post_ml_colorizing_plain_text_dynamically_in_html","date":"2024-11-14T09:11:11.000Z","updated":"2024-12-04T09:40:07.885Z","comments":true,"path":"2024/11/14/post_ml_colorizing_plain_text_dynamically_in_html/","permalink":"https://ooge0.github.io/hexo-blog/2024/11/14/post_ml_colorizing_plain_text_dynamically_in_html/","excerpt":"","text":"For colorizing plain text dynamically in HTML, you can take several approaches based on your needs. Here are some known methods, tools, and frameworks, including both lightweight CSS&#x2F;JavaScript solutions and Python-based frameworks that leverage natural language processing (NLP) and machine learning to analyze and colorize text based on sentiment, keywords, or other factors. CSS and JavaScript Solutions CSS and JavaScript CSS Classes: You can create predefined CSS classes for different semantic categories (like .positive, .negative, .neutral, etc.) and apply these classes conditionally using JavaScript. JavaScript with Sentiment Analysis APIs: JavaScript can detect certain keywords or analyze sentiment using a lightweight API (e.g., Aylien, Twinword). The result can be used to assign CSS classes to colorize the text based on positive, neutral, or negative sentiment. Color Thief (JavaScript Library) Color Thief is a small JavaScript library that extracts dominant colors from images. Although it’s primarily for images, you can adapt it to analyze background colors of images, which could then be applied to corresponding text. This is more complex and situational but works well for aesthetic color matching. Python Libraries and Frameworks TextBlob TextBlob is a Python library for processing textual data. It’s easy to use and provides simple API calls to perform sentiment analysis (polarity, subjectivity). The results can help assign colors based on sentiment. Use Case: After analyzing sentiment polarity, you can map scores to color classes and render the HTML using Python-based templating engines like Jinja2. References:Introducing TextBlobA Python library for processing textual data, NLP framework, sentiment analysis, Sentiment Analysis Project Using TextBlob , Sentiment Analysis with TextBlob** NLTK (Natural Language Toolkit) NLTK provides extensive tools for text processing, and it can be used to categorize words, perform named-entity recognition, or detect topics. You could use these tags to assign specific colors to text. Customization: Use NLTK’s tools to tag parts of speech or entities (like names, locations), then apply color based on each type. SpaCy SpaCy is a powerful NLP library that provides pre-trained models for named entity recognition, parts of speech tagging, and more. You can use these categories to assign colors to text based on identified entities. Usage: SpaCy models can highlight organizations, people, or other entities, and each type can be styled with a specific color. NLTK + Seaborn (for Heatmaps or Color Scaling) Using NLTK for sentiment combined with Seaborn (a visualization library) allows you to create color gradients that reflect the text sentiment strength. For example, stronger positive or negative sentiment can be mapped to color intensity. Example: Map positive sentiments to green shades and negative sentiments to red, and then render HTML with colored spans for different text parts. Reference: seaborn.heatmap , Control color in seaborn heatmaps Pillow (for Color Extraction and Analysis) Pillow can extract dominant colors from an image file and can be combined with textual data in certain contexts (e.g., background-image analysis and text overlay). Example: Use dominant colors from an image as a background for text, enhancing UI consistency and readability. Reference: Medium | Extracting All Colors in Images with Python Machine Learning Solutions TensorFlow &#x2F; Keras (Text Classification) TensorFlow&#x2F;Keras can train models to classify text into categories based on tone, emotion, or topic. A neural network model can be trained to detect emotions or sentiment and output classes that correspond to color themes. Implementation: Use a pre-trained model (or train one) for text sentiment or emotion classification. Then, map each category to a color and apply it dynamically. TensorFlow Lite can further optimize the model for lighter web applications. Hugging Face Transformers The Hugging Face library offers a collection of transformer models for NLP tasks, including sentiment analysis, topic detection, and entity recognition. Using a model like DistilBERT or RoBERTa, you can analyze text and assign colors based on predicted classes. Application: After running text through the model, assign colors based on its classification (e.g., color positive sentiments in green, neutral in gray, and negative in red). OpenAI GPT-3 &#x2F; GPT-4 API OpenAI’s GPT-3 and GPT-4 can analyze text to detect tone, sentiment, or even highlight keywords or important information. You could send text to the API and use its outputs to apply colors conditionally based on analysis. How To Use: **Call the API to get sentiment or topic analysis, then use CSS classes to colorize the HTML text dynamically based on the response. Web-Based Text Analysis Tools with APIs Aylien, MonkeyLearn, and Twinword These are third-party text analysis APIs that offer sentiment analysis, entity recognition, and other NLP features. They can be accessed from both Python and JavaScript environments. Integration: Fetch analysis results via API and use the output categories to style HTML text accordingly. Summary of Recommendations Quick, Lightweight Solution: Use CSS classes with JavaScript or APIs like Aylien and TextBlob for sentiment-based coloring. Python-Based NLP and Visualization: Use TextBlob, NLTK, and Seaborn for Python-only projects requiring sentiment-based or entity-based coloring. Advanced Customization: Use SpaCy, Hugging Face models, or TensorFlow&#x2F;Keras if you need deeper analysis and customization, especially for applications that benefit from entity detection or complex classifications.","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"ML","slug":"ML","permalink":"https://ooge0.github.io/hexo-blog/tags/ML/"},{"name":"html","slug":"html","permalink":"https://ooge0.github.io/hexo-blog/tags/html/"},{"name":"tutorial","slug":"tutorial","permalink":"https://ooge0.github.io/hexo-blog/tags/tutorial/"}]},{"title":"Генеративный искусственный интеллект / Generative Artificial Intelligence","slug":"post_ai__base_gan","date":"2024-11-13T04:18:22.000Z","updated":"2024-11-12T23:21:10.300Z","comments":true,"path":"2024/11/13/post_ai__base_gan/","permalink":"https://ooge0.github.io/hexo-blog/2024/11/13/post_ai__base_gan/","excerpt":"","text":"Генеративный ИИ (Generative AI) — это тип искусственного интеллекта, который создаёт новые данные на основе уже существующих, будь то текст, изображения, звук или другие типы контента. В отличие от традиционных моделей ИИ, которые просто классифицируют или анализируют данные, генеративные модели могут генерировать новые образцы, схожие с обучающим набором данных. Например, такие модели могут создавать реалистичные изображения, стилизовать текст, писать код и т.д. Особенности генеративного ИИ Создание нового контента: Генеративные модели способны создавать уникальные образцы, которые не являются прямым копированием данных из обучающей выборки. Высокая адаптивность: Они могут адаптироваться к разным типам задач, таким как текстовая генерация, создание изображений, синтез речи, и даже проектирование молекул. Использование больших объемов данных: Для создания реалистичного и высококачественного контента модели требуют больших обучающих выборок, чтобы изучить закономерности и зависимости. Сложность в контроле результатов: Генеративные модели могут производить нежелательные результаты или даже давать вредные советы, поскольку сложно заранее контролировать все возможные выводы. Разновидности генеративного ИИ Генеративные состязательные сети (Generative Adversarial Networks, GANs) GAN-сети состоят из двух нейросетей — генератора и дискриминатора, которые работают вместе. Генератор создаёт новые образцы данных, стремясь “обмануть” дискриминатор, который определяет, являются ли данные реальными или синтетическими. Этот подход используется для генерации реалистичных изображений, синтеза речи и других задач, связанных с созданием визуально или акустически правдоподобного контента. Автокодировщики и вариационные автокодировщики (Autoencoders, VAEs) Автокодировщики представляют собой нейронные сети, которые учатся кодировать данные в сжатое представление, а затем восстанавливать их обратно. Вариационные автокодировщики (VAEs) делают это с добавлением вероятностной оценки, что позволяет генерировать более разнообразные результаты. VAEs используются для создания изображений, аудио и текста, а также для задач, где требуется контроль за параметрами выходных данных. Трансформеры и большие языковые модели (LLM) Трансформеры, такие как GPT (Generative Pre-trained Transformer), BERT и их производные, представляют собой архитектуры, особенно популярные для обработки текста. Они обучаются на больших наборах текстов и могут генерировать новые тексты, писать код, вести диалог, переводить и обрабатывать текстовые данные в различных форматах. Такие модели особенно сильны в задачах обработки языка и демонстрируют высокую гибкость в задачах текстовой генерации и анализа. Диффузионные модели Эти модели постепенно добавляют шум к данным, чтобы сделать их менее определёнными, а затем “восстанавливают” данные обратно к оригиналу, но уже с вариативностью. Диффузионные модели используются для генерации изображений и других типов данных, где требуется высокий уровень детализации и вариативности. Рекуррентные нейронные сети (RNN) Хотя RNN не так распространены, как трансформеры для генеративных задач, они до сих пор применяются в ряде случаев, особенно когда важен контекст последовательности (например, в задачах синтеза музыки или генерации текста). LSTM и GRU (в рамках RNN) также могут быть использованы для генеративных задач, где важна зависимость от временной последовательности. Чем генеративный ИИ отличается от других типов ИИ Генеративный ИИ отличается от других типов искусственного интеллекта по ряду характеристик: Создание новых данных vs. анализ и предсказание: Генеративный ИИ создаёт новые данные (например, текст или изображение), которые не существовали в изначальном виде. Дискриминативные модели (например, классификаторы) нацелены на анализ, предсказание и категоризацию, а не на генерацию. Они используют алгоритмы, такие как деревья решений, SVM или CNN для задач распознавания образов и классификации, но не могут самостоятельно создавать новые данные. Использование для творчества и генерации контента: Генеративные модели активно используются для творчества и задач, связанных с созданием мультимедиа-контента. Дискриминативные модели используются в задачах, где требуется анализ существующих данных, например, для диагностики заболеваний, предсказания погоды, выявления мошенничества. Комплексность и требования к обучению: Генеративные модели требуют более крупных и сложных обучающих выборок, чтобы изучить шаблоны и закономерности. Дискриминативные модели, как правило, легче обучаются и требуют меньших вычислительных ресурсов, особенно в задачах бинарной классификации. Применение генеративного ИИГенеративный ИИ находит применение во множестве сфер: Развлечения и медиа: создание музыки, текста, изображений, сценариев. Маркетинг: генерация контента для социальных сетей и рекламы. Наука и медицина: моделирование молекул, создание изображений для диагностики. Разработка продуктов: дизайн, генерация прототипов и улучшение изображений. ЗаключениеГенеративный ИИ — это мощный инструмент, способный создать уникальные данные и открыть новые возможности в различных областях. От традиционных моделей ИИ он отличается именно своей способностью генерировать новые данные, а не просто анализировать и классифицировать существующие.","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"science","slug":"science","permalink":"https://ooge0.github.io/hexo-blog/tags/science/"},{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"ML","slug":"ML","permalink":"https://ooge0.github.io/hexo-blog/tags/ML/"}]},{"title":"My glossaries","slug":"post_my_glossaries","date":"2024-11-11T22:00:00.000Z","updated":"2024-12-04T09:27:25.701Z","comments":true,"path":"/knowledge/my-glossaries/","permalink":"https://ooge0.github.io/hexo-blog/knowledge/my-glossaries/","excerpt":"","text":"This post contains list of glossary&#x2F;vacabularies 🏥 Medical glossaries: ⚕️ General medical glossary: Glossary created by MkDocs Web link: https://ooge0.github.io/glossary/medical-glossary-1/ Parsed resource: https://medconsonline.com/en/glossary 🧙 General oncology glossary: Glossary created by MkDocs Web link: https://ooge0.github.io/glossary/oncology-glossary-1/ Parsed resource: https://chemoteka.com.ua/dictionary 🗿 Philosophy glossaries: General philosophy glossary(ENG): Glossary created by MkDocs Web link: https://ooge0.github.io/glossary/en/en__philosophy_glossary_1/ Parsed resource: plato.stanford.edu, iep.utm.edu, britannica.com&#x2F;topic&#x2F;philosophy General philosophy glossary(UA): Glossary created by MkDocs Web link: https://ooge0.github.io/glossary/uk/ua__philosophy_glossary_1/ Parsed resource: plato.stanford.edu, iep.utm.edu, britannica.com&#x2F;topic&#x2F;philosophy","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"science","slug":"science","permalink":"https://ooge0.github.io/hexo-blog/tags/science/"},{"name":"glossary","slug":"glossary","permalink":"https://ooge0.github.io/hexo-blog/tags/glossary/"},{"name":"knowledge","slug":"knowledge","permalink":"https://ooge0.github.io/hexo-blog/tags/knowledge/"}]},{"title":"blog deploy actions","slug":"maintanance/deploy_hexo","date":"2024-11-11T09:11:11.000Z","updated":"2024-11-14T09:49:09.007Z","comments":true,"path":"2024/11/11/maintanance/deploy_hexo/","permalink":"https://ooge0.github.io/hexo-blog/2024/11/11/maintanance/deploy_hexo/","excerpt":"","text":"github.com&#x2F;hexo-blog&#x2F;actions","categories":[{"name":"Maintanance","slug":"Maintanance","permalink":"https://ooge0.github.io/hexo-blog/categories/Maintanance/"}],"tags":[]},{"title":"AI glossaries list","slug":"post_ai__glossary_list","date":"2024-11-01T15:37:30.000Z","updated":"2024-11-20T19:21:09.368Z","comments":true,"path":"2024/11/01/post_ai__glossary_list/","permalink":"https://ooge0.github.io/hexo-blog/2024/11/01/post_ai__glossary_list/","excerpt":"","text":"List contains glossaries with brief overview Glossary-of-Artificial-Intelligence GitHub project: https://github.com/ahammadmejbah/Glossary-of-Artificial-Intelligence A “Glossary of Artificial Intelligence” is a concise reference resource defining key terms, concepts, and terminology related to AI. It provides explanations and definitions to help individuals understand and navigate the field of artificial intelligence, making it a valuable tool for both beginners and experts in the AI domain. Text + pictures + some python code snippets It covers just ‘A’ letter. Artificial Intelligence and Data Science GitHub project:https://aim-rsf.github.io/Glossary-of-Terms/terms/Artificial-Intelligence.html Glossary contains terms relating to Artificial Intelligence and Data Science. Basic terms for artificial intelligence and data science. Just few terms AI-Glossary A glossary of terms in AI and their corresponding papers. GitHub project: https://github.com/odisha-ml/AI-Glossary Published version: glossary.odishaai.org Generative AI Terminology GitHub project: https://github.com/nomic-ai/gpt4all/wiki/Generative-AI-Terminology AI Dictionary Published version: https://nhsx.github.io/ai-dictionary Online glossary of machine learning terms machinelearning.wtf Well srtuctured and informative resource Other NIST glossary","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"}]},{"title":"Sitemap","slug":"maintanance/sitemap","date":"2024-11-01T10:00:00.000Z","updated":"2024-11-02T09:26:44.121Z","comments":true,"path":"2024/11/01/maintanance/sitemap/","permalink":"https://ooge0.github.io/hexo-blog/2024/11/01/maintanance/sitemap/","excerpt":"","text":"hexo-blog sitemap","categories":[{"name":"Maintanance","slug":"Maintanance","permalink":"https://ooge0.github.io/hexo-blog/categories/Maintanance/"}],"tags":[]},{"title":"HEXIO.IO fixes guide","slug":"tutorials/HEXO.IO_fixes_guide","date":"2024-11-01T07:22:44.472Z","updated":"2024-12-10T12:51:30.094Z","comments":true,"path":"2024/11/01/tutorials/HEXO.IO_fixes_guide/","permalink":"https://ooge0.github.io/hexo-blog/2024/11/01/tutorials/HEXO.IO_fixes_guide/","excerpt":"","text":"For making changes in blog design check that: default configuration of HEXO.IO is not changed (You installed everythong from initial command from Hexo Setup page; you made copy of original project; be ready to install some additional plugins that cna be oudiated or required additionla configurations TOC1. Moving side panel to the left side.2. Removing(hide) Search form on the header3. Changes project structure. If blog published not iun the root of GitHub4. Sitemap for the blog. 5. Add HEXO.IO version details to the footer6. Search option (internal HEXO.IO feature)7. Managing (publishing) several projects from the same local machine (GitHub access + token issues)8. Pagination issue after deploying on GitHub pages Below is a list of changes and improvements. 1. Moving side panel to the left side. Moving side panel to the left side required editing \\themes\\landscape\\_config.yml and add sidebar: left 2. Removing(hide) Search form on the header For hiding ‘Search’ form from the header bar, 1. open \\themes\\landscape\\layout\\_partial\\header.ejs (it works if theme is landscape (It was not chekced on default theme!!!) 2. comment linesin two places: 1. 1st 1&lt;a class=&quot;nav-icon nav-search-btn&quot; title=&quot;&lt;%= __(&#x27;search&#x27;) %&gt;&quot;&gt;&lt;span class=&quot;fa fa-search&quot;&gt;&lt;/span&gt;&lt;/a&gt; 2. and 2nd 1&lt;%- search_form(&#123;button: &#x27;&amp;#xF002;&#x27;, text: __(&#x27;search&#x27;)&#125;) %&gt; 3. Changes project structure. If blog published not iun the root of GitHub If HEXO.IO project will be publoished not in teh root of GitHub , the _comfig.yaml should be updated. Aftrer changes the project structure and making different folders inside the _post folder the blog navigation may not work. Issue is related to at least to sides: 1. When post item has dedicated permalink, project ingine for some reason (if post published no wtih root) conflict of 4. Sitemap for the blog. Install the Sitemap Plugin In your Hexo project directory, run the following command to install hexo-generator-sitemap: 1npm install hexo-generator-sitemap --save This will add the sitemap generator plugin to your project, which automatically generates a sitemap.xml file each time you build your site. Configure _config.yml Open your Hexo _config.yml file (this is the main configuration file in the root directory, not in the theme folder), and add the following configuration under the sitemap settings: 12sitemap: path: sitemap.xml By default, this will create a sitemap.xml file in the root of your site’s output (public&#x2F;) directory. (Optional) Add the Sitemap URL to robots.txt To help search engines find your sitemap, add its URL to a robots.txt file if you haven’t done so already. Here’s how: If you already have a robots.txt file configured, just add this line to it: 1Sitemap: https://your-hexo-site.com/sitemap.xml If you don’t have a robots.txt file, you can create one in source&#x2F;robots.txt, and then add the line above with your site URL. Generate and Deploy Run the following commands to generate your site and deploy it: 12bashhexo clean ; hexo generate ;hexo deploy After deploying, your sitemap should be accessible at https://your-hexo-site.com/sitemap.xml. Verifying the Sitemap Visit https://your-hexo-site.com/sitemap.xml in your browser to confirm the sitemap is generated and accessible. Submit your sitemap to Google Search Console or other search engine webmaster tools for better indexing. 5. Add HEXO.IO version details to the footer Create or edit a file in your theme’s scripts directory, such as hexo_info.js, for getting hexo verion details:12345678910111213141516const &#123; execSync &#125; = require(&#x27;child_process&#x27;);hexo.extend.helper.register(&#x27;hexoVersion&#x27;, function () &#123;const &#123; version &#125; = require(&#x27;hexo/package.json&#x27;);return version;&#125;);hexo.extend.helper.register(&#x27;hexoClient&#x27;, function () &#123;try &#123;const output = execSync(&#x27;hexo --version&#x27;, &#123; encoding: &#x27;utf8&#x27; &#125;);const match = output.match(/hexo-cli: (\\d+\\.\\d+\\.\\d+)/);return match ? match[1] : &#x27;Unknown&#x27;;&#125; catch (error) &#123;return &#x27;Error retrieving CLI version&#x27;;&#125;&#125;); Create ejs file where function retrives existing HEXO.IO details like HEXO.IO version and hexo-client version 12345&lt;div id=&quot;footer-info&quot; class=&quot;inner&quot;&gt;&amp;copy; &lt;%= date(new Date(), &#x27;YYYY&#x27;) %&gt; &lt;%= config.author || config.title %&gt;&lt;br&gt;&lt;%= __(&#x27;powered_by&#x27;) %&gt; &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot;&gt;Hexo&lt;/a&gt; (v&lt;%= hexoVersion() %&gt;)&lt;br&gt;Hexo CLI: &lt;%= hexoClient() %&gt;&lt;/div&gt; 6. Search option (internal HEXO.IO feature) Option-1 (Search option was not possible to configure by these steps)Use script 1npm i -S hexo-generator-search hexo-generator-feed hexo-renderer-less hexo-autoprefixer hexo-generator-json-content hexo-recommended-posts hexo-generator-search https://github.com/wzpan/hexo-generator-search 1$ npm install hexo-generator-search --save hexo-generator-feed hexo-renderer-less hexo-autoprefixer hexo-generator-json-content hexo-recommended-posts 7. Managing (publishing) several projects from the same local machine (GitHub access + token issues)To publish a new Hexo blog to a different GitHub account, you need to configure the deployment settings correctly and ensure the required deployer plugin is installed. Follow these steps: Install the Git Deployer Plugin.Hexo uses plugins for deployment. Install the required plugin: 1npm install hexo-deployer-git --save Update _config.yml for DeploymentEdit the main _config.yml file in your Hexo project directory to include deployment settings for the target GitHub account. Use the following structure: 1234deploy:type: gitrepo: https://&lt;username&gt;:&lt;personal_access_token&gt;@github.com/&lt;username&gt;/&lt;repository_name&gt;.gitbranch: main Replace with the GitHub username for the new account. Replace with a Personal Access Token (PAT) for authentication. Replace with the name of the repository where you want to publish the blog. Replace main with the default branch of your repository, if different. Notes:You can create a Personal Access Token from the new GitHub account under Settings &gt; Developer settings &gt; Personal Access Tokens.Use https for authentication to simplify the setup. Initialize the Deployment RepositoryIf you haven’t already, initialize a repository for the new GitHub account: Log in to the new GitHub account. Create a new repository for your blog. Copy the repository URL and ensure it matches what you’ve configured in _config.yml. Deploy the BlogRun the following command to generate the site and deploy it to the new GitHub repository: 123hexo cleanhexo generatehexo deploy 8. Pagination issue after deploying on GitHub pagesPagination of hexo.io blog works fine on localhost but not working after deploy on GitHub pages.http://localhost:4000/hexo-blog/categories/Posts/_posts/2/ works fine, page opens with contenthttps://ooge0.github.io/hexo-blog/categories/Posts/_posts/2/ shows that page not found Helpful Resources Hexo Documentation - Pagination Solutions: Enable Pretty URLs. Ensure that Hexo generates an index.html file for each page in the pagination. In Hexo, this can be achieved by enabling the proper permalink structure.In the main _config.yml, check or add the following: 1234permalink: :categories/:title/pretty_urls:trailing_index: falsetrailing_html: false This generates URLs like &#x2F;categories&#x2F;Posts&#x2F;_posts&#x2F;2&#x2F; and ensures that an index.html is created in the corresponding folder. In my case permalink section in _config.yml was 1234permalink: :year/:month/:day/:title/pretty_urls:trailing_index: falsetrailing_html: falseAfter changes _config.yml became 1234permalink: :categories/:title/pretty_urls:trailing_index: falsetrailing_html: false Result: On localhost pagination is working On GitHub pages pagination still not working Change pagination_dir from pagination_dir: _posts to pagination_dir: Posts in _config.ymlResult: On localhost pagination is working On GitHub pages pagination still not working ResoluitonPagination issue was related to extending of structure of HEXO project. Nested folder inside the _post folder brakes pagination only on remote host. In this case created individual folders for posts that are related to different topics produces internal conflict of generated by HEXO.IO engine documents that can possible resolve to add the name of the additionally created folder where messages are stored. For this project was created additionally several individual folders , but HEXO accept to use all posts by adding just one name of folder. In this case Posts HEXO.IO footnotesTo use footnotes in format 123 1npm install hexo-footnotes --save If Hexo detect automatically all plugins, that’s all. If that is not the case, register the plugin in your _config.yml file : 12plugins: - hexo-footnotes1.footnote #1 ↩2.footnote #2 ↩3.footnote #3 You just have to install the package with ↩","categories":[{"name":"Tutorials","slug":"Tutorials","permalink":"https://ooge0.github.io/hexo-blog/categories/Tutorials/"}],"tags":[{"name":"tutorial","slug":"tutorial","permalink":"https://ooge0.github.io/hexo-blog/tags/tutorial/"},{"name":"blog","slug":"blog","permalink":"https://ooge0.github.io/hexo-blog/tags/blog/"},{"name":"hexo_io","slug":"hexo-io","permalink":"https://ooge0.github.io/hexo-blog/tags/hexo-io/"}]},{"title":"How to Format a USB Using CMD (Command Prompt)","slug":"notes/format-a-usb-using-cmd","date":"2024-10-30T22:00:00.000Z","updated":"2024-11-01T17:52:30.919Z","comments":true,"path":"2024/10/31/notes/format-a-usb-using-cmd/","permalink":"https://ooge0.github.io/hexo-blog/2024/10/31/notes/format-a-usb-using-cmd/","excerpt":"","text":"source: https://www.wikihow.com/Format-a-USB-Using-Cmd A step-by-step guide to use the Command Prompt to reformat and clean a flash drive Plug your USB drive1 into your computer. Open Command Prompt as an administrator: Option 1: Go to the Start menu, type CMD, and select Run as administrator. Option 2: Press WIN + R, type cmd, and press ↵ Enter. Type diskpart and press ↵ Enter. Type list disk and press ↵ Enter. Type select disk [Your disk] and press ↵ Enter. Type clean and press ↵ Enter. Type create partition primary and press ↵ Enter. Type format fs=ntfs and press ↵ Enter. Type assign letter=[letter] and press ↵ Enter. Type exit and press ↵ Enter. Referencse: 1.USB drive. ↩","categories":[{"name":"Notes","slug":"Notes","permalink":"https://ooge0.github.io/hexo-blog/categories/Notes/"}],"tags":[{"name":"usb","slug":"usb","permalink":"https://ooge0.github.io/hexo-blog/tags/usb/"},{"name":"cmd","slug":"cmd","permalink":"https://ooge0.github.io/hexo-blog/tags/cmd/"}]},{"title":"Symbol encoding (Unicode and family)","slug":"notes/symbol_encoding_unicode_note_1","date":"2024-10-30T22:00:00.000Z","updated":"2024-11-12T23:13:46.937Z","comments":true,"path":"2024/10/31/notes/symbol_encoding_unicode_note_1/","permalink":"https://ooge0.github.io/hexo-blog/2024/10/31/notes/symbol_encoding_unicode_note_1/","excerpt":"","text":"In the world of text and symbols, many encoding systems allow for displaying characters, symbols, and icons in different contexts. Unicode is the primary standard, but there are several other encoding systems and symbol categories, each serving specialized purposes. Here’s a breakdown: Key Symbol Encoding Systems and Categories Unicode Usage: The universal encoding standard supporting characters from virtually all languages, as well as emoji, symbols, and icons. Categories: Includes letters, numbers, emoji, mathematical symbols, box-drawing characters, and more. Examples: Alphabet (A, B), numbers (1, 2), emoji (😊), math symbols (√, ∑), and other unique characters like ↵. Resources: https://emojidb.org/ ASCII (American Standard Code for Information Interchange) Usage: Basic Latin letters, numbers, punctuation, and control characters for simple text encoding. Limitations: Limited to 128 characters, excluding many languages and special symbols. Examples: Basic Latin alphabet (A-Z), numbers (0-9), punctuation marks like . and ,. ISO&#x2F;IEC 8859 (ISO Latin) Usage: Supports European languages with additional Latin characters, like accented letters. Categories: Variants (ISO 8859-1, ISO 8859-5) cover Western European, Eastern European, and Cyrillic scripts. Limitations: Restricted to 256 characters per set. ANSI (Windows-1252) Usage: Commonly used on Windows; extends ASCII with additional characters for Western European languages. Limitations: Non-standard on non-Windows systems. Examples: Includes symbols like €, accented letters, and punctuation. Emoji (Unicode Extended Pictographic Symbols) Usage: Unicode provides a broad range of emoji for digital communication, sorted into categories. Categories: Smileys, People, Animals, Food, Travel, and more. Notes: Rendered differently across platforms (Apple, Google, Microsoft, etc.). HTML Entities Usage: Encodes special symbols in HTML with named (&amp;) or numeric (&#169;) entities. Notes: Often Unicode-based, written as character references in HTML documents for web use. LaTeX Symbols Usage: Essential in academic and scientific fields, especially for mathematical notation. Categories: Greek letters, math operators, functions (\\alpha, \\sum, \\infty). Notes: Not an encoding system but a method to render symbols in scientific documents. Dingbats Usage: Decorative symbols, often used for visual enhancement. Examples: Check marks, arrows, stars. Notes: Originated in typefaces like Zapf Dingbats; Unicode includes a Dingbats block. Box Drawing Characters Usage: Used for simple graphics and box drawing in text files or terminal interfaces. Examples: ┌, ─, ┐, │, ┘. Notes: Useful for ASCII art and table layouts. Control Characters Usage: Non-printing characters to control text display, such as line breaks and tabs. Categories: Includes Line Feed (LF), Carriage Return (CR), and others. Private Use Area (PUA) Usage: Allows companies or individuals to define custom symbols not in Unicode. Examples: Proprietary symbols, company logos. Notes: Non-standard and specific to certain systems or applications. Each of these systems provides unique ways to display and interact with symbols. While Unicode has largely unified global encoding, non-Unicode systems still fulfill niche needs, especially in older systems or specialized fields. Here are a few sources for locating and copying symbols: Unicode Tables home.unicode.org provides official charts for different Unicode blocks. Each chart lists symbols for a specific category, like arrows, geometric shapes, mathematical symbols, etc. You can search by character name or code point to find the exact symbol you need. Example: Unicde = &amp;U+1F47D &#x3D; hexadecimal format = &amp;#x1F47D; result &#x3D; &#x1F47D; Character Map Utility (Windows) Windows includes a built-in tool called “Character Map” (charmap.exe) where you can browse and copy symbols from various fonts that support Unicode. Open it by typing “Character Map” in the Start menu search bar, then find and copy symbols to paste them into your project. Emoji and Symbol Libraries Online Websites like Unicode Table and Coolsymbol provide searchable libraries of Unicode symbols, emojis, and text decorations. You can copy the symbol you want directly from these sites. HTML Entity Reference Some symbols have HTML entity codes (e.g., &larr; for ←). Sites like DevDocs or W3Schools list HTML entities if you want to embed symbols in web pages.","categories":[{"name":"Notes","slug":"Notes","permalink":"https://ooge0.github.io/hexo-blog/categories/Notes/"}],"tags":[{"name":"unicode","slug":"unicode","permalink":"https://ooge0.github.io/hexo-blog/tags/unicode/"},{"name":"ASCII","slug":"ASCII","permalink":"https://ooge0.github.io/hexo-blog/tags/ASCII/"},{"name":"LaTeX","slug":"LaTeX","permalink":"https://ooge0.github.io/hexo-blog/tags/LaTeX/"}]},{"title":"HEXIO.IO layout improvements","slug":"tutorials/HEXO.IO_layout_changes","date":"2024-10-28T18:27:31.134Z","updated":"2024-11-02T13:52:00.138Z","comments":true,"path":"/hexo-layout-improvements/","permalink":"https://ooge0.github.io/hexo-blog/hexo-layout-improvements/","excerpt":"","text":"Table of content Decreasing size of main body area changes-1 changes-2 sub-nav issue with Search form If changes leads to broken portraite view for mobile devices . Adding footnotes Show some number of tags in the ‘Tag’ section. Decreasing size of main body areaheader-inner (section that contains elements of navigation menu) Edited file \\themes\\landscape\\source\\css\\_partial\\header.styl Before changes #header-inner 123#header-inner position: relative overflow: hidden Before changes $nav-link 1234567891011121314151617181920$nav-link &#123; float: left; color: #312602; opacity: 0.6; text-decoration: none; text-shadow: -1px -1px 0 #ff7f50, // Top-left shadow 1px -1px 0 #ff7f50, // Top-right shadow -1px 1px 0 #ff7f50, // Bottom-left shadow 1px 1px 0 #ff7f50, // Bottom-right shadow 1px 1px 2px rgba(0, 0, 0, 0.4); // Original shadow for depth transition: opacity 0.2s; display: block; padding: 20px 15px; &amp;:hover &#123; opacity: 1; &#125;&#125; changes-1After changes-1 #header-inner 1234567#header-inner &#123; position: absolute; bottom: 0; // Move to the bottom of the parent section left: 0; // Align to the left if needed width: 80%; // Ensure it spans the width of the parent section overflow: hidden;&#125; Links on the nav bar are not possible to click changes-2After changes-2 #header-inner 12345678#header-inner &#123; position: absolute; bottom: 0; // Move to the bottom of the parent section left: 0; // Align to the left if needed width: 80%; // Ensure it spans the width of the parent section z-index: 10; // Ensure it is above other elements&#125; and After changes-2 $nav-link 12345678910111213141516171819202122$nav-link &#123; float: left; color: #312602; opacity: 0.6; text-decoration: none; text-shadow: -1px -1px 0 #ff7f50, // Top-left shadow 1px -1px 0 #ff7f50, // Top-right shadow -1px 1px 0 #ff7f50, // Bottom-left shadow 1px 1px 0 #ff7f50, // Bottom-right shadow 1px 1px 2px rgba(0, 0, 0, 0.4); // Original shadow for depth transition: opacity 0.2s; display: block; padding: 20px 15px; position: relative; // Ensures correct stacking within #header-inner &amp;:hover &#123; opacity: 1; cursor: pointer; // Ensures pointer cursor on hover &#125;&#125; nav-bar fixed and works fine for desktop and mobile view. sub-nav issue with Search formLayout of Search form is shifted for mobile devices.File \\themes\\landscape\\source\\css\\_partial\\header.stylshould be changed Before changes-1 header.styl 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#sub-nav float: right margin-right: -15px#search-form-wrap position: absolute top: 15px width: 150px height: 30px right: -150px opacity: 0 transition: 0.2s ease-out &amp;.on opacity: 1 right: 0 @media mq-mobile width: 100% right: -100%.search-form position: absolute top: 0 left: 0 right: 0 background: #fff padding: 5px 15px border-radius: 15px box-shadow: 0 0 10px rgba(0, 0, 0, 0.3).search-form-input border: none background: none color: color-default width: 100% font: 13px font-sans outline: none &amp;::-webkit-search-results-decoration &amp;::-webkit-search-cancel-button -webkit-appearance: none.search-form-submit position: absolute top: 50% right: 10px margin-top: -7px font: 13px font-icon border: none background: none color: #bbb cursor: pointer &amp;:hover, &amp;:focus color: #777 after changes-1 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#sub-nav &#123; float: right; margin-right: -15px;&#125;#search-form-wrap &#123; position: fixed; // Fixes it to the viewport, not affecting layout top: 15px; right: -200px; // Initially off-screen width: 150px; height: 30px; opacity: 0; transition: transform 0.2s ease-out, opacity 0.2s ease-out; &amp;.on &#123; opacity: 1; transform: translateX(-200px); // Slide into view when activated &#125; @media (max-width: 600px) &#123; // Adjust for mobile screens width: 100vw; right: 0; transform: none; // Keep it fixed in place for small screens opacity: 1; // Keep it visible and responsive &#125;&#125;.search-form &#123; position: relative; top: 0; left: 0; right: 0; background: #fff; padding: 5px 15px; border-radius: 15px; box-shadow: 0 0 10px rgba(0, 0, 0, 0.3);&#125;.search-form-input &#123; border: none; background: none; color: #312602; width: 100%; font: 13px sans-serif; outline: none;&#125;.search-form-submit &#123; position: absolute; top: 50%; right: 10px; transform: translateY(-50%); // Center vertically within input font: 13px sans-serif; border: none; background: none; color: #bbb; cursor: pointer; &amp;:hover, &amp;:focus &#123; color: #777; &#125;&#125; If changes leads to broken portraite view for mobile devices .Input form for the search overlap the menu button. Changes-3Before editing \\themes\\landscape\\source\\css\\style.styl 12.mobile-nav-on overflow: hidden Before editing \\themes\\landscape\\source\\css\\_partial\\header.styl 123456789101112131415161718192021#search-form-wrap &#123; position: fixed; // Fixes it to the viewport, not affecting layout top: 210px; right: -200px; // Initially off-screen width: 150px; height: 30px; opacity: 0; transition: transform 0.2s ease-out, opacity 0.2s ease-out; &amp;.on &#123; opacity: 1; transform: translateX(-200px); // Slide into view when activated &#125; @media (max-width: 600px) &#123; // Adjust for mobile screens width: 70vw; right: 0; transform: none; // Keep it fixed in place for small screens opacity: 1; // Keep it visible and responsive &#125;&#125; After editing \\themes\\landscape\\source\\css\\style.styl 12345.mobile-nav-on #search-form-wrap &#123; opacity: 0; // Hide the search form when the mobile nav is open transform: translateX(-200px); // Keep it off-screen transition: opacity 0.2s ease-out, transform 0.2s ease-out; // Smooth transition&#125; After editing &#96;&#96; 1234567891011121314151617181920212223242526272829303132#search-form-wrap &#123; position: fixed; // Fixes it to the viewport, not affecting layout top: 210px; // Set the vertical position from the top right: -200px; // Initially off-screen width: 150px; // Width for desktop view height: 30px; // Fixed height opacity: 0; // Hidden by default transition: transform 0.2s ease-out, opacity 0.2s ease-out; &amp;.on &#123; opacity: 1; // Fully visible when active transform: translateX(-200px); // Slide into view when activated &#125; @media (max-width: 600px) &#123; // General mobile styles width: 70vw; // Adjust width for mobile view right: 0; // Align to the right of the viewport transform: none; // Keep it fixed in place for small screens opacity: 1; // Keep it visible and responsive &#125; @media (orientation: landscape) and (max-width: 600px) &#123; // Styles for landscape mode on mobile width: 210vw; // Adjust width for landscape orientation top: 180px; // Adjust top position for landscape view right: -50%; // Start off-screen to the left by half the viewport width transition: transform 0.2s ease-out, opacity 0.2s ease-out; // Ensure smooth transition &amp;.on &#123; transform: translateX(50%); // Slide into view from left &#125; &#125;&#125; Adding footnotesUse hexo-reference Plugin for Enhanced LinkingInstall hexo-reference: This plugin improves link handling within Markdown files, including footnotes and internal references. Run this command to install it: 1npm install hexo-reference --save Update _config.yml: Configure hexo-reference in your Hexo _config.yml file to enable the plugin. Add this to your _config.yml file: 123# _config.ymlplugins: - hexo-reference Use Links in Markdown: Now, you can use standard anchor links or footnotes, and hexo-reference will convert them to work correctly on the site. Update your Markdown file like this: 123456**Favorite list**[Takedown](#Favorite-items)## Favorite items Generate and Deploy: After setting up, rebuild and deploy your project: 123hexo cleanhexo generatehexo deploy or 1hexo clean; hexo generate; hexo serve Show some number of tags in the ‘Tag’ section.Before changes of content of themes\\landscape\\layout\\_widget\\tag.ejs file was 12345678&lt;% if (site.tags.length)&#123; %&gt; &lt;div class=&quot;widget-wrap&quot;&gt; &lt;h3 class=&quot;widget-title&quot;&gt;&lt;%= __(&#x27;tags&#x27;) %&gt;&lt;/h3&gt; &lt;div class=&quot;widget&quot;&gt; &lt;%- list_tags(&#123;show_count: theme.show_count&#125;) %&gt; &lt;/div&gt; &lt;/div&gt;&lt;% &#125; %&gt; and after changes of content of themes\\landscape\\layout\\_widget\\tag.ejs file ‘Tags’ section shows 10 tags only 123456789101112&lt;% if (site.tags.length)&#123; %&gt; &lt;div class=&quot;widget-wrap&quot;&gt; &lt;h3 class=&quot;widget-title&quot;&gt;&lt;%= __(&#x27;tags&#x27;) %&gt;&lt;/h3&gt; &lt;div class=&quot;widget&quot;&gt; &lt;% site.tags.sort(&#x27;name&#x27;).limit(10).each(function(tag) &#123; %&gt; &lt;li&gt; &lt;a href=&quot;&lt;%= url_for(tag.path) %&gt;&quot;&gt;&lt;%= tag.name %&gt; (&lt;%= tag.length %&gt;)&lt;/a&gt; &lt;/li&gt; &lt;% &#125;); %&gt; &lt;/div&gt; &lt;/div&gt;&lt;% &#125; %&gt; and Turn on&#x2F;off Tags is possible in _config.yml, 12widgets: - tag","categories":[{"name":"Tutorials","slug":"Tutorials","permalink":"https://ooge0.github.io/hexo-blog/categories/Tutorials/"}],"tags":[{"name":"tutorial","slug":"tutorial","permalink":"https://ooge0.github.io/hexo-blog/tags/tutorial/"},{"name":"blog","slug":"blog","permalink":"https://ooge0.github.io/hexo-blog/tags/blog/"},{"name":"hexo_io","slug":"hexo-io","permalink":"https://ooge0.github.io/hexo-blog/tags/hexo-io/"}]},{"title":"ASCII art","slug":"post_acsii_graphics","date":"2024-10-28T14:08:12.000Z","updated":"2024-12-06T13:33:41.945Z","comments":true,"path":"2024/10/28/post_acsii_graphics/","permalink":"https://ooge0.github.io/hexo-blog/2024/10/28/post_acsii_graphics/","excerpt":"","text":"ASCII art in one line: https://1lineart.kulaone.com/#/","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"acsii","slug":"acsii","permalink":"https://ooge0.github.io/hexo-blog/tags/acsii/"}]},{"title":"Evaluating AI Applications. Checklist","slug":"post_ai__evaluating_ai_applications","date":"2024-10-28T14:08:12.000Z","updated":"2024-12-04T09:44:19.862Z","comments":true,"path":"2024/10/28/post_ai__evaluating_ai_applications/","permalink":"https://ooge0.github.io/hexo-blog/2024/10/28/post_ai__evaluating_ai_applications/","excerpt":"","text":"1. Evaluation of AI Prompts1.1 Prompt Clarity Ensure the prompt is clear and unambiguous. Verify that the prompt avoids overly complex or technical language unless required by the application. Assess whether the intent of the prompt is evident to the AI. 1.2 Prompt Relevance Confirm that the prompt aligns with the expected use case or domain of the application. Check if the prompt sufficiently narrows down the expected response scope. Use semantic similarity tools like SentenceTransformers to verify relevance between the prompt and its context. Example:12345678from sentence_transformers import SentenceTransformer, utilmodel = SentenceTransformer(&#x27;all-MiniLM-L6-v2&#x27;)prompt = &quot;Explain the concept of gravity.&quot;context = &quot;A physics tutoring chatbot.&quot;similarity = util.pytorch_cos_sim(model.encode(prompt), model.encode(context))print(f&quot;Semantic Similarity Score: &#123;similarity&#125;&quot;) 1.3 Prompt Variability Evaluate how the AI performs across diverse phrasing of the same question or task. Test prompts with varied structures, synonyms, or tone adjustments. Test AI with variations of prompts using paraphrasing libraries like transformers. Example:12345678from transformers import pipelineparaphraser = pipeline(&quot;text2text-generation&quot;, model=&quot;t5-small&quot;)prompt = &quot;Describe the process of photosynthesis.&quot;variations = paraphraser(prompt, max_length=50, num_return_sequences=3)for v in variations: print(v[&#x27;generated_text&#x27;]) Reference: HuggingFace Transformers 1.4 Performance Across Scenarios Use edge-case prompts, including ambiguous, contradictory, or incomplete inputs. Analyze how the AI handles multilingual or culturally specific prompts.Approach: Develop a test suite with diverse prompts, including edge cases. Example edge case list: Ambiguous: “What is it?” Contradictory: “Why is a circle square?” Multilingual: “¿Cómo estás?” Automate testing with a loop: Example: 12345prompts = [&quot;What is it?&quot;, &quot;Why is a circle square?&quot;, &quot;¿Cómo estás?&quot;]for prompt in prompts: response = ai_model.generate_response(prompt) # Replace with actual AI call print(f&quot;Prompt: &#123;prompt&#125; | Response: &#123;response&#125;&quot;) 2. Linguistic AI Prompt QA2.1 Response Accuracy Assess if the AI’s responses are factually correct. Verify that responses align with the context and requirements of the prompt. Compare responses against a reference dataset using BLEU, ROUGE, or exact match metrics. Example: 123456from nltk.translate.bleu_score import sentence_bleureference = [&quot;Paris is the capital of France.&quot;]response = &quot;Paris is the capital of France.&quot;score = sentence_bleu([reference], response.split())print(f&quot;BLEU Score: &#123;score&#125;&quot;) Reference: NLTK Documentation 2.2 Response Completeness Ensure responses fully address all aspects of the prompt. Check for missing details or incomplete information in multi-part prompts. Use keyword extraction (spaCy, nltk) to verify if key elements are addressed in the response. Example: 12345import spacynlp = spacy.load(&quot;en_core_web_sm&quot;)response = nlp(&quot;Photosynthesis converts sunlight into energy.&quot;)keywords = [token.text for token in response if token.pos_ in [&quot;NOUN&quot;, &quot;VERB&quot;]]print(f&quot;Keywords: &#123;keywords&#125;&quot;) 2.3 Language Quality Verify that responses use proper grammar, spelling, and punctuation. Evaluate readability, ensuring the output is concise and coherent. Use libraries like LanguageTool for grammar and style checks. Example:123456import language_tool_pythontool = language_tool_python.LanguageTool(&#x27;en-US&#x27;)response = &quot;Photosyntesis is a natural proess.&quot;matches = tool.check(response)print(f&quot;Grammar Issues: &#123;[match.message for match in matches]&#125;&quot;) Reference: LanguageTool 2.4 Tone and Style Appropriateness Confirm that the tone matches the intended audience or use case. Check for professional, formal, or casual responses based on expectations. Use sentiment analysis and tone classification (VADER, transformers). Example: 12345from transformers import pipelinetone_analyzer = pipeline(&quot;sentiment-analysis&quot;)response = &quot;Thank you for your query. Let me help you.&quot;tone = tone_analyzer(response)print(f&quot;Tone Analysis: &#123;tone&#125;&quot;) 2.5 Bias and Ethical Considerations Evaluate responses for unintended biases, stereotypes, or offensive language. Confirm adherence to ethical guidelines and appropriate handling of sensitive topics. 3. General QA Considerations3.1 Response Consistency Test the AI’s ability to provide consistent answers to repeated prompts. Verify consistency across similar but differently worded prompts. Automate repeated prompt tests and compare responses using hashes for quick consistency checks. Example:1234567import hashlibprompt = &quot;Explain gravity.&quot;response1 = ai_model.generate_response(prompt)response2 = ai_model.generate_response(prompt)hash1 = hashlib.md5(response1.encode()).hexdigest()hash2 = hashlib.md5(response2.encode()).hexdigest()print(f&quot;Consistent: &#123;hash1 == hash2&#125;&quot;) 3.2 Error Handling Evaluate how the AI handles invalid or nonsensical inputs. Assess whether the AI provides useful clarifications or fallback responses. Test with invalid inputs and evaluate fallback mechanisms. Example invalid inputs: Empty: “” Nonsensical: “asdfghjkl”1234invalid_prompts = [&quot;&quot;, &quot;asdfghjkl&quot;]for prompt in invalid_prompts: response = ai_model.generate_response(prompt) print(f&quot;Prompt: &#123;prompt&#125; | Response: &#123;response&#125;&quot;) 3.3 Adaptability Test the AI’s ability to refine responses based on follow-up questions or clarifications. Check adaptability to user feedback or corrections during interactions. Use sequential prompts to test follow-up handling. Example:responses = [] prompts = [&quot;What is gravity?&quot;, &quot;Can you elaborate?&quot;] for prompt in prompts: response = ai_model.generate_response(prompt) responses.append(response) print(f&quot;Prompt: &#123;prompt&#125; | Response: &#123;response&#125;&quot;) 4. Documentation and Reporting Maintain detailed records of tests performed, including prompt variations and observed outcomes. Highlight areas for improvement, including specific examples of weak or incorrect responses. Provide suggestions for refining prompts or fine-tuning the AI model where needed. Store all test results in structured formats (e.g., CSV, JSON). Example:import csv results = [&#123;&quot;prompt&quot;: &quot;What is gravity?&quot;, &quot;response&quot;: &quot;A force of attraction.&quot;, &quot;accuracy&quot;: 1.0&#125;] with open(&#39;test_results.csv&#39;, &#39;w&#39;, newline=&#39;&#39;) as csvfile: fieldnames = results[0].keys() writer = csv.DictWriter(csvfile, fieldnames=fieldnames) writer.writeheader() writer.writerows(results) Note: This checklist should be adapted to the specific requirements of the AI application under evaluation.","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"ML","slug":"ML","permalink":"https://ooge0.github.io/hexo-blog/tags/ML/"},{"name":"prompt_engineering","slug":"prompt-engineering","permalink":"https://ooge0.github.io/hexo-blog/tags/prompt-engineering/"}]},{"title":"Prompt Engineering - start.","slug":"post_ai__promt_engineer_start","date":"2024-10-28T14:08:12.000Z","updated":"2024-11-30T15:14:10.400Z","comments":true,"path":"2024/10/28/post_ai__promt_engineer_start/","permalink":"https://ooge0.github.io/hexo-blog/2024/10/28/post_ai__promt_engineer_start/","excerpt":"","text":"Prompt engineering has emerged as a crucial role in the field of artificial intelligence (AI), focusing on crafting, optimizing, and evaluating queries to effectively interact with advanced models like GPT-4, Claude, or Bard. This guide outlines the technical skills, tools, and practices necessary for a successful career in this domain, with references to state-of-the-art resources and technologies. 1. Core Knowledge in Artificial Intelligence and Machine LearningUnderstanding generative AI and large language models (LLMs) is foundational to prompt engineering. Key Topics: Natural Language Processing (NLP): Learn about tokenization, embedding, and attention mechanisms. Familiar tools: spaCy (2020+), Transformers by Hugging Face. Language Models: Explore architectures like GPT (OpenAI), BERT, and T5. Key tools: OpenAI API, Google’s TensorFlow, and Hugging Face’s datasets. Fine-Tuning Techniques: Study transfer learning for customizing LLMs with domain-specific data. Libraries: LoRA (Low-Rank Adaptation for fine-tuning large models). Paper: LoRA: Low-Rank Adaptation of Large Language Models &#x2F; DOI arxiv.org&#x2F;abs&#x2F;2106.09685 References: Vaswani et al., 2017, “Attention is All You Need” - DOI: 10.48550&#x2F;arXiv.1706.03762. Efficient Fine-Tuning Techniques for LLMs (Hugging Face PEFT). 2. Data Handling SkillsPrompt engineering often involves working with large datasets to understand and refine AI responses. Tools and Techniques: Data Preprocessing: Techniques: Noise reduction, stemming, and lemmatization. Tools: Cleanlab, NLTK (for text-specific preprocessing). Data Analysis: Understand statistical trends in model responses. Tools: Polars, an alternative to Pandas for high-performance data handling. Tokenization Analysis: Evaluate token usage in models using OpenAI’s tiktoken library (GitHub). 3. Prompt Design and OptimizationCrafting effective prompts is at the heart of this role. Advanced Practices: Prompt Templates: Use structured prompts with placeholders for variables. Tools: LangChain for modular prompt management. Iterative Refinement: Use tools like PromptPerfect to analyze and enhance prompt effectiveness. Handling Context: Manage long contexts using techniques like windowing or chunking, supported by Pinecone for vector search. Other techniques and tools presented in other post: ‘Techniques for Handling Context in AI Models’ 4. Metrics for Evaluating Prompt EffectivenessEvaluating AI responses requires robust metrics and tools. Metrics: Relevance and Accuracy: Techniques: Embedding similarity scoring (e.g., cosine similarity). Tools: Sentence-Transformers. Quality and Fluency: Automated tools like TextBlob or Grammarly API. BLEU, ROUGE, METEOR: BLEU ROUGE METEOR Paper: METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments, 2005. Reaad on aclanthology.org Paper: The Meteor metric for automatic evaluation of machine translation, 2009 DOI:10.1007&#x2F;s10590-009-9059-4 Read on sci-hub Post: What is the METEOR Score (Metric for Evaluation of Translation with Explicit Ordering)? Reaad on klu.ai Compare responses to benchmarks using Evaluate. Evaluate - Installation Evaluate - A quick tour Evaluate - Choosing a metric for your task 5. Testing and Automation in Prompt EngineeringAutomation ensures scalability and reliability in prompt testing. Best Practices: Automated Test Suites: Frameworks: pytest, Great Expectations for data validation. Version Control for Prompts: Use GitHub or GitLab with prompt-specific repositories. Continuous Testing Pipelines: CI&#x2F;CD integration using GitHub Actions or CircleCI. 6. Tools for Working with Language ModelsAdvanced tools simplify interaction with modern AI systems. Recommendations: Model APIs: OpenAI GPT-4, Cohere API (Cohere Docs). Interactive Platforms: Notion AI, Google Colab. Visual Analytics: Dashboards using Streamlit or Plotly Dash. 7. Software Development SkillsIntegrating AI prompts into workflows requires coding expertise. Key Skills: Python for AI: Libraries: Hugging Face Transformers, OpenAI’s Python SDK. REST API Development: Tools: FastAPI, Postman for testing. Versioning: Git and platforms like DVC (Data Version Control) for managing prompt iterations. 8. Ethics and Limitations in AIResponsibility is a key aspect of prompt engineering. Topics: Ethical Prompt Design: Avoid introducing biases into generated outputs. Privacy and Security: Ensure that personal or sensitive data is anonymized. Tools: Presidio for PII detection. ReferencesTutorials and Resources: Umar Jamil: YouTube, Website. Videos: GitHub videos Yannic Kilcher: YouTube. Research Papers: “Attention is All You Need” - Vaswani et al., DOI: 10.48550&#x2F;arXiv.1706.03762. “Efficient Fine-Tuning Techniques for LLMs” - DOI: 10.48550&#x2F;arXiv.2106.09685. Tools and Libraries: OpenAI API (Docs). Hugging Face Transformers. LangChain (Website). Other posts Prompt Design and Engineering: Introduction and Advanced Methods. Read on arxiv.org ConclusionPrompt engineering is an evolving domain, combining technical expertise with creativity. With a solid foundation in AI, data analysis, and ethics, specialists can craft meaningful interactions between humans and machines. By leveraging modern tools and methodologies, professionals can drive innovation in the AI ecosystem.","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"science","slug":"science","permalink":"https://ooge0.github.io/hexo-blog/tags/science/"},{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"ML","slug":"ML","permalink":"https://ooge0.github.io/hexo-blog/tags/ML/"},{"name":"prompt_engineering","slug":"prompt-engineering","permalink":"https://ooge0.github.io/hexo-blog/tags/prompt-engineering/"}]},{"title":"Techniques for Handling Context in AI Models","slug":"post_ai_llm__techniques_for_handling_context_in_ai_models","date":"2024-10-28T14:08:12.000Z","updated":"2024-12-10T17:05:28.389Z","comments":true,"path":"/techniques-for-handling-context-in-ai-models/","permalink":"https://ooge0.github.io/hexo-blog/techniques-for-handling-context-in-ai-models/","excerpt":"","text":"Large Language Models (LLMs) often face challenges with long contexts due to token limitations or memory constraints. To address this, researchers and engineers have developed various techniques and tools to enhance context management. Below is a detailed list of techniques and associated tools. Techniques1. Windowing (Sliding Window Technique) Title: Training RNN and it’s Variants Using Sliding Window Technique. 2020 DOI: 10.1109&#x2F;SCEECS48394.2020.93 Read ‘Training RNN and it’s Variants Using Sliding Window Technique. 2020’ on sci-hub.se 2. Chunking and Overlapping Contexts Title: Hierarchical Attention Networks for Document Classification DOI: 10.18653&#x2F;v1&#x2F;N16-1174 Read ‘Hierarchical Attention Networks for Document Classification’ on aclanthology.org 3. Hierarchical Context Representations Title: Hierarchical Learning for Generation with Long Source Sequences. 2021 DOI: 10.48550&#x2F;arXiv.2104.07545 Read ‘Hierarchical Learning for Generation with Long Source Sequences. 2021’ on arxiv.org 4. Memory-Augmented Neural Networks (MANNs) Title: One-shot Learning with Memory-Augmented Neural Networks. 2016 DOI: 10.48550&#x2F;arXiv.1605.06065 Read ‘One-shot Learning with Memory-Augmented Neural Networks. 2016’ on arxiv.org 5. Transformer Variants for Long ContextsLongformer Title: Longformer: The Long-Document Transformer. 2020 DOI: 10.48550&#x2F;arXiv.2004.05150 Read ‘Longformer: The Long-Document Transformer. 2020’ on arxiv.org Reformer Title: Reformer: The Efficient Transformer. 2020 DOI: 10.48550&#x2F;arXiv.2001.04451 Read ‘Reformer: The Efficient Transformer. 2020’ on arxiv.org 6. Compression-Based Context Management (Summarization) Title: Efficient Adaptation of Pretrained Transformers for Abstractive Summarization. 2019 DOI: 10.48550&#x2F;arXiv.1906.00138 Read ‘Compression-Based Context Management (Summarization)’ on arxiv.org 7. Causal Attention Mechanisms Title: Attention Is All You Need. 2017 DOI: arXiv.1706.03762 Read ‘Attention Is All You Need. 2017’ on arxiv.org 8. Vector-Based Semantic Search (Dense Vector Representations) Title: Sentence-BERT: Sentence Embeddings Using Siamese Networks. 2019 DOI: 10.48550&#x2F;arXiv.1908.10084 Read Sentence-BERT: Sentence Embeddings Using Siamese Networks. 2019’ on arxiv.org 9. Retrieval-Augmented Generation (RAG) Title: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. 2020 DOI: 10.48550&#x2F;arXiv.2005.11401 Read ‘Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. 2020’ on arxiv.org 10. Cache Augmented Models for Recurrent Usage Title: RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation. 2024 DOI: 10.48550&#x2F;arXiv.2404.12457 Read ‘RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation. 2024’ on arxiv.org Tools and Frameworks1. Pinecone Provides vector-based memory and semantic search capabilities. 2. LangChain Handles chunking, memory management, and retrieval tasks for LLMs. 3. FAISS (Facebook AI Similarity Search) Optimized for efficient similarity search on dense vectors. 4. Weaviate Offers scalable vector search and knowledge graph integrations. 5. Hugging Face’s Transformers Implements state-of-the-art transformer models like Longformer and Reformer. 6. OpenAI Embeddings Provides embeddings for vector search and semantic tasks. 7. Redis Vector Store Lightweight memory storage for vectorized data. 8. GPT Index (LlamaIndex) Automates document splitting, chunking, and embedding management for LLMs. 9. Haystack An open-source framework for retrieval-augmented generation (RAG). 10. MemGPT Enhances memory for multi-session GPT-based interactions.","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"science","slug":"science","permalink":"https://ooge0.github.io/hexo-blog/tags/science/"},{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"ML","slug":"ML","permalink":"https://ooge0.github.io/hexo-blog/tags/ML/"},{"name":"prompt_engineering","slug":"prompt-engineering","permalink":"https://ooge0.github.io/hexo-blog/tags/prompt-engineering/"},{"name":"LLM","slug":"LLM","permalink":"https://ooge0.github.io/hexo-blog/tags/LLM/"}]},{"title":"Chatbots from scratch","slug":"post_chatbots_from_scratch","date":"2024-10-28T14:08:12.000Z","updated":"2024-11-25T19:58:50.539Z","comments":true,"path":"2024/10/28/post_chatbots_from_scratch/","permalink":"https://ooge0.github.io/hexo-blog/2024/10/28/post_chatbots_from_scratch/","excerpt":"","text":"How to build a Llama 2 chatbot","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"ML","slug":"ML","permalink":"https://ooge0.github.io/hexo-blog/tags/ML/"},{"name":"chatbot","slug":"chatbot","permalink":"https://ooge0.github.io/hexo-blog/tags/chatbot/"},{"name":"Llama","slug":"Llama","permalink":"https://ooge0.github.io/hexo-blog/tags/Llama/"}]},{"title":"Principles of categorizing","slug":"post_principles_of_categorizing","date":"2024-10-28T14:08:12.000Z","updated":"2024-12-04T16:48:35.322Z","comments":true,"path":"2024/10/28/post_principles_of_categorizing/","permalink":"https://ooge0.github.io/hexo-blog/2024/10/28/post_principles_of_categorizing/","excerpt":"","text":"This post contains information about fundamental books for classic principles of categorizing. Fundamental books that delve deeply into the principles of categorizing and organizing knowledge across different fields: Metaphysics by Aristotle Description: Aristotle’s work is foundational in Western philosophy, with early explorations into categorization and classification, particularly his concept of “Categories,” which influenced logical structures and classification in science, philosophy, and more. Book reference: Metaphysics by Aristotle Categories by Aristotle Description: In this essential work, Aristotle1 introduces a framework for understanding and categorizing different forms of knowledge, laying the groundwork for logical classification systems. Book reference: Categories by Aristotle Mathematical Principles of Naturalis Philosophy by Isaac Newton 2 Description: lthough primarily a scientific text, Newton’s “Principia” exemplifies the early scientific method’s approach to categorizing natural phenomena, setting the stage for systematic inquiry and structured scientific classification. Book reference: Mathematical Principles of Natural Philosophy by Isaac Newton Philosophiae Naturalis Principia Mathematica by Isaac Newton A System of Logic, Ratiocinative and Inductive by John Stuart Mill Description: Mill’s book is a landmark work on logic, covering classification, reasoning, and scientific methods for organizing knowledge and evidence, crucial for systematic thinking and empirical sciences. DOI: 10.1017&#x2F;CBO9781139149839 Read on gutenberg.org Read here A Guide to the Classification Theorem for Compact Surfaces by Jean Gallier Description: This work is significant in mathematics and topology, providing insight into classifying surfaces and structures, which has implications for mathematical taxonomy. On the Origin of Species by Charles Darwin Description: Darwin’s work revolutionized biological classification, showing how species evolve and adapt. His theories influenced the way we classify life forms, emphasizing evolutionary relationships as a basis for taxonomy. The Logic of Scientific Discovery by Karl Popper Description: Popper’s book discusses the scientific method and the role of classification in developing theories and hypotheses, providing a foundation for scientific knowledge organization. Systematics and the Origin of Species by Ernst Mayr Description: Mayr’s work is pivotal in evolutionary biology and taxonomy, exploring how species are classified based on genetics, morphology, and evolutionary history. Elements by Euclid Description: An essential work in mathematics, Euclid’s “Elements” systematically categorizes geometric principles, setting a precedent for organizing mathematical knowledge. The Structure of Scientific Revolutions by Thomas Kuhn Description: Kuhn’s book introduced the concept of paradigm shifts and discusses how scientific knowledge is organized, structured, and periodically reclassified in response to new discoveries. The Species Problem by David L. Hull Description: Hull examines the philosophical issues in defining and categorizing species, a classic text in understanding how biological classification raises complex questions about the nature of categories. Principia Ethica by G.E. Moore Description: Although a work in ethics, Moore’s analysis on categorizing moral concepts influenced the logical and systematic classification of ethical terms and principles. Classifications and Typologies in Qualitative Research by Ian Dey Description: This text delves into classification systems in qualitative research, explaining how data is categorized and analyzed in social sciences. Conceptual Spaces: The Geometry of Thought by Peter Gärdenfors Description: Gärdenfors explores the cognitive basis of categorization and how we form mental categories, linking psychology with systematic classification principles. Introduction to the Theory of Sets by Joseph Breuer Description: This work on set theory is fundamental to mathematics and logic, providing a structured framework for understanding categorization through the concept of sets, which underlies much of modern classification theory. For books that explore systematic knowledge categorization and classification, here are some highly regarded titles: Classification Theory: Foundations and Applications by William G. Wilson Description: This book provides a comprehensive overview of classification theories, exploring the foundations, methodologies, and applications across different fields. It discusses systematic approaches to categorization in science, information systems, and data organization. Syntactic Structures by Noam Chomsky Description: Although primarily focused on linguistics, Chomsky’s work introduced systematic methods for categorizing language structure, which has influenced knowledge classification in other fields, especially in computational linguistics and artificial intelligence. The Order of Things: An Archaeology of the Human Sciences by Michel Foucault Description: Foucault analyzes the historical evolution of classification systems in the sciences, examining how humans have historically categorized knowledge and how these structures shape our perception of the world. Classification, 2nd Edition by David B. Suthers Description: This book provides a deep dive into systematic classification for information science, covering frameworks like taxonomies, ontologies, and faceted classification. It’s a useful resource for understanding the foundations of knowledge organization. Library Classification and Cataloging by Lois Mai Chan Description: Focused on library science, this book explores classification systems like the Dewey Decimal and Library of Congress systems. It’s a practical guide to the systematic categorization and retrieval of knowledge. Knowledge and the Flow of Information by Fred Dretske Description: This philosophical work examines how knowledge is categorized, structured, and communicated. Dretske’s theories are foundational in information theory and knowledge management. DOI: 10.2307&#x2F;2214939 Read on sci-hub.se Information Architecture for the World Wide Web by Louis Rosenfeld and Peter Morville Description: A practical book for categorizing digital information. It discusses principles of taxonomy, metadata, and controlled vocabularies, which are essential for creating structured information on the web. Read on yunus.hacettepe.edu.tr The Library: A Fragile History by Andrew Pettegree and Arthur der Weduwen Description: This historical account discusses how libraries have categorized and preserved knowledge over centuries, exploring classification as a tool for knowledge organization and accessibility. General System Theory: Foundations, Development, Applications by Ludwig von Bertalanffy Description: Bertalanffy’s work is foundational in systems theory, which underpins classification across sciences. This book explores how systems thinking helps organize knowledge systematically. Ontology Learning and Population from Text: Algorithms, Evaluation and Applications by Philipp Cimiano Description: Focusing on ontology, this book explains methods for building structured, hierarchical classifications from text data, valuable for machine learning and data science. Books that explore systematic knowledge categorization and classification, here are some highly regarded titles: Classification Theory: Foundations and Applications by William G. Wilson Description: This book provides a comprehensive overview of classification theories, exploring the foundations, methodologies, and applications across different fields. It discusses systematic approaches to categorization in science, information systems, and data organization. Syntactic Structures by Noam Chomsky Description: Although primarily focused on linguistics, Chomsky’s work introduced systematic methods for categorizing language structure, which has influenced knowledge classification in other fields, especially in computational linguistics and artificial intelligence. The Order of Things: An Archaeology of the Human Sciences by Michel Foucault Description: Foucault analyzes the historical evolution of classification systems in the sciences, examining how humans have historically categorized knowledge and how these structures shape our perception of the world. Classification, 2nd Edition by David B. Suthers Description: This book provides a deep dive into systematic classification for information science, covering frameworks like taxonomies, ontologies, and faceted classification. It’s a useful resource for understanding the foundations of knowledge organization. Library Classification and Cataloging by Lois Mai Chan Description: Focused on library science, this book explores classification systems like the Dewey Decimal and Library of Congress systems. It’s a practical guide to the systematic categorization and retrieval of knowledge. Knowledge and the Flow of Information by Fred Dretske Description: This philosophical work examines how knowledge is categorized, structured, and communicated. Dretske’s theories are foundational in information theory and knowledge management. Information Architecture for the World Wide Web by Louis Rosenfeld and Peter Morville Description: A practical book for categorizing digital information. It discusses principles of taxonomy, metadata, and controlled vocabularies, which are essential for creating structured information on the web. The Library: A Fragile History by Andrew Pettegree and Arthur der Weduwen Description: This historical account discusses how libraries have categorized and preserved knowledge over centuries, exploring classification as a tool for knowledge organization and accessibility. General System Theory: Foundations, Development, Applications by Ludwig von Bertalanffy Description: Bertalanffy’s work is foundational in systems theory, which underpins classification across sciences. This book explores how systems thinking helps organize knowledge systematically. Ontology Learning and Population from Text: Algorithms, Evaluation and Applications by Philipp Cimiano Description: Focusing on ontology, this book explains methods for building structured, hierarchical classifications from text data, valuable for machine learning and data science. 1.Aristotle - Aristotle(Greek: Ἀριστοτέλης Aristotélēs; 384–322 BC) was an Ancient Greek philosopher and polymath. His writings cover a broad range of subjects spanning the natural sciences, philosophy, linguistics, economics, politics, psychology, and the arts. As the founder of the Peripatetic school of philosophy in the Lyceum in Athens, he began the wider Aristotelian tradition that followed, which set the groundwork for the development of modern science. wikipedia: Aristotle ↩2.Isaac Newton - Sir Isaac Newton FRS (25 December 1642 – 20 March 1726/27) was an English polymath active as a mathematician, physicist, astronomer, alchemist, theologian, and author who was described in his time as a natural philosopher.He was a key figure in the Scientific Revolution and the Enlightenment that followed. wikipedia: Isaac Newton ↩","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"science","slug":"science","permalink":"https://ooge0.github.io/hexo-blog/tags/science/"},{"name":"knowledge","slug":"knowledge","permalink":"https://ooge0.github.io/hexo-blog/tags/knowledge/"}]},{"title":"REsources","slug":"post_resources","date":"2024-10-28T14:08:12.000Z","updated":"2024-12-06T09:51:30.818Z","comments":true,"path":"2024/10/28/post_resources/","permalink":"https://ooge0.github.io/hexo-blog/2024/10/28/post_resources/","excerpt":"","text":"Here is a list of all the links from the blog, all in one place roundrobin.pub","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"acsii","slug":"acsii","permalink":"https://ooge0.github.io/hexo-blog/tags/acsii/"}]},{"title":"Resources. Global platforms and people","slug":"post_resources__global_platforms","date":"2024-10-28T14:08:12.000Z","updated":"2024-11-25T09:27:55.553Z","comments":true,"path":"2024/10/28/post_resources__global_platforms/","permalink":"https://ooge0.github.io/hexo-blog/2024/10/28/post_resources__global_platforms/","excerpt":"","text":"List of global platforms of resources Archive.org Home page: https://Archive.org/ About: Internet Archive is a non-profit library of millions of free texts, movies, software, music, websites, and more brownstone.org Home page: https://brownstone.org/ About: Brownstone Institute is a nonprofit 501(c)(3) organization founded May 2021. Its vision is of a society that places the highest value on the voluntary interaction of individuals and groups while minimizing the use of violence and force including that which is exercised by public or private authorities. This vision is that of the Enlightenment which elevated learning, science, progress, and universal rights to the forefront of public life. It is constantly threatened by ideologies and systems that would take the world back to before the triumph of the ideal of freedom. List of people Joe Rogan Home page: https://www.joerogan.com/ About: A standup comedian for over 30 years, Rogan’s seventh hour long comedy special Joe Rogan: Burn the Boats premiered live on Netflix on August 3, 2024. Rogan’s previous comedy specials include Joe Rogan: Strange Times (2018) and Joe Rogan: Triggered (2016) for Netflix, Joe Rogan: Rocky Mountain High (2014) for Comedy Central, Joe Rogan: Live from the Tabernacle (2012) released via his website, Talking Monkeys in Space (2009) on CD &amp; DVD, and Joe Rogan Live (2007) on DVD. Additionally, Rogan released the CDs Shiny Happy Jihad (2007) and I’m Gonna Be Dead Someday (2000).","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"global_knowledge","slug":"global-knowledge","permalink":"https://ooge0.github.io/hexo-blog/tags/global-knowledge/"},{"name":"resources","slug":"resources","permalink":"https://ooge0.github.io/hexo-blog/tags/resources/"}]},{"title":"My Statistics page","slug":"maintanance/my_statistics_page","date":"2024-10-22T19:49:00.762Z","updated":"2024-11-01T17:22:41.613Z","comments":true,"path":"/maintanance/my_statistics_page/","permalink":"https://ooge0.github.io/hexo-blog/maintanance/my_statistics_page/","excerpt":"","text":"Total views: last hour last week last month Longest session: last hour last week last month User locations:","categories":[{"name":"Maintanance","slug":"Maintanance","permalink":"https://ooge0.github.io/hexo-blog/categories/Maintanance/"}],"tags":[{"name":"statistics","slug":"statistics","permalink":"https://ooge0.github.io/hexo-blog/tags/statistics/"}]},{"title":"HEXIO.IO instalation guide","slug":"tutorials/HEXO.IO_installation_guide","date":"2024-10-22T19:49:00.747Z","updated":"2024-12-02T10:03:30.200Z","comments":true,"path":"2024/10/22/tutorials/HEXO.IO_installation_guide/","permalink":"https://ooge0.github.io/hexo-blog/2024/10/22/tutorials/HEXO.IO_installation_guide/","excerpt":"","text":"Make HEXO.IO blog https://hexo.io/ -A fast, simple &amp; powerful blog framework Install HexoTo get started with Hexo, ensure you have Node.js and Git installed on your machine. Then follow these steps: Install Hexo CLI1npm install -g hexo-cli Create a New Hexo Project1hexo init blog cd my-blog npm install Start the Local Server1hexo server Visit http://localhost:4000 to see your Hexo blog locally. 2. Publish the Blog on GitHub PagesSet Up a GitHub Repository Create a new repository on GitHub named yourusername.github.io. Clone the repository locally: 1git clone https://github.com/yourusername/yourusername.github.io Configure Deployment in Hexo Open your Hexo project. Install the deployment tool: 1npm install hexo-deployer-git --save Open the _config.yml file in the root of your Hexo project. Add the following lines for deployment configuration: 1234deploy: type: git repo: https://github.com/yourusername/yourusername.github.io.git branch: main # Use &#x27;master&#x27; for older repositories Generate Static Files and Deploy12hexo generate hexo deploy Your blog will now be live at https://yourusername.github.io. 3. Customizing Hexo LayoutHexo uses themes to define the layout. You can use an existing theme or customize your own. Change or Install a Theme Browse Hexo Themes. Install the theme: 1git clone https://github.com/theme-name/theme.git themes/theme-name` Update the _config.yml file to use the new theme: 1theme: theme-name Customize the LayoutHexo themes are built using EJS, Swig, or Pug templating engines. You can modify these template files in the theme’s layout folder to adjust the HTML structure, styles, and other customizations. If you want to add a left side panel with a menu, you’ll need to modify the layout/_partial/sidebar.ejs (or similar) file in the theme folder. 4. Add a Left Side Panel and MenuCreate the Left Sidebar Edit the theme’s layout file that contains the sidebar (e.g., layout/_partial/sidebar.ejs). Add the following menu HTML structure:123456789&lt;div class=&quot;sidebar&quot;&gt; &lt;ul class=&quot;menu&quot;&gt; &lt;li&gt;&lt;a href=&quot;/posts&quot;&gt;My Posts&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/notes&quot;&gt;My Notes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/stats&quot;&gt;My Stats&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/tags&quot;&gt;Tags&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/div&gt; Customize Styles for the SidebarIn your theme’s source/css/style.css (or similar stylesheet), add custom CSS to style the sidebar: 123456789101112131415161718192021.sidebar &#123; position: fixed; width: 250px; height: 100%; background-color: #f4f4f4; overflow-y: auto;&#125;.menu &#123; list-style-type: none; padding: 0;&#125;.menu li &#123; margin: 15px 0;&#125;.menu li a &#123; text-decoration: none; color: #333;&#125; 5. Add Pages for Menu ItemsCreate Pages for Custom SectionsFor each menu item (e.g., “My Posts”, “My Notes”), create corresponding pages: 1hexo new page &quot;posts&quot; hexo new page &quot;notes&quot; hexo new page &quot;stats&quot; hexo new page &quot;tags&quot; Each of these commands will create a new .md file in the source folder. You can edit them to include content specific to that section. Add Tag Listing PageHexo automatically supports tag pages, but you can enhance it. Ensure you have a tags page by running: 1hexo new page &quot;tags&quot; In source/tags/index.md, add: 1234---title: Tagslayout: tags--- 6. Main Panel with Scroll BarTo ensure that your posts have a scrollable main content area, adjust the CSS for the main content area in your theme’s stylesheet (usually source/css/style.css). 1234.main-panel &#123; margin-left: 260px; /* Account for sidebar */ overflow-y: scroll;&#125; Ensure that your theme’s HTML structure assigns the main-panel class to the main content area. 7. Main Configuration Options in HexoIn Hexo’s _config.yml file, you can customize many aspects of your site. Here are some of the most common settings: 123456789101112131415161718192021222324# Sitetitle: My Blogsubtitle: My Blog Subtitledescription: A blog about my personal projects.author: Your Namelanguage: en# URL Configurationurl: https://yourusername.github.ioroot: /permalink: :year/:month/:day/:title/# Deploymentdeploy: type: git repo: https://github.com/yourusername/yourusername.github.io.git branch: main# Pluginsplugins: - hexo-generator-tag - hexo-generator-archive - hexo-deployer-git 8. Table of Hexo Features Feature Description Static Site Generator Generates static HTML files for fast performance. Markdown Support Write posts in Markdown for easy formatting. Themes &amp; Customization Use pre-built themes or create your own. Plugins Extend functionality with various plugins (e.g., tags, archives). Tag System Organize posts with a powerful tag system. SEO Friendly Built-in SEO optimization features. Deployment One-click deployment to GitHub, GitLab, or custom servers. Drafts Manage drafts and publish them later. Multi-language Support Write blog in multiple languages. Extensible with Plugins Add analytics, social media sharing, and more via plugins. import existing Markdown (.md) and HTML files into HexoHEXO.IO commandshexo clean + generate + hexo 1hexo clean; hexo generate; hexo serve TOChttps://github.com/bubkoo/hexo-tochttps://github.com/bennycode/hexo-insert-toc HEXO + Markdown hexo-renderer-markdown-it This renderer plugin uses Markdown-it as a render engine on Hexo. Adds support for Markdown and CommonMark. https://github.com/hexojs/hexo-renderer-markdown-it Other resources xlog.wangyunzi.com | My Blog Building Journey - Hexo","categories":[{"name":"Tutorials","slug":"Tutorials","permalink":"https://ooge0.github.io/hexo-blog/categories/Tutorials/"}],"tags":[{"name":"tutorial","slug":"tutorial","permalink":"https://ooge0.github.io/hexo-blog/tags/tutorial/"},{"name":"blog","slug":"blog","permalink":"https://ooge0.github.io/hexo-blog/tags/blog/"},{"name":"hexo_io","slug":"hexo-io","permalink":"https://ooge0.github.io/hexo-blog/tags/hexo-io/"}]},{"title":"Cheatsheet list","slug":"cheatsheets/cheatsheet_list","date":"2024-10-22T19:49:00.743Z","updated":"2024-10-28T18:09:58.488Z","comments":true,"path":"2024/10/22/cheatsheets/cheatsheet_list/","permalink":"https://ooge0.github.io/hexo-blog/2024/10/22/cheatsheets/cheatsheet_list/","excerpt":"","text":"Markdown Cheat Sheet : https://www.markdownguide.org/cheat-sheet/ A suite a helpful free tools for working with Markdown.MarkdownTools is open source: https://blog.markdowntools.com/ Master Markdown Tables: A Complete Guide &amp; Tips: https://blog.markdowntools.com/posts/markdown-table-ultimate-guide","categories":[],"tags":[{"name":"tutorial","slug":"tutorial","permalink":"https://ooge0.github.io/hexo-blog/tags/tutorial/"},{"name":"cheatsheets","slug":"cheatsheets","permalink":"https://ooge0.github.io/hexo-blog/tags/cheatsheets/"}]},{"title":"stats","slug":"maintanance/blog_statistics","date":"2024-10-21T19:47:59.000Z","updated":"2024-10-22T12:16:01.633Z","comments":true,"path":"2024/10/21/maintanance/blog_statistics/","permalink":"https://ooge0.github.io/hexo-blog/2024/10/21/maintanance/blog_statistics/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Blog's categories","slug":"categories/index","date":"2024-10-21T19:47:57.000Z","updated":"2024-11-01T09:25:47.046Z","comments":true,"path":"/categories/","permalink":"https://ooge0.github.io/hexo-blog/categories/","excerpt":"","text":"Here you can find most valuable categories of my blog My contribution Posts Tutorials Notes Maintanance Personal information","categories":[{"name":"Categories","slug":"Categories","permalink":"https://ooge0.github.io/hexo-blog/categories/Categories/"}],"tags":[]},{"title":"Tags","slug":"maintanance/my_tags","date":"2024-10-21T19:47:39.000Z","updated":"2024-11-01T09:27:24.245Z","comments":true,"path":"2024/10/21/maintanance/my_tags/","permalink":"https://ooge0.github.io/hexo-blog/2024/10/21/maintanance/my_tags/","excerpt":"","text":"This page will show the cloud of tags…. sometimes later","categories":[{"name":"Maintanance","slug":"Maintanance","permalink":"https://ooge0.github.io/hexo-blog/categories/Maintanance/"}],"tags":[{"name":"tags","slug":"tags","permalink":"https://ooge0.github.io/hexo-blog/tags/tags/"}]},{"title":"Software QA - DB testing approach","slug":"post_qa__software_qa_db_testing_approach","date":"2024-10-20T21:00:00.000Z","updated":"2024-11-21T22:29:19.511Z","comments":true,"path":"2024/10/21/post_qa__software_qa_db_testing_approach/","permalink":"https://ooge0.github.io/hexo-blog/2024/10/21/post_qa__software_qa_db_testing_approach/","excerpt":"","text":"Table of Content INTRO1. Data Setup for Tests2. Data Verification3. Testing Stored Procedures4. Data Cleanup After Tests5. Automated Testing and Validation INTROInteracting with complex SQL scripts and stored procedures can be an essential part of the testing process. Below, I have outlined common scenarios along with examples of SQL scripts and stored procedures that could be useful in a QA context. These examples cover areas such as data verification, data setup for tests, and automated testing of stored procedures. 1. Data Setup for TestsExample SQL Script to Prepare Test Data: This script creates sample data that can be used for testing purposes. 1234567891011121314151617181920212223242526272829303132333435-- Create a test databaseCREATE DATABASE TestDB;-- Use the test databaseUSE TestDB;-- Create Example TablesCREATE TABLE Users ( UserId INT PRIMARY KEY IDENTITY, UserName NVARCHAR(50), Email NVARCHAR(100), CreatedAt DATETIME DEFAULT GETDATE());CREATE TABLE Orders ( OrderId INT PRIMARY KEY IDENTITY, UserId INT, OrderDate DATETIME DEFAULT GETDATE(), Amount DECIMAL(10, 2), FOREIGN KEY (UserId) REFERENCES Users(UserId));-- Insert Test Data into UsersINSERT INTO Users (UserName, Email)VALUES(&#x27;John Doe&#x27;, &#x27;john@example.com&#x27;),(&#x27;Jane Smith&#x27;, &#x27;jane@example.com&#x27;),(&#x27;Alice Johnson&#x27;, &#x27;alice@example.com&#x27;);-- Insert Test Data into OrdersINSERT INTO Orders (UserId, OrderDate, Amount)VALUES(1, &#x27;2023-01-01&#x27;, 150.00),(1, &#x27;2023-02-05&#x27;, 200.00),(2, &#x27;2023-03-10&#x27;, 99.99); QA Approach: Use this script to create a clean database environment before running tests. Ensure that the right amount of test data is present for testing various scenarios. 2. Data VerificationExample SQL Script to Verify Data: This script checks if the data setup is correct. 12345678910111213-- Check the count of users in Users tableSELECT COUNT(*) AS UserCount FROM Users;-- Check total order amount for each userSELECT U.UserName, SUM(O.Amount) AS TotalAmountFROM Users UJOIN Orders O ON U.UserId = O.UserIdGROUP BY U.UserName; QA Approach: Write scripts to verify that the setup is complete and correct before running tests. Use assertions in automated tests to match expected results with SQL query outputs. 3. Testing Stored ProceduresExample Stored Procedure: This stored procedure calculates total orders for a user. 12345678910111213CREATE PROCEDURE GetUserTotalOrders @UserId INTASBEGIN SET NOCOUNT ON; SELECT SUM(Amount) AS TotalAmount FROM Orders WHERE UserId = @UserId;END; Testing the Stored Procedure: You can write a script to test the stored procedure and verify its results. 12345678-- Testing the stored procedureDECLARE @TotalAmount DECIMAL(10, 2);EXEC @TotalAmount = GetUserTotalOrders @UserId = 1;PRINT &#x27;Total Amount for User 1: &#x27; + CAST(@TotalAmount AS NVARCHAR(10));-- Expected output for verification in QA Testing:-- Should match the expected total amount. QA Approach: Create unit tests for stored procedures to ensure they work as expected. Check edge cases such as invalid user IDs, no orders, and performance. 4. Data Cleanup After TestsExample SQL Script to Clean Up Test Data: 123456-- Drop Test DataDROP TABLE IF EXISTS Orders;DROP TABLE IF EXISTS Users;-- Optionally drop the TestDBDROP DATABASE IF EXISTS TestDB; QA Approach: Always clean up test databases or data after tests to avoid state dependency in future tests. Automate cleanup in the CI&#x2F;CD pipeline. 5. Automated Testing and ValidationExample of Testing with SQL Queries in Automated Tests: If using a test framework like Python’s pytest or a Java-based framework, you can include SQL verification steps post-execution of application code. Here’s a pseudo-code example for a test that might be written in Python: 1234567891011121314151617181920import pyodbcdef test_user_order_total(): # Execute the application code that triggers order creation create_order(user_id=1, amount=200.00) # Connect to the Database conn = pyodbc.connect(&#x27;Driver=&#123;SQL Server&#125;;&#x27; &#x27;Server=your_server;&#x27; &#x27;Database=TestDB;&#x27; &#x27;UID=user;&#x27; &#x27;PWD=password;&#x27;) cursor = conn.cursor() # Execute verification SQL cursor.execute(&quot;EXEC GetUserTotalOrders @UserId = 1&quot;) result = cursor.fetchone() # Assert the expected value","categories":[{"name":"QA","slug":"QA","permalink":"https://ooge0.github.io/hexo-blog/categories/QA/"}],"tags":[{"name":"qa","slug":"qa","permalink":"https://ooge0.github.io/hexo-blog/tags/qa/"},{"name":"db","slug":"db","permalink":"https://ooge0.github.io/hexo-blog/tags/db/"}]},{"title":"HEXO.IO blog references","slug":"notes/HEXO_IO_blog _references","date":"2024-10-20T21:00:00.000Z","updated":"2024-10-31T08:38:51.233Z","comments":true,"path":"2024/10/21/notes/HEXO_IO_blog _references/","permalink":"https://ooge0.github.io/hexo-blog/2024/10/21/notes/HEXO_IO_blog%20_references/","excerpt":"","text":"Here is collection of HEXO.IO blog and short comments on how to reuse some idea and funcitonality. List of referenceskiko.io kiko.io link: https://kiko.io/ Descrtiption: User friendly blog with extended functionality and components","categories":[{"name":"Notes","slug":"Notes","permalink":"https://ooge0.github.io/hexo-blog/categories/Notes/"}],"tags":[{"name":"tutorial","slug":"tutorial","permalink":"https://ooge0.github.io/hexo-blog/tags/tutorial/"},{"name":"hexo_io","slug":"hexo-io","permalink":"https://ooge0.github.io/hexo-blog/tags/hexo-io/"}]},{"title":"Add more beautiful HEXO blog pages","slug":"notes/HEXO_IO_make_md_more_beautiful","date":"2024-10-20T21:00:00.000Z","updated":"2024-11-01T17:50:30.885Z","comments":true,"path":"2024/10/21/notes/HEXO_IO_make_md_more_beautiful/","permalink":"https://ooge0.github.io/hexo-blog/2024/10/21/notes/HEXO_IO_make_md_more_beautiful/","excerpt":"","text":"Here are some HTML and CSS code snippets that you can use in Markdown to style and enhance text areas, add background colors, or create visually distinct sections without JavaScript or complex manipulations. 1. Text Block with Background Color and Padding Use this snippet to create a colored background for a paragraph or block of text. 123&lt;div style=&quot;background-color: #f0f8ff; padding: 15px; border-radius: 5px;&quot;&gt; &lt;p&gt;This is a block of text with a light blue background color, padding, and rounded corners. You can style each paragraph separately by adding new `&lt;p&gt;` tags within this div.&lt;/p&gt;&lt;/div&gt; result below This is a block of text with a light blue background color, padding, and rounded corners. You can style each paragraph separately by adding new `` tags within this div. 2. Highlighted Section with Border and Background Create a visually distinct area with a subtle border, background, and some padding. 1234&lt;div style=&quot;background-color: #dff0d8; border: 1px solid #d6e9c6; padding: 20px; border-radius: 8px;&quot;&gt;&lt;h3 style=&quot;color: #3c763d;&quot;&gt;Important Notice&lt;/h3&gt;&lt;p&gt;This section is highlighted with a green background, a border, and some padding to make it stand out.&lt;/p&gt;&lt;/div&gt; result below Important Notice This section is highlighted with a green background, a border, and some padding to make it stand out. 3. Colored Quote Block Emphasize quotes or important messages with a block style and colored left borde 123 &lt;blockquote style=&quot;border-left: 4px solid #8e44ad; background-color: #f7f1ff; padding: 15px; margin: 10px 0;&quot;&gt; &lt;p style=&quot;color: #4a235a;&quot;&gt;&quot;Here is a highlighted quote or important message with a unique style.&quot;&lt;/p&gt;&lt;/blockquote&gt; result below \"Here is a highlighted quote or important message with a unique style.\" 4. Centered Text with Decorative Style Center text with a larger font size, bold style, and color. 123&lt;div style=&quot;text-align: center; font-size: 1.5em; font-weight: bold; color: #2980b9;&quot;&gt;&lt;p&gt;This is centered, larger, and bold text with a custom color for emphasis.&lt;/p&gt;&lt;/div&gt; result below This is centered, larger, and bold text with a custom color for emphasis. 5. Callout Box with Shadow and Gradient Background Create a callout box with a gradient background and shadow to make it “pop.” 1234&lt;div style=&quot;background: linear-gradient(to right, #ffecd2, #fcb69f); padding: 15px; border-radius: 10px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);&quot;&gt;&lt;h3 style=&quot;color: #b03a2e;&quot;&gt;Tip of the Day&lt;/h3&gt;&lt;p&gt;Use gradients and shadows to create eye-catching highlights in your content!&lt;/p&gt;&lt;/div&gt; result below Tip of the Day Use gradients and shadows to create eye-catching highlights in your content! 6. Badge or Label for Text Use a styled span element to create a badge that you can place inline with text. 1&lt;p&gt;Get our latest updates: &lt;span style=&quot;background-color: #3498db; color: white; padding: 3px 10px; border-radius: 12px; font-size: 0.9em;&quot;&gt;New&lt;/span&gt;&lt;/p&gt; result below Get our latest updates: New 7. Box with Border and Light Shadow for Emphasis Create a simple box with a light border and shadow effect. 123 &lt;div style=&quot;border: 1px solid #ccc; padding: 20px; border-radius: 5px; box-shadow: 2px 2px 5px rgba(0,0,0,0.1);&quot;&gt; &lt;p&gt;This is a separated section with a border and light shadow effect for emphasis.&lt;/p&gt;&lt;/div&gt; result below This is a separated section with a border and light shadow effect for emphasis.","categories":[{"name":"Notes","slug":"Notes","permalink":"https://ooge0.github.io/hexo-blog/categories/Notes/"}],"tags":[{"name":"blog","slug":"blog","permalink":"https://ooge0.github.io/hexo-blog/tags/blog/"},{"name":"hexo_io","slug":"hexo-io","permalink":"https://ooge0.github.io/hexo-blog/tags/hexo-io/"},{"name":"note","slug":"note","permalink":"https://ooge0.github.io/hexo-blog/tags/note/"},{"name":"md_format","slug":"md-format","permalink":"https://ooge0.github.io/hexo-blog/tags/md-format/"}]},{"title":"HEXO blog design","slug":"notes/HEXO_IO_blog_design","date":"2024-10-20T21:00:00.000Z","updated":"2024-11-01T09:40:16.713Z","comments":true,"path":"2024/10/21/notes/HEXO_IO_blog_design/","permalink":"https://ooge0.github.io/hexo-blog/2024/10/21/notes/HEXO_IO_blog_design/","excerpt":"","text":"My plan to build&#x2F;redesign HEXO blog site based on landsacpe theme Header Navigation menu: Menu items: Home. Added Notes. Added Archives. Added Categories. Added About me. Added Main body Change dafault pagination &#x3D; 12 Added footnotes. See hexo-layout-improvements Side panel Added. Nested subcategories Categories Tags Tag Cloud Archives Recent PostsFooter Performance metter. Something like Page loaded in 0.11 at 1912 x 959. Can be used META(javascript:dialog.pageMeta();) My First Note","categories":[{"name":"Notes","slug":"Notes","permalink":"https://ooge0.github.io/hexo-blog/categories/Notes/"}],"tags":[{"name":"blog","slug":"blog","permalink":"https://ooge0.github.io/hexo-blog/tags/blog/"},{"name":"hexo_io","slug":"hexo-io","permalink":"https://ooge0.github.io/hexo-blog/tags/hexo-io/"},{"name":"note","slug":"note","permalink":"https://ooge0.github.io/hexo-blog/tags/note/"},{"name":"md_format","slug":"md-format","permalink":"https://ooge0.github.io/hexo-blog/tags/md-format/"}]},{"title":"Moving from old blog site","slug":"notes/moving_from_old_blog_site","date":"2024-10-20T21:00:00.000Z","updated":"2024-11-01T09:40:19.974Z","comments":true,"path":"2024/10/21/notes/moving_from_old_blog_site/","permalink":"https://ooge0.github.io/hexo-blog/2024/10/21/notes/moving_from_old_blog_site/","excerpt":"","text":"Tasks Update My GitHub projects post. Update links, descriptions, etc. Update HEXO.IO design for lanscape theme Improve header section Improve footer section","categories":[{"name":"Notes","slug":"Notes","permalink":"https://ooge0.github.io/hexo-blog/categories/Notes/"}],"tags":[{"name":"blog","slug":"blog","permalink":"https://ooge0.github.io/hexo-blog/tags/blog/"},{"name":"note","slug":"note","permalink":"https://ooge0.github.io/hexo-blog/tags/note/"},{"name":"md_format","slug":"md-format","permalink":"https://ooge0.github.io/hexo-blog/tags/md-format/"}]},{"title":"Favorite post categories","slug":"my_contribution/my_collected_posts","date":"2024-10-20T21:00:00.000Z","updated":"2024-10-31T09:48:10.941Z","comments":true,"path":"2024/10/21/my_contribution/my_collected_posts/","permalink":"https://ooge0.github.io/hexo-blog/2024/10/21/my_contribution/my_collected_posts/","excerpt":"","text":"Table of Contents Post Details LinkedIn Medium Other Resources Post DetailsLinkedIn How to identify and authorize the world | My LinkedIn post | Test design techniques (Diagram) | My LinkedIn post | Google Chrome Extensions for Software Testers | LinkedIn post | Machine Learning Community (Moderated) | LinkedIn post | List of channels about historical processes on YouTube (will be updated) | My LinkedIn post | DOM data | My LinkedIn post | One-time email services | My LinkedIn post | Worldwide virtual ward companies | My LinkedIn post | Export extensions from Chrome | My LinkedIn post | Medium Internet Control Message Protocol (ICMP) — overview | My Medium post | Apr, 2024 | Medium post | Cyclomatic complexity (Basics) | Medium post | Navigating the Gaming Cosmos: A Comprehensive Guide to Testing Game Tiers | My Medium post | Hugging Face Transformer UML diagram | My Medium post | How Is ChatGPT’s Behavior Changing | Medium post | Other Resources Open RAN explained | Nokia git-flow cheatsheet Penetration Testing - Complete Guide with Penetration Testing Sample Test Cases Linux Directories Explained in 100 Seconds - YouTube A ROUTE TO NET ZERO EUROPEAN AVIATION TUTORIAL. An Introduction to OAuth 2 OAuth2 101: OAuth 2 Detailed Grant Flow Diagrams, Security Consideration and Best Practice Nonfunctional Requirements - Scaled Agile Framework List of User Agent strings - DeviceAtlas Overview | Lighthouse | Chrome for Developers Education Services - Palo Alto Networks Continuous Integration With Python: An Introduction - Real Python Testing Strategies in a Microservice Architecture Flags are not languages - A blog about designing global user experiences: beyond language, location &amp; culture. Sigma.js Bobby Iliev - Introduction to SQL eBook SQL&#x2F;Tutorials&#x2F; SQLite tutorial.md at master · ooge0&#x2F;SQL Challenge Response Authentication Mechanism (CRAM) - Scaler Topics UML Diagram Types | Learn About All 14 Types of UML Diagrams Choosing the Right UML Diagram: State Diagrams, Sequence Diagrams, or Activity Diagrams? - Visual Paradigm Guides Export extensions from Chrome | LinkedIn Top 10 software development models in a nutshell Git Pull Force – How to Overwrite Local Changes With Git Author: Piotr Gaczkowski","categories":[{"name":"My contribution","slug":"My-contribution","permalink":"https://ooge0.github.io/hexo-blog/categories/My-contribution/"}],"tags":[{"name":"my_contribution","slug":"my-contribution","permalink":"https://ooge0.github.io/hexo-blog/tags/my-contribution/"},{"name":"my_medium_post","slug":"my-medium-post","permalink":"https://ooge0.github.io/hexo-blog/tags/my-medium-post/"},{"name":"my_linkedin_post","slug":"my-linkedin-post","permalink":"https://ooge0.github.io/hexo-blog/tags/my-linkedin-post/"}]},{"title":"Examples of test tasks","slug":"my_contribution/my_examples_of_test_tasks","date":"2024-10-20T21:00:00.000Z","updated":"2024-11-08T10:06:07.010Z","comments":true,"path":"/about-me/my_contribution/my_examples_of_test_tasks/","permalink":"https://ooge0.github.io/hexo-blog/about-me/my_contribution/my_examples_of_test_tasks/","excerpt":"","text":"Here are several examples of home tasks I’ve completed. This list includes several documents that I find particularly interesting. Content of attached documents are different as a time range that was required for compliting each task.Each document contains thoughts, arguments, some questions to the task writer, clarifications to my steps and actions. TECHNICAL TEST -1 (software QA&#x2F;QC)Task descriptionThis task involves testing a function that verifies whether two input characters are valid letters or numbers, returning 1 if true and 0 otherwise. A previous bug caused Cyrillic letters to be misclassified; this has been fixed, and regression tests have beenconducted to confirm the correction.Download the document here. TECHNICAL TEST -2 (software QA&#x2F;QC)Task descriptionThis task involves creating manual tests to validate the functionality of a currency converter tool, ensuring it meets core requirements for accurate conversions, displays essential details, and provides accessible API endpoints.Download the document here. TECHNICAL TEST -3 (software QA&#x2F;QC)Task descriptionThis task involves analyzing and identifying all potential scenarios for the Sign Up and Log In processes on XYZ.com. The goal is to ensure these flows function correctly, handle errors effectively, and provide a seamless user experience and recommending appropriate testing types to ensure robust validation of these processes.Download the document here. TECHNICAL TEST -4 (software QA&#x2F;QC)Task descriptionThis task involves analyzing web platform, invstigating test artifacts and reporting discovered issues, creating test cases.Download the document here. TECHNICAL TEST -5 (software QA&#x2F;QC)Task descriptionBasic audit of payment web platformDownload the document here. Total views:","categories":[{"name":"My contribution","slug":"My-contribution","permalink":"https://ooge0.github.io/hexo-blog/categories/My-contribution/"}],"tags":[{"name":"my_contribution","slug":"my-contribution","permalink":"https://ooge0.github.io/hexo-blog/tags/my-contribution/"},{"name":"test_tasks","slug":"test-tasks","permalink":"https://ooge0.github.io/hexo-blog/tags/test-tasks/"}]},{"title":"My GitHub projects","slug":"my_contribution/my_github_projects","date":"2024-10-20T21:00:00.000Z","updated":"2024-12-04T08:03:47.440Z","comments":true,"path":"2024/10/21/my_contribution/my_github_projects/","permalink":"https://ooge0.github.io/hexo-blog/2024/10/21/my_contribution/my_github_projects/","excerpt":"","text":"IMPORTANT !!!All information you find here is published “as is”.You can copy, clone and reuse the information without any restrictions from my side.Otherwise, please refer to official documentation or original sources. Below is a list of my My GitHub projects with short decription. Table of GitHub projectsMy HEXO.IO blogooge0.github.iopython_TA_web_api_frameworkAPI tests for petstore.swagger.ioTest automation framework based on RestAssured test framework.kafka-app-demoAppium FrameworkExtendedAppium framework (pet project)ssLv_Gherkin_JAVA My HEXO.IO blog Blog URL: Blog: ooge0.github.io&#x2F;hexo-blog&#x2F; GitHub link: ooge0.github.io Languages: HTML, JavaScript, CSS Description: Repository for the current website, including all pages, scripts, and CSS files. This project is my second blog project that I created based on HEXO.IO framework. Blog published and hosted on GitHub. It was created from scratch using basic HEXO.IO options and then configured using available internet resources. ooge0.github.io Blog URL: Blog: ooge0.github.io GitHub link: ooge0.github.io Languages: HTML, JavaScript, CSS Description: This project is my first blog project that I published and hosted on GitHub. It was created from scratch using pure JS, CSS codding and examples of code snippets available in the internet. python_TA_web_api_framework GitHub link: python_TA_web_api_framework Languages: Python, Gherkin Description: Hybrid test automation framework for UI and API testing. Created tests processed by PyTest library. Project contains mixed approach of: reused resources; generating test data using custom methods; using pytest and custom defined fixtures; using DB, config files, Excel docs for storing test data and references; generating reports (Allure, Pytest). API tests for petstore.swagger.ioGitHub link: demo_petLanguages: Python, GherkinDescription: Back-end tests framework for API testing. Destination source is https://petstore.swagger.io/. Test automation framework based on RestAssured test framework.GitHub link: restAssuredLanguages: JavaDescription: Helper for managing webDrivers. Forked from . I used this repo as a source of ideas. kafka-app-demoGitHub link: kafka-app-demoLanguages: PythonDescription: Pet project for investigating Kafka features. Created simple app for managing behavior of consumer&#x2F;poducer actions. Appium FrameworkExtendedGitHub link: AppiumFrameworkExtendedLanguages: JavaDescription: Pet project for investigating Appium features. More complex and detailed than appiumQA project. Project has detailed README.MD file, valid configuration for executing different tests, generating reports. Appium framework (pet project)GitHub link: appiumQALanguages: Java, BatchfileDescription: Pet project for investigating Appium features. There is no complex logic, just few classes for checking main Appium flow for interacting with Android app. ssLv_Gherkin_JAVAHelper for managing webDriversGitHub link: ssLv_Gherkin_JAVALanguages: Java, GherkinDescription: Project contains finished test scenario based on Gherkin language for checking web sites functionality. All scenarios are expected to use Page Object model. All scenarios are expected to use separated step definition files. Total views:","categories":[{"name":"My contribution","slug":"My-contribution","permalink":"https://ooge0.github.io/hexo-blog/categories/My-contribution/"}],"tags":[{"name":"my_contribution","slug":"my-contribution","permalink":"https://ooge0.github.io/hexo-blog/tags/my-contribution/"},{"name":"my_porjects","slug":"my-porjects","permalink":"https://ooge0.github.io/hexo-blog/tags/my-porjects/"}]},{"title":"My tools","slug":"my_contribution/my_tools","date":"2024-10-20T21:00:00.000Z","updated":"2024-11-21T22:13:01.444Z","comments":true,"path":"/about-me/my_contribution/my-tools/","permalink":"https://ooge0.github.io/hexo-blog/about-me/my_contribution/my-tools/","excerpt":"","text":"Daily appsMessangers Slack Zoom https://zoom.us/download API tools Postman Insomnia DB manager SSMS Dbviewer Proxy tools Fiddler Charles Local proxy Message broker Kafka Project management tools Jira TestRail Cloud virtual machines Azure Virtual machines list Amazon Virtual machines Launch a Windows Virtual Machine in Amazon Lightsail Amazon EC2 instances Virtual Box Virtual Box &gt;&gt; how to install the virtual box app Virtual Box image of Windows 10 you can find here Virtual Box image of Ubuntu you can find here Monitoring tools Kibana Grafana Azure Application Insights OCR services ocr.space structurise.com | make OCR from the clipboard Temporary Email Services No. Temporary Email Service URL 1 10 Minute Mail https://10minutemail.com/ 2 EmailOnDeck https://www.emailondeck.com/ 3 Fake Mail Generator https://www.fakemailgenerator.net/ 4 FakeMail &#x2F; TempMailAddress https://tempmailaddress.com/ 5 GetNada https://getnada.com/ 6 Guerrilla Mail https://www.guerrillamail.com/ 7 MailCatch http://mailcatch.com/ 8 MailDrop https://maildrop.cc/ 9 Mailinator https://www.mailinator.com/ 10 Mailnesia https://www.mailnesia.com/ 11 Moakt https://www.moakt.com/ 12 MyTemp.email https://www.mytemp.email/ 13 Proton mail https://proton.me/mail 14 Temp Mail https://temp-mail.org/ 15 Tempail https://tempail.com/ 16 Temp-Mail.io https://temp-mail.io/ 17 Throwawaymail https://www.throwawaymail.com/ 18 YOPmail https://www.yopmail.com/ Other toolsScreenrecordersOffline screenrecorderOBS studio Mac &#x2F; Windows &#x2F; Android https://obsproject.com/download Online screenrecorderLoom Mac &#x2F; Windows &#x2F; Android https://www.loom.com/screen-recorder GeneratorsUser fake data jsonplaceholder.typicode.com fake data generator via JS code snippets https://jsonplaceholder.typicode.com/ mockaroo.com fake data generator with prettty UI configuration https://www.mockaroo.com/ Image generators image generator via parametrised HTTP requests https://dummyjson.com/docs/image Free video samplesfile-examples.com&#x2F;video AVI &#x2F; MOV &#x2F; OGG &#x2F; MP4 &#x2F; WMV &#x2F;WEBM https://file-examples.com/index.php/sample-video-files/ Free imagesfile-examples.com&#x2F;images JPG &#x2F; PNG &#x2F;GIF &#x2F; TIFF &#x2F; ICO &#x2F; SVG &#x2F; WEBP https://file-examples.com/index.php/sample-images-download/ Display extensionspacedeskSpacedesk software turns mobile devices, laptops and desktops into an additional extension display monitor for Windows machines. Web site: https://www.spacedesk.net/ Download page: https://www.spacedesk.net/download/ Installation guide for Windows: https://www.youtube.com/watch?v=iJ2bFcDUol8&amp;t=262s","categories":[{"name":"My contribution","slug":"My-contribution","permalink":"https://ooge0.github.io/hexo-blog/categories/My-contribution/"}],"tags":[{"name":"tutorial","slug":"tutorial","permalink":"https://ooge0.github.io/hexo-blog/tags/tutorial/"},{"name":"apps","slug":"apps","permalink":"https://ooge0.github.io/hexo-blog/tags/apps/"},{"name":"my_contribution","slug":"my-contribution","permalink":"https://ooge0.github.io/hexo-blog/tags/my-contribution/"}]},{"title":"HEXO.IO blog examples","slug":"tutorials/hexo.io_tutorial_list","date":"2024-10-20T21:00:00.000Z","updated":"2024-10-30T14:26:21.973Z","comments":true,"path":"2024/10/21/tutorials/hexo.io_tutorial_list/","permalink":"https://ooge0.github.io/hexo-blog/2024/10/21/tutorials/hexo.io_tutorial_list/","excerpt":"","text":"Setup my blog using Hexo: https://chuanjin.me/2015/01/17/Setup-my-blog-using-Hexo/ using and optimizing the hexo-tag-cloud tag cloud: https://tiven.cn/p/8dbf2af/ hexo-daily-news: https://github.com/Shiguang-coding/hexo-daily-news","categories":[{"name":"Tutorials","slug":"Tutorials","permalink":"https://ooge0.github.io/hexo-blog/categories/Tutorials/"}],"tags":[{"name":"hexo_io","slug":"hexo-io","permalink":"https://ooge0.github.io/hexo-blog/tags/hexo-io/"},{"name":"examples","slug":"examples","permalink":"https://ooge0.github.io/hexo-blog/tags/examples/"}]},{"title":"MarkDowm tutorial-2","slug":"tutorials/markdown_tutorial_2","date":"2024-10-20T21:00:00.000Z","updated":"2024-10-30T14:27:02.276Z","comments":true,"path":"2024/10/21/tutorials/markdown_tutorial_2/","permalink":"https://ooge0.github.io/hexo-blog/2024/10/21/tutorials/markdown_tutorial_2/","excerpt":"","text":"This post is originated from here and is used for testing markdown style. This post contains nearly every markdown usage. Make sure all the markdown elements below show up correctly. This is intended as a quick reference and showcase. For more complete info, see John Gruber’s original spec and the Github-flavored Markdown info page. This cheatsheet is specifically Markdown Here’s version of Github-flavored Markdown. This differs slightly in styling and syntax from what Github uses, so what you see below might vary a little from what you get in a Markdown Here email, but it should be pretty close. Table of ContentsHeadersEmphasisListsLinksImagesCode and Syntax HighlightingBlockquotesInline HTMLHorizontal RuleLine Breaks ## Headers 1234567891011121314# H1## H2### H3#### H4##### H5###### H6Alternatively, for H1 and H2, an underline-ish style:Alt-H1======Alt-H2------ H1H2H3H4H5H6Alternatively, for H1 and H2, an underline-ish style: Alt-H1Alt-H2 ## Emphasis 12345Emphasis, aka italics, with *asterisks* or _underscores_.Strong emphasis, aka bold, with **asterisks** or __underscores__.Combined emphasis with **asterisks and _underscores_**. Emphasis, aka italics, with asterisks or underscores. Strong emphasis, aka bold, with asterisks or underscores. Combined emphasis with asterisks and underscores. ## Lists 1234567891011121. First ordered list item2. Another item * Unordered sub-list. 1. Actual numbers don&#x27;t matter, just that it&#x27;s a number 1. Ordered sub-list4. And another item. Some text that should be aligned with the above item.* Unordered list can use asterisks- Or minuses+ Or pluses First ordered list item Another item Unordered sub-list. Actual numbers don’t matter, just that it’s a number Ordered sub-list And another item. Some text that should be aligned with the above item. Unordered list can use asterisks Or minuses Or pluses ## Links There are two ways to create links. 12345678910111213[I&#x27;m an inline-style link](https://www.google.com)[I&#x27;m a reference-style link][Arbitrary case-insensitive reference text][You can use numbers for reference-style link definitions][1]Or leave it empty and use the [link text itself][]Some text to show that the reference links can follow later.[arbitrary case-insensitive reference text]: https://www.mozilla.org[1]: http://slashdot.org[link text itself]: http://www.reddit.com I’m an inline-style link I’m a reference-style link You can use numbers for reference-style link definitions Or leave it empty and use the link text itself Some text to show that the reference links can follow later. Images1Here&#x27;s our logo (hover to see the title text): Inline-style: Reference-style: 12345678910111213141516171819Here&#x27;s our logo (hover to see the title text):Inline-style: ![alt text](https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png &quot;Logo Title Text 1&quot;)Reference-style: ![alt text][logo][logo]: https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png &quot;Logo Title Text 2&quot;&lt;a name=&quot;code&quot;/&gt;## Code and Syntax HighlightingCode blocks are part of the Markdown spec, but syntax highlighting isn&#x27;t. However, many renderers -- like Github&#x27;s and *Markdown Here* -- support syntax highlighting. *Markdown Here* supports highlighting for dozens of languages (and not-really-languages, like diffs and HTTP headers); to see the complete list, and how to write the language names, see the [highlight.js demo page](http://softwaremaniacs.org/media/soft/highlight/test.html).```no-highlightInline `code` has `back-ticks around` it. Inline code has back-ticks around it. Blocks of code are either fenced by lines with three back-ticks ```, or are indented with four spaces. I recommend only using the fenced code blocks – they’re easier and only they support syntax highlighting. 123```javascriptvar s = &quot;JavaScript syntax highlighting&quot;;alert(s); 12s = &quot;Python syntax highlighting&quot;print s 12No language indicated, so no syntax highlighting. But let&#x27;s throw in a &lt;b&gt;tag&lt;/b&gt;. 1234```javascriptvar s = &quot;JavaScript syntax highlighting&quot;;alert(s); 12s = &quot;Python syntax highlighting&quot;print s 12No language indicated, so no syntax highlighting in Markdown Here (varies on Github). But let&#x27;s throw in a &lt;b&gt;tag&lt;/b&gt;. (Github Wiki pages don’t seem to support syntax highlighting, so the above won’t be colourful (the strings are not red, for example). Try it out in a Markdown Here email or a Github Markdown README or Github Issue – you can preview a new Issue without submitting it.) Again, to see what languages are available for highlighting, and how to write those language names, see the highlight.js demo page. ## Blockquotes 123456&gt; Blockquotes are very handy in email to emulate reply text.&gt; This line is part of the same quote.Quote break.&gt; This is a very long line that will still be quoted properly when it wraps. Oh boy let&#x27;s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can *put* **Markdown** into a blockquote. Blockquotes are very handy in email to emulate reply text.This line is part of the same quote. Quote break. This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can put Markdown into a blockquote. ## Inline HTML You can also use raw HTML in your Markdown, and it’ll mostly work pretty well. Here are a couple of common examples: 12345678910111213141516171819202122232425262728&lt;dl&gt; &lt;dt&gt;Definition list&lt;/dt&gt; &lt;dd&gt;Is something people use sometimes.&lt;/dd&gt; &lt;dt&gt;Markdown in HTML&lt;/dt&gt; &lt;dd&gt;Does *not* work **very** well. Use HTML &lt;em&gt;tags&lt;/em&gt;.&lt;/dd&gt;&lt;/dl&gt;&lt;table&gt; &lt;tr&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;Tables&lt;/th&gt; &lt;th&gt;Are&lt;/th&gt; &lt;th&gt;Cool&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Zebra&lt;/th&gt; &lt;td&gt;Stripes&lt;/td&gt; &lt;td&gt;Are&lt;/td&gt; &lt;td&gt;Pretty&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Here&lt;/th&gt; &lt;td&gt;Is&lt;/td&gt; &lt;td&gt;Another&lt;/td&gt; &lt;td&gt;Row&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt; Definition list Is something people use sometimes. Markdown in HTML Does *not* work **very** well. Use HTML tags. Tables Are Cool Zebra Stripes Are Pretty Here Is Another Row Github-flavored Markdown supports a special table syntax, but Markdown Here does not support it yet. There’s an issue for it. ## Horizontal Rule 12345678910111213Three or more...---Hyphens***Asterisks___Underscores Three or more… Hyphens Asterisks Underscores ## Line Breaks My basic recommendation for learning how line breaks work is to experiment and discover – hit &lt;Enter&gt; once, then hit it twice, see what happens. You’ll soon learn to get what you want. “Markdown Toggle” is your friend. Here are some things to try out: 1234567With only a single newline, this line andthis line will be a *single line*.But this one is separated by two newlines and so will be a *separate paragraph*.This line has two spaces at the end (hard for you to see, but trust me!). So this is a separate line in the *same paragraph*. With only a single newline, this line andthis line will be a single line. But this one is separated by two newlines and so will be a separate paragraph. This line has two spaces at the end (hard for you to see, but trust me!).So this is a separate line in the same paragraph.","categories":[{"name":"Tutorials","slug":"Tutorials","permalink":"https://ooge0.github.io/hexo-blog/categories/Tutorials/"}],"tags":[{"name":"tutorial","slug":"tutorial","permalink":"https://ooge0.github.io/hexo-blog/tags/tutorial/"},{"name":"markdown","slug":"markdown","permalink":"https://ooge0.github.io/hexo-blog/tags/markdown/"}]},{"title":"AI papers","slug":"post_ai__mix_of_instances","date":"2024-10-18T08:11:11.000Z","updated":"2024-11-27T23:34:31.358Z","comments":true,"path":"2024/10/18/post_ai__mix_of_instances/","permalink":"https://ooge0.github.io/hexo-blog/2024/10/18/post_ai__mix_of_instances/","excerpt":"","text":"Variational AutoencoderPaper: Book: An Introduction to Variational Autoencoders. Diederik P. Kingma DOI:10.1561&#x2F;2200000056 Read on arxiv.org Read on sci-hub.se","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"ML","slug":"ML","permalink":"https://ooge0.github.io/hexo-blog/tags/ML/"},{"name":"papers","slug":"papers","permalink":"https://ooge0.github.io/hexo-blog/tags/papers/"}]},{"title":"AI papers","slug":"post_ai__papers","date":"2024-10-18T08:11:11.000Z","updated":"2024-11-20T10:06:55.828Z","comments":true,"path":"2024/10/18/post_ai__papers/","permalink":"https://ooge0.github.io/hexo-blog/2024/10/18/post_ai__papers/","excerpt":"","text":"Artificial Intelligence. Scientific journal (UA) https://jai.in.ua/index.php/en/ The journal has been published since 1995. The journal registered in the list of specialized editions of Ukraine, where main dissertations results for the scientific degree in the area of engineering science and physics &amp; mathematics are published (Order of MES of Ukraine dated by 15.04.2021, № 420). The DOI name is assigned to the journal (Digital Object Identifier). Archive. Journal issues by years sci-hub https://sci-hub.se/ Sci-Hub is the most controversial project in modern science. The goal of Sci-Hub is to provide free and unrestricted access to all scientific knowledge.","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"ML","slug":"ML","permalink":"https://ooge0.github.io/hexo-blog/tags/ML/"},{"name":"papers","slug":"papers","permalink":"https://ooge0.github.io/hexo-blog/tags/papers/"}]},{"title":"Psychology","slug":"post_psy__papers","date":"2024-10-18T08:11:11.000Z","updated":"2024-11-27T23:20:07.740Z","comments":true,"path":"2024/10/18/post_psy__papers/","permalink":"https://ooge0.github.io/hexo-blog/2024/10/18/post_psy__papers/","excerpt":"","text":"Carroll Izard. The Psychology of Emotions Patterns of emotions. A new analysis of anxiety and depression. Carroll Izard The American mind; an interpretation of American thought and character since the 1880’s Read on archive.org","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"ML","slug":"ML","permalink":"https://ooge0.github.io/hexo-blog/tags/ML/"},{"name":"papers","slug":"papers","permalink":"https://ooge0.github.io/hexo-blog/tags/papers/"}]},{"title":"LLM system prompts","slug":"post_ai_ml__llm_system_prompts","date":"2024-09-28T09:18:33.000Z","updated":"2024-11-21T18:20:04.381Z","comments":true,"path":"/ai_ml_llm_system_prompts/","permalink":"https://ooge0.github.io/hexo-blog/ai_ml_llm_system_prompts/","excerpt":"","text":"LLM System Prompt Generator💡 Creative Idea Generator — AI-powered brainstorming tool for generating creative solutions and ideas 🤖 ChatGPT: LLM System Prompt Generator — Generate optimized system prompts for different LLM model sizes (3B, 33B, 70B, etc.)CORE FEATURES CUSTOMIZED PROMPT ENGINEERING:Specializes in crafting highly optimized prompts tailored to transform language models into expert agents for specific domains and tasks. CHAIN OF THOUGHT (CoT) IMPLEMENTATION:Embeds a step-by-step reasoning process into prompts, ensuring models respond logically and systematically to complex tasks. OPTIMIZATION FOR MODEL SIZE: Small Models (e.g., 1B parameters): Simple instructions, focus on one clear task, and basic language. Large Models (e.g., 100B+ parameters): Nuanced context, multi-layered tasks, and detailed guidance. NEGATIVE PROMPTING:Defines undesired behaviors explicitly to prevent the model from producing irrelevant or low-quality outputs. FEW-SHOT AND ZERO-SHOT DESIGN: Few-shot examples: Embedded for complex tasks. Zero-shot examples: Designed for instant task execution without examples when tasks are straightforward. ROBUST ERROR HANDLING:Includes instructions for edge cases, ambiguities, and incomplete inputs to maintain response quality. MULTI-TASK OPTIMIZATION:Adapts prompts for different tasks (e.g., classification, summarization, generation) with clear instructions and strategies. SCALABILITY:Prompts are modular and scalable, working across tasks, domains, and model sizes with minimal customization. EMBEDDED DOMAIN KNOWLEDGE:Integrates task-relevant technical terminology and context to enable models to act as subject matter experts. STRUCTUREEvery prompt is designed using a modular structure for clarity and effectiveness: Role Definition:Clearly defines the model’s role (e.g., “YOU ARE THE WORLD’S FOREMOST EXPERT IN…”) to set expectations for tone, expertise, and behavior. Task Instructions:Provides actionable, precise instructions for the task, often broken down into smaller steps for clarity. Chain of Thoughts (CoT):Embeds a logical framework for reasoning: Understand: Read and comprehend the task. Basics: Identify fundamental concepts. Break Down: Divide the problem into smaller subtasks. Analyze: Use facts and reasoning to address each subtask. Build: Assemble the solution coherently. Edge Cases: Consider and address exceptions. Final Answer: Present the solution clearly. What Not To Do Section:Explicitly defines behaviors and outputs the model must avoid. Negative prompts are written in ALL CAPS to reinforce importance. Few-Shot&#x2F;Zero-Shot Examples: Few-shot: Embedded for complex tasks to guide behavior. Zero-shot: Simplified for tasks where examples are unnecessary. Error Handling Guidelines:Ensures models can handle incomplete, ambiguous, or incorrect inputs. Optimization Instructions:Includes suggestions to maximize accuracy, relevance, and response quality based on the task and model size. ALGORITHMSThe following principles guide the chatbot for prompt engineering process: TASK DECOMPOSITION:Breaks complex tasks into smaller subtasks and uses a structured Chain of Thoughts (CoT) framework to guide step-by-step reasoning. ITERATIVE REFINEMENT:Prompts are iteratively refined to balance clarity, depth, and specificity while eliminating ambiguities. MODEL-CENTRIC DESIGN: Smaller Models: Focus on simplicity and clarity. Larger Models: Leverage capabilities by introducing multi-task prompts and nuanced instructions. RELEVANCE FILTERING:Ensures that only task-relevant details are included in prompts, reducing noise and improving focus. NEGATIVE PROMPT ALGORITHMS:Outlines specific failure modes and structures the prompt to avoid undesired behaviors or outputs. SCALABLE TEMPLATES:Prompts are built on scalable templates that can be adjusted across domains, tasks, and model sizes with ease. RULESTo ensure high-quality outputs, chatbot follow these rules: CHAIN OF THOUGHTS IS MANDATORY:Prompts must guide the model through step-by-step reasoning for complex tasks. CLEAR INSTRUCTIONS:Tasks must be unambiguous, actionable, and easy to interpret. TAILOR TO MODEL SIZE:Adjust complexity and instruction depth based on model capacity: Small Models: Simplified instructions. Large Models: Advanced and multi-layered instructions. INCLUDE NEGATIVE PROMPTS:Define behaviors and outputs to avoid, ensuring the model remains focused and produces high-quality responses. DOMAIN KNOWLEDGE INTEGRATION:For expert-level tasks, prompts must include relevant background information and context. ROBUST ERROR HANDLING:Include guidance for edge cases and ambiguous inputs. NO REDUNDANCY:Avoid repetitive instructions that could confuse the model. RESTRICTIONSTo prevent undesired behaviors, these restrictions are imposed: NEVER ALLOW GENERIC OUTPUTS:Responses must be detailed and specific. Avoid vague answers like “It depends” or “More information is needed” unless explicitly allowed. AVOID UNINFORMED RESPONSES:If the model lacks knowledge, it must explicitly state limitations rather than guessing. NO CONTRADICTIONS:Prompts must avoid contradictory instructions that could confuse the model. RESTRICT INACCURACIES:Outputs must be factually accurate and avoid misinformation. DO NOT OVERCOMPLICATE SIMPLE TASKS:Ensure simpler tasks are addressed concisely without unnecessary complexity. NO UNDEFINED JARGON:Use domain-specific terminology appropriately, avoiding undefined or irrelevant jargon. NO MULTIPLE TASKS WITHOUT CLEAR SEGREGATION:If multiple tasks are included, they must be clearly separated to avoid conflation. EXAMPLESHere’s how these principles are applied in real-world scenarios: 1. Medical Expert PromptYOU ARE A BOARD-CERTIFIED MEDICAL EXPERT WITH 15 YEARS OF EXPERIENCE IN DIAGNOSING AND TREATING COMPLEX CONDITIONS... ### Instructions ### - Provide evidence-based answers... - Reference guidelines such as the CDC and WHO... - Use concise, professional medical terminology... ### Chain of Thoughts ### 1. Understand the condition... 2. Consider symptoms... 3. Suggest diagnostic tests... 4. Propose evidence-based treatments... ...","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"ML","slug":"ML","permalink":"https://ooge0.github.io/hexo-blog/tags/ML/"},{"name":"LLM","slug":"LLM","permalink":"https://ooge0.github.io/hexo-blog/tags/LLM/"}]},{"title":"AI - Machine Learning frameworks","slug":"post_ai_ml__ml_frameworks","date":"2024-09-28T09:18:33.000Z","updated":"2024-11-30T15:13:06.251Z","comments":true,"path":"2024/09/28/post_ai_ml__ml_frameworks/","permalink":"https://ooge0.github.io/hexo-blog/2024/09/28/post_ai_ml__ml_frameworks/","excerpt":"","text":"ML frameworks (Example-1) Linkedin post: Understanding Machine Learning Frameworks Stanford CS229 I Machine Learning I Building Large Language Models (LLMs) YouTube video course. Duration: 1h 44 mins This lecture provides a concise overview of building a ChatGPT-like model, covering both pretraining (language modeling) and post-training (SFT&#x2F;RLHF). For each component, it explores common practices in data collection, algorithms, and evaluation methods. This guest lecture was delivered by Yann Dubois in Stanford’s CS229: Machine Learning course, in Summer 2024. For more information about Stanford’s Artificial Intelligence programs visit: https://stanford.io/ai","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"ML","slug":"ML","permalink":"https://ooge0.github.io/hexo-blog/tags/ML/"}]},{"title":"Machine Learning Techniques - Overview","slug":"post_ai_ml__techniques_overview_1","date":"2024-09-28T09:18:33.000Z","updated":"2024-11-23T22:49:49.972Z","comments":true,"path":"2024/09/28/post_ai_ml__techniques_overview_1/","permalink":"https://ooge0.github.io/hexo-blog/2024/09/28/post_ai_ml__techniques_overview_1/","excerpt":"","text":"In this guide, we will review popular machine learning techniques with a brief overview of their applications, complexity levels, and some historical context. This list provides a starting point for understanding core machine learning methods and their practical uses. 1. Linear Regression Overview: A fundamental statistical method used to predict a dependent variable based on an independent variable. Commonly applied in forecasting and trend analysis. Complexity: ⭐ Inventor: Sir Francis Galton (DOB: 1822-1911) and Karl Pearson (DOB: 1857-1936) Key Publication: Book:Pearson, K. (1896). “Mathematical Contributions to the Theory of Evolution“ 2. Logistic Regression Overview: Primarily used for binary classification, logistic regression estimates the probability of an outcome and is used widely in classification tasks like spam detection. Complexity: ⭐⭐ Inventor: David Cox (DOB: 1924-2022) Key Publication: Cox, D.R. (1958). “The Regression Analysis of Binary Sequences.” 3. Decision Trees Overview: A tree-like structure for decision-making and predictive modeling. Suitable for classification and regression problems. Complexity: ⭐⭐ Inventor: Ross Quinlan (DOB: 1944) Key Publication: Quinlan, J.R. (1986). “Induction of Decision Trees.” 4. Random Forests Overview: An ensemble method that builds multiple decision trees and combines them to improve accuracy. Known for its robustness and ease of use. Complexity: ⭐⭐⭐ Inventor: Leo Breiman (DOB: 1928-2005) Key Publication: Breiman, L. (2001). “Random Forests.” 5. Support Vector Machines (SVM) Overview: An effective classification technique that finds the hyperplane maximizing the margin between data classes. Widely used in image and text classification. Complexity: ⭐⭐⭐ Inventor: Vladimir Vapnik (DOB: 1936) Key Publication: Book: Vapnik, V. (1995). “The Nature of Statistical Learning Theory.” 6. K-Nearest Neighbors (KNN) Overview: A simple, non-parametric method used for classification and regression. It predicts outcomes based on the ‘k’ nearest data points. Complexity: ⭐⭐ Inventor: Evelyn Fix (DOB: 1904-1987) and Joseph Hodges (DOB: 1922-2000) Key Publication: Paper: DOI: 10.2307&#x2F;1403797 Fix, E., &amp; Hodges, J.L. (1951). “Discriminatory Analysis.” Book: Fix, E., &amp; Hodges, J.L. (1951). “Discriminatory Analysis.” 7. Naive Bayes Overview: Based on Bayes’ theorem, this technique is widely used for text classification due to its simplicity and effectiveness with large datasets. Complexity: ⭐ Inventor: Thomas Bayes (DOB: 1701-1761) Key Publication: Paper: Bayes, T. (1763). “An Essay towards solving a Problem in the Doctrine of Chances.” 8. Gradient Boosting Overview: An ensemble technique that combines weak learners to minimize errors iteratively. Known for its high accuracy in structured data tasks. Complexity: ⭐⭐⭐⭐ Inventor: Jerome Friedman (DOB: 1939) Key Publication: DOI: 10.2307&#x2F;2699986 Paper: Friedman, J.H. (2001). “Greedy Function Approximation: A Gradient Boosting Machine.” 9. Neural Networks Overview: Inspired by biological neural networks, these algorithms are used in complex tasks like image recognition and language processing. Complexity: ⭐⭐⭐⭐⭐ Inventor: Frank Rosenblatt (DOB: 1928-1971) Key Publication: Paper: Rosenblatt, F. (1958). “The Perceptron: A Probabilistic Model for Information Storage and Organization.” 10. K-Means Clustering Overview: An unsupervised algorithm that partitions data into ‘k’ clusters based on similarity. Commonly used for market segmentation and image compression. Complexity: ⭐⭐ Inventor: Stuart Lloyd (DOB: 1933-2006) Key Publication: Paper: DOI:10.1109&#x2F;TIT.1982.1056489 Lloyd, S.P. (1982). “Least Squares Quantization in PCM.” 11. Principal Component Analysis (PCA) Overview: A dimensionality reduction technique that transforms data into a set of orthogonal components, enhancing interpretability without sacrificing information. Complexity: ⭐⭐ Inventor: Karl Pearson (DOB: 1857-1936) Key Publication: Paper: DOI:10.1080&#x2F;14786440109462720 Pearson, K. (1901). “On Lines and Planes of Closest Fit to Systems of Points.” 12. Reinforcement Learning (RL) Overview: An area focused on training models to make sequences of decisions, particularly for game playing and robotics. Complexity: ⭐⭐⭐⭐⭐ Inventor: Richard Sutton (DOB: 1952) Key Publication: Books in Brief: DOI:10.1109&#x2F;TNN.1998.712192 Sutton, R., &amp; Barto, A. (1998). “Reinforcement Learning: An Introduction.” semanticscholar link Book: Sutton, R., &amp; Barto, A. (1998). “Reinforcement Learning: An Introduction.” - online_1 Book: Sutton, R., &amp; Barto, A. (1998). “Reinforcement Learning: An Introduction.” - online_2 These techniques represent a wide spectrum of machine learning methods used across industries. Whether for predictive modeling, classification, clustering, or decision-making, understanding these core methods is foundational to exploring more advanced machine learning concepts. Below is other view on ML models Model Name Year of Creation Inventor(s) Key Publication DOI Linear Discriminant Analysis (LDA) Early 1900s Ronald Fisher The Use of Multiple Measurements in Taxonomic Problems.1936 Read on sci-hub.se 10.1111&#x2F;j.1469-1809.1936.tb02137.x Support Vector Machine (SVM) 1960s Vladimir Vapnik The Nature of Statistical Learning Theory. Read on sci-hub.se 10.1007&#x2F;978-1-4757-3264-1 Kernel SVM 1990s Bernhard Schölkopf, Alexander Smola Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond 1 10.1007&#x2F;978-0-387-31471-7 Naive Bayes 1763s Thomas Bayes An Essay towards solving a Problem in the Doctrine of Chances. 1763 Read on sci-hub.se Read on bayes.wustl.edu 10.1098&#x2F;rstl.1763.0053 Naive Bayes Early 1900s Pierre-Simon Laplace Théorie Analytique des Probabilités - Logistic Regression Early 1900s Various Researchers Statistical Methods for Research Workers. Read on sci-hub.se 10.1016&#x2F;b978-044450871-3&#x2F;50148-0 Decision Tree 1960s J. Ross Quinlan Induction of decision tree.1s986 Read on sci-hub.se 10.1023&#x2F;A:1022643204877 Random Forest 1990s Leo Breiman Random Forests 10.1023&#x2F;A:1010933404324 Gradient Boosting Machine (GBM) 1990s Leo Breiman Friedman, J. H. (1999). Greedy Function Approximation: A Gradient Boosting Machine. 2001 Read on sci-hub or Read on jerryfriedman.su 10.1214&#x2F;aos&#x2F;1013203451 Gaussian Mixture Model (GMM) Early 1900s Karl Pearson Contributions to the Mathematical Theory of Evolution.1896 Read on sci-fi.se or Read on quantresearch.org 10.1098&#x2F;rsta.1896.0007 Hidden Markov Model (HMM) 1960s Leonard E. Baum, Lloyd R. Rabiner Statistical Methods for Speech Recognition.2006 Read on sci-hub.se 10.1016&#x2F;b0-08-044854-2&#x2F;00907-x","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"ML","slug":"ML","permalink":"https://ooge0.github.io/hexo-blog/tags/ML/"}]},{"title":"Publishing.ReadTheDocs vs GitHub pages. Tutorial-1","slug":"post_docs_publishing__readthedoc_vs_github_pages_tutorial_1","date":"2024-09-28T09:18:33.000Z","updated":"2024-12-10T13:47:12.476Z","comments":true,"path":"2024/09/28/post_docs_publishing__readthedoc_vs_github_pages_tutorial_1/","permalink":"https://ooge0.github.io/hexo-blog/2024/09/28/post_docs_publishing__readthedoc_vs_github_pages_tutorial_1/","excerpt":"","text":"Here is comparison for publishing of ReadTheDocs and GitHub: pages project. Now I faced with some issues of publishing existing project documentation that was created by Sphinx1For my personal project on GIT: python_TA_web_api_framework I creted project documentation and generated by Sphinx all necessary data but for some technical reason had no luck to publish it in the web. Researching existing approaches showed me that there are several platforms for publishing documentation, created mostly in rst2 formatConfiguration of GitHub: Pages and ReadTheDocs: for same project content is different. GitHub: Pages ReadTheDocs In the end after many attempts of building GitHub: Pages and ReadTheDocs: I found test project for playing readthedocs on official readthedocs GitHub: Assumption of making similar projects on GitHub: Pages and ReadTheDocs: failed. Result below Source ReadTheDocs projects was https://github.com/ooge0/test-builds https://test-builds.readthedocs.io/ Main configuration files for ReadTheDocs: publishing are below.readthedocs.yaml 12345678910111213version: 2build: os: ubuntu-24.04 tools: python: latestsphinx: configuration: docs/conf.pypython: install: - requirements: requirements.txt requirements.txt 12sphinx-autorunsphinx-rtd-theme&gt;=3.0.0rc2 Another projects for investigation of publishing&#x2F;deploying conflicts coding-with-kids GitHub: coding-with-kids ReadTheDocs: coding-with-kids test-builds GitHub: test-builds ReadTheDocs: test-builds readthedocs-demo-app(my project) GitHub: readthedocs-demo-app GitHub Pages: readthedocs-demo-app ReadTheDocs: readthedocs-demo-app. Copied and published based on GitHub: test-builds docs.readthedocs.io GitHub: docs.readthedocs.io ReadTheDocs: docs.readthedocs.io ReadTheDocs: readthedocs-demo-app Copied and published based on GitHub: test-builds jubilant-lamp GitHub: jubilant-lamp GitHub pages: jubilant-lamp References:1. Post: Sphinx to GitHub Pages via GitHub Actions2. Post: Генератор документации Sphinx3. Automate Python Documentation | Use Sphinx + GitHub actions to publish documentation1.Sphinx is a powerful documentation generator that has many features for writing technical documentation. Sphinx is written in Python, and supports documentation written in reStructuredText and Markdown. Official docs on the web site: https://www.sphinx-doc.org ↩2.reStructuredText (RST, ReST, or reST). reStructuredText is an easy-to-read, what-you-see-is-what-you-get plaintext markup syntax and parser system. It is useful for in-line program documentation (such as Python docstrings), for quickly creating simple web pages, and for standalone documents. reStructuredText is designed for extensibility for specific application domains. The reStructuredText parser is a component of Docutils. reStructuredText is a revision and reinterpretation of the StructuredText and Setext lightweight markup systems. ↩","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"ReadTheDocs","slug":"ReadTheDocs","permalink":"https://ooge0.github.io/hexo-blog/tags/ReadTheDocs/"},{"name":"[object Object]","slug":"object-Object","permalink":"https://ooge0.github.io/hexo-blog/tags/object-Object/"},{"name":"DocPublishing","slug":"DocPublishing","permalink":"https://ooge0.github.io/hexo-blog/tags/DocPublishing/"}]},{"title":"About me","slug":"about/my_bio","date":"2024-09-28T09:18:33.000Z","updated":"2024-12-04T20:03:52.492Z","comments":true,"path":"/about-me/","permalink":"https://ooge0.github.io/hexo-blog/about-me/","excerpt":"","text":"&nbsp;&nbsp;&nbsp;&nbsp;I’m excited to share insights, tips, and resources from nearly 10 years as a QA Software Test Engineer. My first step into the qulity assurance area was in LLC\"NVK\"Spectr\" company where I was a part of the development team. There I was working on the documentation and tested IZR-051 Radiation ashmeter device. &nbsp;&nbsp;&nbsp;&nbsp;Later I continued as field service engineer (Engineer-radiologist) in All-Ukrainian Radiosurgery Center/Clinical Hospital \"Feofaniya\". There main focus was on maintaing and servicing medical equilpment Clinac iX, Novalix TX, BrinaLab/ExacTrack and medical software Aria. &nbsp;&nbsp;&nbsp;&nbsp;At this place I completed course \"Adminstration - Oncology Information System\" by Varian company in Zug, Switzerland. PDF you can find here. &nbsp;&nbsp;&nbsp;&nbsp;After all challenges I was focusing on software industry and switched into software QA/QC completely. &nbsp;&nbsp;&nbsp;&nbsp;In 2024 I have successfully passed Certified ISTQB® professional exam. Cerititficate in PDF you can find here. &nbsp;&nbsp;&nbsp;&nbsp;And now with a passion for continuous learning and delivering high-quality software continue working in software QA/QC industry. &nbsp;&nbsp;&nbsp;&nbsp;Please find my contact details on my LinkedIn page. &nbsp;&nbsp;&nbsp;&nbsp;My latest CV you can see here. My strong sides are: Proven ability to design, execute, and automate comprehensive test plans across web, mobile, and desktop applications. Skilled in collaborating effectively within cross-functional teams to achieve customer satisfaction. Eager to leverage experience and knowledge in hybrid technologies to contribute to a stable and innovative company. --- Highlights: ISTQB® Certified Tester - Foundation Level Extensive experience in manual and automated testing utilizing different methodologies. Proficient in various tools and technologies (JIRA, Azure DevOps, Postman, Selenium, etc.) Full list of tools that I'm using in my daily works is here. Experience in building and maintaining test automation frameworks Strong communication and collaboration skills Passionate about learning modern technologies (web scrapping, data processing, artificial intelligence) Remote work experience List of completed test reports >> here --- Additional Information: Available for a full-time remote position only Available for remote consultation, maintaing products, audit of web site. --- Hobbies: Cycling, Hiking, python coding, IOT, exploring AI area. Full details and work experience available upon request. My best achievement is the finished endurance trip in 136 км from Kolka to Dubulti in 2019. I completed that trip in 52 hours. More detilas about event you can find on the official BK Kolka - Dubulti website., my number is 943. &nbsp;&nbsp;&nbsp;&nbsp;*Please read carefully all information on the pages and if you require any further information, feel free to contact me via LinkedIn Total views:","categories":[{"name":"Personal information","slug":"Personal-information","permalink":"https://ooge0.github.io/hexo-blog/categories/Personal-information/"}],"tags":[{"name":"about_me","slug":"about-me","permalink":"https://ooge0.github.io/hexo-blog/tags/about-me/"}]},{"title":"UX UI tools","slug":"tutorials/ux_ui_tools","date":"2024-09-28T09:18:33.000Z","updated":"2024-11-02T09:00:17.788Z","comments":true,"path":"2024/09/28/tutorials/ux_ui_tools/","permalink":"https://ooge0.github.io/hexo-blog/2024/09/28/tutorials/ux_ui_tools/","excerpt":"","text":"This post contains a list of discovered UX/UI web tools and additional information to help understand their features. List of tools cssgradient.io cssgradient.io Home Page https://cssgradient.io/ See Also: Color Shades Gradient Backgrounds","categories":[{"name":"Tutorials","slug":"Tutorials","permalink":"https://ooge0.github.io/hexo-blog/categories/Tutorials/"}],"tags":[{"name":"tutorial","slug":"tutorial","permalink":"https://ooge0.github.io/hexo-blog/tags/tutorial/"},{"name":"ux_ui","slug":"ux-ui","permalink":"https://ooge0.github.io/hexo-blog/tags/ux-ui/"}]},{"title":"Hello World","slug":"hello-world","date":"2024-09-20T19:47:58.000Z","updated":"2024-10-22T15:35:07.540Z","comments":true,"path":"2024/09/20/hello-world/","permalink":"https://ooge0.github.io/hexo-blog/2024/09/20/hello-world/","excerpt":"","text":"Welcome to Hexo! This is my very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"Software QA - Basic check list","slug":"post_qa__software_qa_basic_check_list","date":"2024-08-27T21:00:00.000Z","updated":"2024-11-21T22:32:36.431Z","comments":true,"path":"2024/08/28/post_qa__software_qa_basic_check_list/","permalink":"https://ooge0.github.io/hexo-blog/2024/08/28/post_qa__software_qa_basic_check_list/","excerpt":"","text":"1 Login form Test Cases Positive test cases: Valid data: Correct login and password. Various email formats (with dots, hyphens, etc.). Different password lengths (minimum, maximum, average). Combinations of uppercase and lowercase letters, numbers, special characters. Login after registration: Login with a newly created account. Login after password recovery: Login after successful password recovery. Login after deleting account and registering new one using same credentials Login using social networks: Login through various social networks (if this feature is available). Negative test cases: Invalid data: Empty fields. Incorrect email format. Password too short&#x2F;long. Incorrect password. Special characters instead of letters and numbers. SQL injection. XSS attacks. Blocking: Account blocking after several failed login attempts. IP address blocking after several failed login attempts. Missing fields: Attempting to log in without filling in required fields. Non-existent user: Attempting to log in with a non-existent login. Other tests: User interface tests: Checking the visual design of the form. Checking the accessibility of form elements using the keyboard. Checking the operation of prompts and error messages. Performance tests: Checking the response time of the form under heavy load. Security tests: Checking for vulnerabilities to various types of attacks (SQL injection, XSS, CSRF, etc.). 2 Order creation Test Cases Positive test cases: Valid data: Correct user information (name, address, contact details). Valid payment information (credit card, PayPal, etc.). Correct shipping address. Valid product selection. Correct quantity. Successful order placement. Order modification: Changing shipping address. Modifying product quantity. Canceling an order. Order tracking: Tracking order status from placement to delivery. Order history: Viewing previous orders. Negative test cases: Invalid data: Empty fields. Incorrect email format. Invalid payment information. Incorrect shipping address. Out-of-stock products. Negative quantity. Payment errors: Declined payment. Insufficient funds. Invalid payment method. Order placement errors: Server errors. System failures. Other tests: User interface tests: Checking the visual design of the order form. Checking the accessibility of form elements using the keyboard. Checking the operation of prompts and error messages. Performance tests: Checking the response time of the order process under heavy load. Security tests: Checking for vulnerabilities to various types of attacks (SQL injection, XSS, CSRF, etc.).","categories":[{"name":"QA","slug":"QA","permalink":"https://ooge0.github.io/hexo-blog/categories/QA/"}],"tags":[{"name":"qa","slug":"qa","permalink":"https://ooge0.github.io/hexo-blog/tags/qa/"},{"name":"qa_check_list","slug":"qa-check-list","permalink":"https://ooge0.github.io/hexo-blog/tags/qa-check-list/"}]},{"title":"Tools for Augmenting Data and Generating Texts","slug":"post_ai__tools_for_augmenting_data_and_generating_texts","date":"2024-05-10T15:18:22.000Z","updated":"2024-11-25T20:01:42.003Z","comments":true,"path":"2024/05/10/post_ai__tools_for_augmenting_data_and_generating_texts/","permalink":"https://ooge0.github.io/hexo-blog/2024/05/10/post_ai__tools_for_augmenting_data_and_generating_texts/","excerpt":"","text":"Text Generation ModelsText generation models are trained on large corpora and can generate semantically coherent and lexically varied sentences. GPT-3&#x2F;GPT-4: Generates fluent and contextually relevant text. Supports fine-tuning and controlled outputs through prompt engineering. GPT-2: Effective for text generation and fine-tuning to produce specific semantic or syntactic structures. T5 (Text-to-Text Transfer Transformer): Handles tasks by converting input into text-to-text format, generating sentences from structured input. BERT: Primarily for understanding tasks, but paired with generation heads like BART or T5, it can generate relevant sentences. Text Augmentation LibrariesLibraries for NLP data augmentation by altering existing sentences while retaining their meaning. nlpaug: Augments text by synonym replacement, random insertions, and transformations using word embeddings. PIP reference Post: Data Augmentation library for text nlpaug.readthedocs.io TextAttack: Provides adversarial attacks and paraphrasing methods for generating variations of sentences. TextAttack Documentation Semantic Structure-based Text GenerationTools for precise control over semantic structure in generated sentences. Controlled Text Generation (via GPT-3&#x2F;4): Uses structured prompts to guide text generation toward desired structures or concepts. OpenAI Codex: Generates text based on semantic instructions or structural descriptions. DeepAI’s Text Generation API: Generates text based on input semantics and structure. CTRL (Conditional Transformer Language Model): Conditions text generation on control codes for specific topics or structures. Rule-based Text GenerationGenerates text based on predefined templates or rules. OpenNLP: Uses templates and rules for sentence generation. Apache OpenNLP Developer Documentation Template-based Generation: Tools like Yarn or Jinja2 allow for generating text using templates with placeholders. Lexical Substitution and Paraphrasing ToolsTools for modifying words or phrases while maintaining semantic meaning. Paraphrase Generation with BART or T5: Generates sentence variations that preserve meaning. WordNet-based tools: Uses lexical substitution to replace words with synonyms or semantically related words. Generating Text Based on Semantic StructuresTools and techniques to guide text generation using semantic roles or lexical features. Graph-based Models: Represent word or sentence relationships to guide generation using tools like spaCy. Semantic Role Labeling (SRL): Tools like AllenNLP tag sentence components to generate text following specific roles or patterns. Read more about Semantic Role Labeling in paper: PriMeSRL-Eval: A Practical Quality Metric for Semantic Role Labeling Systems Evaluation Lexical and Syntactic Features: Fine-tunes models to control lexical variety or syntactic structure based on desired patterns. ConclusionA wide range of tools like GPT-3, T5, TextAttack, and nlpaug are available for augmenting data and generating text. They provide flexibility for creating semantically diverse and lexically varied text, while specialized tools like AllenNLP enable controlled generation based on specific structures and constraints.","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"ML","slug":"ML","permalink":"https://ooge0.github.io/hexo-blog/tags/ML/"},{"name":"augmentation","slug":"augmentation","permalink":"https://ooge0.github.io/hexo-blog/tags/augmentation/"},{"name":"text_generation","slug":"text-generation","permalink":"https://ooge0.github.io/hexo-blog/tags/text-generation/"}]},{"title":"Site for blogging","slug":"post_site_for_blogging","date":"2024-04-28T08:22:42.000Z","updated":"2024-11-07T15:52:43.639Z","comments":true,"path":"2024/04/28/post_site_for_blogging/","permalink":"https://ooge0.github.io/hexo-blog/2024/04/28/post_site_for_blogging/","excerpt":"","text":"List of blogging platformsblogger.comindieblog.pageghost.org blogger.com https://www.blogger.com/ indieblog.page https://indieblog.page/all ghost.org https://ghost.org/ Simple example that can be used by . ko6e4ka skookworks","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"blog","slug":"blog","permalink":"https://ooge0.github.io/hexo-blog/tags/blog/"}]},{"title":"Types of protocols for interaction on client-server architecture level.","slug":"post_network_protocols","date":"2024-04-09T06:23:12.000Z","updated":"2024-11-13T14:40:15.193Z","comments":true,"path":"2024/04/09/post_network_protocols/","permalink":"https://ooge0.github.io/hexo-blog/2024/04/09/post_network_protocols/","excerpt":"","text":"Below is a list of most popular types of protocols for interaction on client-server architecture level. Hypertext Transfer Protocol (HTTP): The foundation of the web, HTTP defines how clients (web browsers) request and receive resources (web pages, images, etc.) from servers. It’s a request-response protocol where clients send GET or POST requests and servers respond with data and status codes. File Transfer Protocol (FTP): This protocol allows for the transfer of files between clients and servers. It offers functionalities like uploading, downloading, directory listing, and file management. SSH File Transfer Protocol (SFTP): SFTP (SSH File Transfer Protocol) is a secure file transfer protocol. It runs over the SSH protocol. It supports the full security and authentication functionality of SSH. Simple Mail Transfer Protocol (SMTP): Used for sending emails, SMTP defines how email clients connect to email servers and transmit emails. It focuses on message delivery without concern for content or user interface. Post Office Protocol (POP3) and Internet Message Access Protocol (IMAP): These protocols are used for retrieving emails from a server. POP3 downloads emails to the client, while IMAP allows users to access and manage emails on the server itself. Domain Name System (DNS): This protocol doesn’t directly interact between clients and servers, but it’s crucial for client-server communication. DNS translates human-readable domain names (like [invalid URL removed]) into numerical IP addresses that computers can understand. Remote Procedure Call (RPC): This protocol allows a client to execute a procedure on a server as if it were running locally. It provides a transparent way for clients to access functionalities offered by servers. In advance exists gRPC protocol (gRPC Remote Procedure Calls) is a cross-platform high-performance remote procedure call (RPC) framework. gRPC was initially created by Google, but is open source and is used in many organizations. Secure Sockets Layer (SSL)&#x2F;Transport Layer Security (TLS): These protocols secure communication between clients and servers by encrypting data transmission. They are essential for protecting sensitive information like passwords and credit card data. Message Queueing Protocols: Protocols like AMQP and RabbitMQ enable asynchronous communication between clients and servers. Messages are placed in a queue and processed by the server at its own pace, decoupling clients from server availability. References: What is Client-Server Architecture? Explained in Detail Efficient Client-Server Communication: An Overview of Protocols and Techniques High Performance Browser Networking An overview of HTTP(by MDN Web Docs) Hypertext Transfer Protocol (HTTP) &#x2F; rfc2616 File Transfer Protocol (FTP) &#x2F; rfc114 SSH File Transfer Protocol (SFTP): Get SFTP client &amp; server Simple Mail Transfer Protocol (SMTP) &#x2F; rfc5321 Domain Name System (DNS) &#x2F; rfc1035 Remote Procedure Call (RPC) in Operating System Remote Procedure Call (RPC) &#x2F;rfc5531 Secure Sockets Layer (SSL) &#x2F; rfc6101 Transport Layer Security (TLS) &#x2F; rfc5246 RabbitMQ: An Introduction to Message Queuing, Protocols, and Policies","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"network_protocols","slug":"network-protocols","permalink":"https://ooge0.github.io/hexo-blog/tags/network-protocols/"},{"name":"protocols","slug":"protocols","permalink":"https://ooge0.github.io/hexo-blog/tags/protocols/"}]},{"title":"Test design techniques for mobile applications","slug":"post_qa__test_design_techniques_for_mobile_applications","date":"2024-04-09T06:23:12.000Z","updated":"2024-11-13T14:21:43.662Z","comments":true,"path":"2024/04/09/post_qa__test_design_techniques_for_mobile_applications/","permalink":"https://ooge0.github.io/hexo-blog/2024/04/09/post_qa__test_design_techniques_for_mobile_applications/","excerpt":"","text":"Equivalence Partitioning (EP) What it is: Divides input data into partitions of equivalent data from which test cases can be derived. How it’s used in mobile testing: Input fields (e.g., login forms, search boxes) are tested by creating test cases for valid and invalid partitions (e.g., different password lengths, valid email formats). Boundary Value Analysis (BVA) What it is: Tests boundaries between partitions where errors tend to occur. How it’s used in mobile testing: Test extreme values for input fields, such as the minimum and maximum length of text fields or file uploads (e.g., minimum and maximum resolution of an image upload). Decision Table Testing What it is: Models system behavior with different input conditions using decision tables. How it’s used in mobile testing: Mobile application settings often involve multiple options (e.g., turning on notifications, dark mode, permissions). Decision tables help test combinations of these settings. State Transition Testing What it is: Tests how the application behaves when transitioning from one state to another. How it’s used in mobile testing: Mobile apps often have distinct states, such as login&#x2F;logout, background&#x2F;foreground, or different navigation flows. State transition testing ensures correct app behavior as users interact with various features and transitions between states (e.g., switching between active screens, minimizing and reopening the app). Use Case Testing What it is: Validates the system behavior by following specific user scenarios or workflows. How it’s used in mobile testing: Create real-world scenarios such as registering a new user, placing an order, or navigating a map in an app. It ensures the app works from the user’s perspective for each typical action. Exploratory Testing What it is: Ad-hoc testing technique where the tester actively explores the application without predefined test cases. How it’s used in mobile testing: Discover unexpected behaviors by interacting with various parts of the app (e.g., swiping, pinch-to-zoom, rotating the device). This is particularly useful for UX testing. Usability Testing What it is: Evaluates the app’s user-friendliness and overall user experience. How it’s used in mobile testing: Test the app’s layout, navigation, responsiveness, and interaction. Verify the ease of use of buttons, touch gestures, app loading speed, and error messages. Also, ensure the app is intuitive and consistent with mobile design guidelines (e.g., iOS vs. Android UI&#x2F;UX standards). Compatibility Testing What it is: Verifies the application’s compatibility across different mobile devices, screen sizes, resolutions, and operating systems. How it’s used in mobile testing: Test the app on various device models (iPhones, Android phones) and OS versions (e.g., iOS 14, iOS 15, Android 10, Android 11). Also, check how the app adapts to different screen sizes (phones, tablets) and orientation (portrait vs. landscape). Interrupt Testing What it is: Ensures the application can handle interruptions gracefully. How it’s used in mobile testing: Test how the app reacts to incoming calls, text messages, low battery warnings, switching between apps, or airplane mode. The app should resume properly after the interruption. Localization Testing What it is: Ensures the app supports different languages, time zones, and regional formats. How it’s used in mobile testing: Verify that the app correctly displays translated text, numbers, and date formats across different languages and regions (e.g., US vs. UK date formats, local currencies). 11.** Performance Testing** * What it is: Evaluates the app’s performance in terms of speed, responsiveness, and resource usage. * How it’s used in mobile testing: Simulate low internet bandwidth or unstable network conditions. Test app loading times, battery consumption, memory usage, and responsiveness under heavy usage or background processes. Security Testing What it is: Identifies vulnerabilities and weaknesses in the application. How it’s used in mobile testing: Test login mechanisms, encryption of sensitive data (e.g., payment information), secure transmission of data, authentication (e.g., two-factor authentication), and handling of user sessions. 13.** Network and Connectivity Testing** * What it is: Ensures the app performs well across different network conditions (e.g., Wi-Fi, 4G, 5G). * How it’s used in mobile testing: Test how the app behaves when switching between networks or under different connectivity scenarios like weak or lost signal, airplane mode, and reconnections. Regression Testing What it is: Ensures that recent code changes haven’t introduced new bugs or negatively impacted existing functionality. How it’s used in mobile testing: After bug fixes or feature updates, run automated or manual test cases to verify that previous functionality still works as expected. Gesture-Based Testing What it is: Tests mobile app interactions that rely on gestures like tap, swipe, pinch, zoom, or rotate. How it’s used in mobile testing: Validate the proper response of gestures on different devices and screen sizes (e.g., map zoom-in&#x2F;zoom-out, swiping between screens). Installation and Update Testing What it is: Ensures the application installs, updates, and uninstalls properly. How it’s used in mobile testing: Test app installation under different scenarios, such as a fresh install, app updates, rollbacks, and app removal. Validate data persistence after an update and check the impact on user preferences or data.","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"test_design","slug":"test-design","permalink":"https://ooge0.github.io/hexo-blog/tags/test-design/"},{"name":"mobile_application","slug":"mobile-application","permalink":"https://ooge0.github.io/hexo-blog/tags/mobile-application/"}]},{"title":"Basic loguru configuration for python","slug":"post_dev_side__loguru_loger_basic_configuration","date":"2023-10-25T13:21:12.000Z","updated":"2024-12-06T12:21:09.929Z","comments":true,"path":"2023/10/25/post_dev_side__loguru_loger_basic_configuration/","permalink":"https://ooge0.github.io/hexo-blog/2023/10/25/post_dev_side__loguru_loger_basic_configuration/","excerpt":"","text":"Below is a basic configuration for the loguru logging library. It includes logging to both the console and a file, with separate levels and formatting. Loguru details Loguru installation command via PIP 1pip install loguru Logugu PIP page Loguru home page Loguru configurationBelow is an example of ‘loguru’ configuration and usage. 1234567891011121314151617181920212223242526272829303132from loguru import loggerimport sys# Configure Logurulogger.remove() # Remove the default logger# Add a console loggerlogger.add( sink=sys.stdout, level=&quot;INFO&quot;, # Log level for console format=&quot;&lt;green&gt;&#123;time:YYYY-MM-DD HH:mm:ss&#125;&lt;/green&gt; | &lt;level&gt;&#123;level: &lt;8&#125;&lt;/level&gt; | &lt;cyan&gt;&#123;name&#125;&lt;/cyan&gt;:&lt;cyan&gt;&#123;function&#125;&lt;/cyan&gt;:&lt;cyan&gt;&#123;line&#125;&lt;/cyan&gt; - &lt;level&gt;&#123;message&#125;&lt;/level&gt;&quot;, enqueue=True, # Thread-safe)# Add a file loggerlogger.add( sink=&quot;app.log&quot;, level=&quot;DEBUG&quot;, # Log level for file format=&quot;&#123;time:YYYY-MM-DD HH:mm:ss&#125; | &#123;level: &lt;8&#125; | &#123;name&#125;:&#123;function&#125;:&#123;line&#125; - &#123;message&#125;&quot;, rotation=&quot;10 MB&quot;, # Rotate logs every 10 MB retention=&quot;7 days&quot;, # Retain logs for 7 days compression=&quot;zip&quot;, # Compress old log files enqueue=True, # Thread-safe)# Example usageif __name__ == &quot;__main__&quot;: logger.debug(&quot;This is a debug message.&quot;) logger.info(&quot;This is an info message.&quot;) logger.warning(&quot;This is a warning message.&quot;) logger.error(&quot;This is an error message.&quot;) logger.critical(&quot;This is a critical message.&quot;)","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"dev_side","slug":"dev-side","permalink":"https://ooge0.github.io/hexo-blog/tags/dev-side/"},{"name":"loguru","slug":"loguru","permalink":"https://ooge0.github.io/hexo-blog/tags/loguru/"},{"name":"logger","slug":"logger","permalink":"https://ooge0.github.io/hexo-blog/tags/logger/"}]},{"title":"Ubuntu web traffic meters","slug":"post_dev_side__ubuntu_web_trafic_meter","date":"2023-10-25T13:21:12.000Z","updated":"2024-11-21T22:41:12.738Z","comments":true,"path":"2023/10/25/post_dev_side__ubuntu_web_trafic_meter/","permalink":"https://ooge0.github.io/hexo-blog/2023/10/25/post_dev_side__ubuntu_web_trafic_meter/","excerpt":"","text":"On Ubuntu, several applications are available to control, monitor, and visualize network traffic and web activity for applications or services. Here are some of the top tools for these purposes: Get list of network devices by UNIX apps:ip 1ip link showifconfig 1ifconfig -a 1. Wireshark Description: Wireshark is a powerful, GUI-based network protocol analyzer that captures and displays detailed information about network traffic. It supports filtering, visualization, and analysis of network packets. Features: Packet filtering, protocol analysis, real-time capture, and color-coded visualizations. Installation: 1sudo apt update sudo apt install wireshark -y Usage: Launch with wireshark and select a network interface to start capturing packets. 2. nload Description: A command-line tool that provides a real-time visual representation of incoming and outgoing network traffic per interface. Features: Bandwidth monitoring with visual bars and traffic graphs. Installation: 1sudo apt update sudo apt install nload -y Usage: Run nload followed by the network interface name, for example: 1nload eth0 3. ntopng Description: ntopng is a web-based tool for monitoring network traffic. It provides detailed analytics on application usage, IP addresses, and protocol data. It also supports various filters. Features: Web-based UI, detailed traffic statistics, protocol analysis, and application-level monitoring. Installation: 1sudo apt update sudo apt install ntopng -y Usage: Access the web interface at http://localhost:3000. 4. iftop Description: A terminal-based tool for monitoring network traffic. It shows a list of connections with their bandwidth usage. Features: Sortable columns, bandwidth tracking, and customizable filters. Installation: 1sudo apt update sudo apt install iftop -y Usage: Run with sudo for full functionality: 1sudo iftop 5. Tcpdump Description: A powerful command-line packet analyzer that captures network traffic and saves it for analysis. Features: Highly customizable packet capture, supports saving data in pcap format for analysis in Wireshark. Installation: 1sudo apt update sudo apt install tcpdump -y &#96; Usage: For capturing HTTP traffic on a specific interface, for example: 1sudo tcpdump -i eth0 &#x27;port 80&#x27; Each of these tools offers different advantages depending on your needs, from basic bandwidth","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"dev_side","slug":"dev-side","permalink":"https://ooge0.github.io/hexo-blog/tags/dev-side/"},{"name":"API","slug":"API","permalink":"https://ooge0.github.io/hexo-blog/tags/API/"},{"name":"ubuntu","slug":"ubuntu","permalink":"https://ooge0.github.io/hexo-blog/tags/ubuntu/"}]},{"title":"UNIX command tutorials","slug":"post_dev_side__unix_comand_tutorial","date":"2023-10-25T13:21:12.000Z","updated":"2024-11-21T22:45:38.798Z","comments":true,"path":"2023/10/25/post_dev_side__unix_comand_tutorial/","permalink":"https://ooge0.github.io/hexo-blog/2023/10/25/post_dev_side__unix_comand_tutorial/","excerpt":"","text":"Below is a list of some popular UNIX command-line tools designed to help learn and practice UNIX commands: 1. bashcrawl: A game-like, interactive tutorial for learning UNIX commands inside the bash shell. It guides you through different directories and tasks, teaching basic UNIX commands in the process. Where to get it: bashcrawl GitHub Repository 2. vimtutor: While this tool specifically teaches the vim editor, it’s useful for learning command-line navigation and basics. Most UNIX systems come with vim, and running vimtutor provides an interactive way to learn. Command to run: vimtutor 3. explainshell: This is an online tool that helps explain UNIX commands by breaking down shell command syntax and arguments. You input a command, and it shows a description of each part of the command. Website: explainshell.com 4. tldr (Too Long Didn’t Read): A simplified and community-driven collection of UNIX command explanations. It provides short, practical examples for UNIX commands, unlike the often-detailed man pages. Install: sudo apt install tldr (on Ubuntu) or tldr GitHub Usage: tldr &lt;command&gt; 5. cheat A cheat-sheet tool that gives you concise examples and explanations of how to use UNIX commands. You can use it to quickly look up syntax and examples for common commands. Install: pip install cheat Usage: cheat &lt;command&gt; GitHub: cheat GitHub 6. cheatsheets This repository contains community-sourced cheatsheets to be used with cheat and similar applications. GitHub: cheatsheets Github","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"dev_side","slug":"dev-side","permalink":"https://ooge0.github.io/hexo-blog/tags/dev-side/"},{"name":"unix","slug":"unix","permalink":"https://ooge0.github.io/hexo-blog/tags/unix/"}]},{"title":"Python basics","slug":"post_dev_side__python_basics","date":"2023-06-12T18:42:05.000Z","updated":"2024-12-03T11:00:15.757Z","comments":true,"path":"2023/06/12/post_dev_side__python_basics/","permalink":"https://ooge0.github.io/hexo-blog/2023/06/12/post_dev_side__python_basics/","excerpt":"","text":"Python CorePython is a high-level, interpreted programming language known for its simplicity and versatility. It supports multiple programming paradigms, including procedural, object-oriented, and functional programming. Mastering Python’s core concepts is essential for building scalable applications. Python Basics | python.org Learn Python’s syntax, variables, and basic programming constructs. Input and Output in PythonPython provides various ways to take input from users and output information. The input() function is used for input, while print() is commonly used for output. Mastering I&#x2F;O is critical for creating interactive programs. Python Input and Output | geeksforgeeks.org Understand how to handle user inputs and format outputs. Python Data TypesPython supports several built-in data types like int, float, str, list, and more. These types form the foundation of Python’s dynamic nature. Python Data Types | geeksforgeeks.org Explore various data types and their uses in Python. Python Data Types | programiz.com Python Data StructuresData structures help organize and manipulate data efficiently. Python’s built-in data structures like list, dict, set, and tuple simplify data management. Python Data Structures | geeksforgeeks.org Learn to use Python’s powerful data structures for different scenarios. Python OperatorsOperators are special symbols that carry out operations on variables and values. Python supports arithmetic, logical, bitwise, and comparison operators. Python Operators | realpython.com Understand how to use operators for data manipulation. Conditional Statements in PythonConditional statements like if, elif, and else allow programs to make decisions. They are fundamental for implementing logic. Python Conditional Statements | w3schools.com Learn to control program flow using conditional statements. Related to this topic Chapter 5.1. Loops | python-book.softuni.org Chapter 7.1. Complex Loops | python-book.softuni.org Replacement Replacing Loops in Python Using Programmatic TricksReplacing traditional loops in Python with advanced techniques can enhance code readability and performance. Below are some key approaches to consider: 1. List Comprehensions List comprehensions provide a concise way to replace simple for loops, especially for creating or filtering lists. They are more readable and faster than traditional loops for straightforward transformations. Related to this topic Python List Comprehensions | realpython.com Mastering List Comprehensions | geeksforgeeks.org 2. Map, Filter, and Reduce Use functional programming tools like map for transformations, filter for conditional filtering, and reduce for aggregations. These methods eliminate the need for explicit loops and promote a functional programming style. Related to this topic Functional Programming in Python | realpython.com Python map(), filter(), and reduce() | programiz.com 3. Generators and Generator Expressions Replace loops with generators for memory-efficient processing of large datasets. Generator expressions can streamline code and work well with functions like sum() and any(). Related to this topic Generators in Python | realpython.com Python Generators | geeksforgeeks.org Python Itertools | geeksforgeeks.org 4. Recursion as a Loop Replacement Replace certain types of nested loops with recursion for problems like tree traversal. While not always efficient, recursion simplifies some complex loop structures. Related to this topic Recursion in Python | programiz.com Difference between Recursion and Iteration | geeksforgeeks.org 5. Vectorized Operations with NumPy Replace loops with vectorized operations in libraries like NumPy for mathematical or array computations. These operations are faster and more efficient than traditional looping mechanisms. Related to this topic NumPy Quick Start | numpy.org Understanding Vectorization (Sklearn + Pandas + NumPy) | towardsdatascience.com Understanding Vectorization in NumPy and Pandas | medium.com 6. Using Built-in Python Functions Replace loops with Python’s built-in functions like sum(), max(), any(), etc., to perform common operations. Related to this topic Built-in Functions in Python | docs.python.org Optimizing with Built-in Functions | realpython.com These techniques not only replace traditional loops but also align with Python’s philosophy of simplicity and elegance. Loops in Python - For, While and Nested LoopsLoops are used to execute a block of code repeatedly. Python supports for and while loops, along with nested loops for complex tasks. Python Loops | tutorialspoint.com Master loops for repetitive tasks and data iteration. Python FunctionsFunctions allow you to encapsulate code into reusable blocks. Python supports user-defined, lambda, and built-in functions. Python Functions | programiz.com Understand function declarations, arguments, and scope. Python OOPs ConceptsObject-oriented programming (OOP) in Python revolves around objects and classes. Key concepts include inheritance, encapsulation, and polymorphism. Python OOP | realpython.com Learn the principles of OOP and their implementation in Python. Python Functional ProgrammingFunctional programming treats computation as the evaluation of mathematical functions. Python’s support for map, reduce, and filter simplifies this paradigm. Python Functional Programming | medium.com Dive into functional programming with Python. Python ModulesModules in Python are files containing Python code that can define functions, classes, and variables. They promote code reuse. Python Modules | python.org Learn to import and use Python modules effectively. Python Exception HandlingException handling allows you to manage errors gracefully using try, except, finally, and raise. Python Exception Handling | geeksforgeeks.org Master exception handling to write robust code. Python PackagesPackages are collections of modules grouped together. They are used to organize related modules for larger applications. Python Packages | w3schools.com Explore how to create and manage Python packages. Python Collections ModulePython’s collections module provides specialized container data types like Counter, deque, OrderedDict, and defaultdict. These can simplify complex data operations. Python Collections Module | realpython.com Learn to use advanced data containers in Python. Python Projects - Beginner to AdvancedPython projects provide practical experience with coding concepts. Start with simple projects and gradually take on more advanced challenges. Python Project Ideas | geeksforgeeks.org Discover project ideas for every skill level. Python QuizQuizzes help reinforce your Python knowledge by testing your understanding of concepts. Python Quiz | w3schools.com Test your skills with interactive Python quizzes. Python Quiz | geeksforgeeks.org Python Quiz | pynative.com Python Quiz | codechef.com Python Quiz | evamariakiss.de","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"python","slug":"python","permalink":"https://ooge0.github.io/hexo-blog/tags/python/"},{"name":"dev_side","slug":"dev-side","permalink":"https://ooge0.github.io/hexo-blog/tags/dev-side/"}]},{"title":"Python libs","slug":"post_dev_side__python_libs","date":"2023-06-10T18:42:05.000Z","updated":"2024-12-12T10:43:40.441Z","comments":true,"path":"2023/06/10/post_dev_side__python_libs/","permalink":"https://ooge0.github.io/hexo-blog/2023/06/10/post_dev_side__python_libs/","excerpt":"","text":"Below is a list of Python packages, their documentation links, and the corresponding installation commands. The packages are arranged alphabetically. TOC allure-pytest cheat colorama faker graphviz hypothesis jsonschema loguru numpy openpyxl pandas pipdeptree pytest pytest-html pytest-lazy-fixture pytest-xdist pydocstyle regex requests scikit selenium sphinx tach tqdm typing_extensions allure-pytest pypi.org docs: https://pypi.org/project/allure-pytest/ related info: https://docs.qameta.io/allure/ 1pip install allure-pytest cheat pypi.org docs: https://pypi.org/project/cheat/ related info: https://github.com/cheat/cheat 1pip install cheat colorama pypi.org docs: https://pypi.org/project/colorama/ related info: https://github.com/tartley/colorama 1pip install colorama faker pypi.org docs: https://pypi.org/project/Faker/ related info: https://faker.readthedocs.io/ 1pip install faker graphviz pypi.org docs: https://pypi.org/project/graphviz/ related info: https://graphviz.org/ 1pip install graphviz hypothesis pypi.org docs: https://pypi.org/project/hypothesis/ related info: https://hypothesis.readthedocs.io/ 1pip install hypothesis jsonschema pypi.org docs: https://pypi.org/project/jsonschema/ related info: https://python-jsonschema.readthedocs.io/ 1pip install jsonschema loguru pypi.org docs: https://pypi.org/project/loguru/ related info: https://github.com/Delgan/loguru 1pip install loguru numpy pypi.org docs: https://pypi.org/project/numpy/ related info: https://numpy.org/ 1pip install numpy openpyxl pypi.org docs: https://pypi.org/project/openpyxl/ related info: https://openpyxl.readthedocs.io/ 1pip install openpyxl pandas pypi.org docs: https://pypi.org/project/pandas/ related info: https://pandas.pydata.org/ 1pip install pandas pipdeptree pypi.org docs: https://pypi.org/project/pipdeptree/ related info: https://github.com/naiquevin/pipdeptree 1pip install pipdeptree pytest pypi.org docs: https://pypi.org/project/pytest/ related info: https://docs.pytest.org/en/latest/ 1pip install pytest pytest-html pypi.org docs: https://pypi.org/project/pytest-html/ related info: https://pytest-html.readthedocs.io/ 1pip install pytest-html pytest-lazy-fixture pypi.org docs: https://pypi.org/project/pytest-lazy-fixture/ related info: https://github.com/tvorog/pytest-lazy-fixture 1pip install pytest-lazy-fixture pytest-xdist pypi.org docs: https://pypi.org/project/pytest-xdist/ related info: https://pytest-xdist.readthedocs.io/ 1pip install pytest-xdist pydocstyle pypi.org docs: https://pypi.org/project/pydocstyle/ related info: https://github.com/PyCQA/pydocstyle 1pip install pydocstyle regex pypi.org docs: https://pypi.org/project/regex/ related info: https://github.com/mrabarnett/mrab-regex 1pip install regex requests pypi.org docs: https://pypi.org/project/requests/ related info: https://docs.python-requests.org/ 1pip install requests scikit pypi.org docs: https://pypi.org/project/scikit-learn/ related info: https://scikit-learn.org/ 1pip install scikit-learn selenium pypi.org docs: https://pypi.org/project/selenium/ related info: https://www.selenium.dev/ 1pip install selenium sphinx pypi.org docs: https://pypi.org/project/Sphinx/ related info: https://www.sphinx-doc.org/ 1pip install sphinx tach pypi.org docs: https://pypi.org/project/tach/ related info: https://github.com/gauge-sh/tach 1pip install tach tqdm pypi.org docs: https://pypi.org/project/tqdm/ related info: https://tqdm.github.io/ 1pip install tqdm typing_extensions pypi.org docs: https://pypi.org/project/typing-extensions/ related info: https://docs.python.org/3/library/typing.html 1pip install typing_extensions","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"python","slug":"python","permalink":"https://ooge0.github.io/hexo-blog/tags/python/"},{"name":"dev_side","slug":"dev-side","permalink":"https://ooge0.github.io/hexo-blog/tags/dev-side/"}]},{"title":"Postman tests examples","slug":"post_dev_side__postman_tests_examples","date":"2023-05-11T14:32:05.000Z","updated":"2024-11-21T22:34:51.410Z","comments":true,"path":"2023/05/11/post_dev_side__postman_tests_examples/","permalink":"https://ooge0.github.io/hexo-blog/2023/05/11/post_dev_side__postman_tests_examples/","excerpt":"","text":"Below are examples of Postman tests written with Chai BDD&#x2F;TDD assertion library. Table of content#1 Validate that the response body contains a specific#2 Check if the response status is 200#3 Ensure the response time is less than 500ms#4 Verify the user’s first name in the JSON response#5 Validate that the Content-Type header is application&#x2F;json#6 Ensure that the JSON array is not empty#7 Verify that a specific key exists in the response body#8 Validate the response body includes the string ‘success’#9 Check if a specific field is of a certain data type#10. Validate that a specific field has a value greater than zero#11. Ensure that a specific header exists (X-Request-ID)#12. Verify that the response body does not contain a specific string#13. Check if the response body contains a specific key-value pair()(email)#14. Ensure the response contains multiple keys#15. Verify that a field’s value is within a specific range#16. Validate that the response body contains a specific substring#17. Check if a numeric field in the response body is even#18. Ensure that the response body has a non-empty string field#19. Verify that a boolean field in the response body is true#20. Validate that an array field has a specific length#21. Check if a date field in the response body is in the past#22. Ensure that a response header does not exist#23. Verify that a specific field contains a valid email address#24. Validate that a field’s value matches a specific pattern#25. Check if the response body has no null values in an array#26. Ensure the response includes a specific nested object#27. Verify that a list of IDs in the response body is unique#28. Validate that a numeric field falls within a certain percentile #29. Check if a timestamp in the response body is in ISO 8601 format#31. Verify that a specific URL in the response body is reachable#32. Validate that a key’s value is within a specific set of allowed()values#33. Check if the response contains a nested array with at least one()object#34. Ensure that the sum of a numeric array field equals a specific()value#35. Verify that the response body does not contain any undefined()fields#36. Validate that all objects in an array have a specific key#37. Check if a field’s value is a valid JSON object#38. Ensure the response body does not exceed a certain length#39. Verify that the response body does not contain a specific key#40. Validate that an array field in the response body is sorted 1. Validate that the response body contains a specific email123pm.test(&quot;validating responsebody contains specific email&quot;, function () &#123; pm.expect(pm.response.text()).to.include(&quot;george.bluth@magic.com&quot;);&#125;); 2. Check if the response status is 200123pm.test(&quot;validating response status is 200&quot;, function () &#123; pm.response.to.have.status(200);&#125;); 3. Ensure the response time is less than 500ms123pm.test(&quot;validating response time is less than 500ms&quot;, function () &#123; pm.expect(pm.response.responseTime).to.be.below(500);&#125;); 4. Verify the user’s first name in the JSON response1234pm.test(&quot;validating user&#x27;s first name in JSON response&quot;, function () &#123; var jsonData = pm.response.json(); pm.expect(jsonData.data.first_name).to.eql(&quot;George&quot;);&#125;); 5. Validate that the Content-Type header is application&#x2F;Json123pm.test(&quot;validating content-type header is application/json&quot;, function () &#123; pm.expect(pm.response.headers.get(&quot;Content-Type&quot;)).to.eql(&quot;application/json&quot;);&#125;); 6. Ensure that the JSON array is not empty1234pm.test(&quot;validating JSON array is not empty&quot;, function () &#123; var jsonData = pm.response.json(); pm.expect(jsonData.data).to.be.an(&#x27;array&#x27;).that.is.not.empty;&#125;); 7. Verify that a specific key exists in the response body1234pm.test(&quot;validating key existence in response body&quot;, function () &#123; var jsonData = pm.response.json(); pm.expect(jsonData).to.have.property(&quot;data&quot;);&#125;); 8.Validate the response body includes the string ‘success’123pm.test(&quot;validating response body includes the string &#x27;success&#x27;&quot;, function () &#123; pm.expect(pm.response.text()).to.include(&quot;success&quot;);&#125;); 9. Check if a specific field is of a certain data type1234pm.test(&quot;validating field data type&quot;, function () &#123; var jsonData = pm.response.json(); pm.expect(jsonData.data.id).to.be.a(&#x27;number&#x27;);&#125;); 10. Validate that a specific field has a value greater than zero1234pm.test(&quot;validating field value is greater than zero&quot;, function () &#123; var jsonData = pm.response.json(); pm.expect(jsonData.data.age).to.be.above(0);&#125;); 11. Ensure that a specific header exists (X-Request-ID)123pm.test(&quot;validating header existence X-Request-ID&quot;, function () &#123; pm.expect(pm.response.headers.has(&quot;X-Request-ID&quot;)).to.be.true;&#125;); 12. Verify that the response body does not contain a specific string123pm.test(&quot;validating absence of string &#x27;error&#x27; in response body&quot;, function () &#123; pm.expect(pm.response.text()).to.not.include(&quot;error&quot;);&#125;); 13. Check if the response body contains a specific key-value pair1234pm.test(&quot;validating key-value pair (email)&quot;, function () &#123; var jsonData = pm.response.json(); pm.expect(jsonData.data.email).to.eql(&quot;george.bluth@magic.com&quot;);&#125;); 14. Ensure the response contains multiple keys1234pm.test(&quot;validating multiple keys existence (data, support)&quot;, function () &#123; var jsonData = pm.response.json(); pm.expect(jsonData).to.have.keys([&quot;data&quot;, &quot;support&quot;]);&#125;); 15. Verify that a field’s value is within a specific range1234pm.test(&quot;validating field value is within range 1-10&quot;, function () &#123; var jsonData = pm.response.json(); pm.expect(jsonData.data.page).to.be.within(1, 10);&#125;); 16. Validate that the response body contains a specific substring123pm.test(&quot;validating response body contains substring &#x27;welcome&#x27;&quot;, function () &#123; pm.expect(pm.response.text()).to.include(&quot;welcome&quot;);&#125;); 17. Check if a numeric field in the response body is even1234pm.test(&quot;validating numeric field is even&quot;, function () &#123; var jsonData = pm.response.json(); pm.expect(jsonData.data.id % 2).to.eql(0);&#125;); 18. Ensure that the response body has a non-empty string field1234pm.test(&quot;validating non-empty string field&quot;, function () &#123; var jsonData = pm.response.json(); pm.expect(jsonData.data.name).to.have.length.above(0);&#125;); 19. Verify that a boolean field in the response body is true1234pm.test(&quot;validating boolean field is true&quot;, function () &#123; var jsonData = pm.response.json(); pm.expect(jsonData.data.isActive).to.be.true;&#125;); 20. Validate that an array field has a specific length1234pm.test(&quot;validating array length is 5&quot;, function () &#123; var jsonData = pm.response.json(); pm.expect(jsonData.data.items).to.have.lengthOf(5);&#125;); 21. Check if a date field in the response body is in the past1234pm.test(&quot;validating date field is in the past&quot;, function () &#123; var jsonData = pm.response.json(); pm.expect(new Date(jsonData.data.date)).to.be.below(new Date());&#125;); 22. Ensure that a response header does not exist123pm.test(&quot;validating header does not exist X-Deprecated&quot;, function () &#123; pm.expect(pm.response.headers.has(&quot;X-Deprecated&quot;)).to.be.false;&#125;); 23. Verify that a specific field contains a valid email address1234pm.test(&quot;validating valid email address&quot;, function () &#123; var jsonData = pm.response.json(); pm.expect(jsonData.data.email).to.match(/\\S+@\\S+\\.\\S+/);&#125;); 24. Validate that a field’s value matches a specific pattern1234pm.test(&quot;validating field matches phone number pattern&quot;, function () &#123; var jsonData = pm.response.json(); pm.expect(jsonData.data.phone).to.match(/^\\d&#123;3&#125;-\\d&#123;3&#125;-\\d&#123;4&#125;$/);&#125;); 25. Check if the response body has no null values in an array1234pm.test(&quot;validating no null values in array&quot;, function () &#123; var jsonData = pm.response.json(); pm.expect(jsonData.data).to.not.deep.include.members([null]);&#125;); 26. Ensure the response includes a specific nested object1234pm.test(&quot;validating nested object existence (address)&quot;, function () &#123; var jsonData = pm.response.json(); pm.expect(jsonData.data.address).to.have.property(&quot;city&quot;);&#125;); 27. Verify that a list of IDs in the response body is unique12345pm.test(&quot;validating unique IDs in list&quot;, function () &#123; var jsonData = pm.response.json(); var ids = jsonData.data.map(item =&gt; item.id); pm.expect(new Set(ids).size).to.eql(ids.length);&#125;); 28. Validate that a numeric field falls within a certain percentile1234pm.test(&quot;validating field is in top 10 percentile&quot;, function () &#123; var jsonData = pm.response.json(); pm.expect(jsonData.data.score).to.be.above(90);&#125;); 29. Check if a timestamp in the response body is in ISO 8601 format:1234pm.test(&quot;validating timestamp format is ISO 8601&quot;, function () &#123; var jsonData = pm.response.json(); pm.expect(jsonData.data.timestamp).to.match(/\\d&#123;4&#125;-\\d&#123;2&#125;-\\d&#123;2&#125;T\\d&#123;2&#125;:\\d&#123;2&#125;:\\d&#123;2&#125;.\\d&#123;3&#125;Z/);&#125;); 30. Ensure that a specific field’s value does not contain special characters:1234pm.test(&quot;validating no special characters in field&quot;, function () &#123; var jsonData = pm.response.json(); pm.expect(jsonData.data.username).to.match(/^[a-zA-Z0-9]+$/);&#125;); 31. Verify that a specific URL in the response body is reachable:123pm.test(&quot;validating URL reachability&quot;, function () &#123; pm.expect(pm.response.text()).to.match(/(http|https):\\/\\/\\S+/g);&#125;); 32. Validate that a key’s value is within a specific set of allowed values:1234pm.test(&quot;validating key value in allowed set&quot;, function () &#123; var jsonData = pm.response.json(); pm.expect(jsonData.data.status).to.be.oneOf([&quot;Pending&quot;, &quot;Completed&quot;, &quot;Canceled&quot;]);&#125;); 33. Check if the response contains a nested array with at least one object:1234pm.test(&quot;validating nested array contains at least one object&quot;, function () &#123; var jsonData = pm.response.json(); pm.expect(jsonData.data.orders).to.have.length.above(0);&#125;); 34. Ensure that the sum of a numeric array field equals a specific value:12345pm.test(&quot;validating sum of array values equals 100&quot;, function () &#123; var jsonData = pm.response.json(); var sum = jsonData.data.prices.reduce((acc, price) =&gt; acc + price, 0); pm.expect(sum).to.eql(100);&#125;); 35. Verify that the response body does not contain any undefined fields:12345pm.test(&quot;validating no undefined fields&quot;, function () &#123; var jsonData = pm.response.json(); var hasUndefined = Object.values(jsonData.data).includes(undefined); pm.expect(hasUndefined).to.be.false;&#125;); 36. Validate that all objects in an array have a specific key:1234pm.test(&quot;validating key &#x27;id&#x27; in all array objects&quot;, function () &#123; var jsonData = pm.response.json(); pm.expect(jsonData.data).to.satisfy(items =&gt; items.every(item =&gt; item.hasOwnProperty(&quot;id&quot;)));&#125;); 37. Check if a field’s value is a valid JSON object:1234pm.test(&quot;validating field is a valid JSON object&quot;, function () &#123; var jsonData = pm.response.json(); pm.expect(jsonData.data.config).to.be.an(&#x27;object&#x27;);&#125;); 38. Ensure the response body does not exceed a certain length:123pm.test(&quot;validating response body length is at most 500&quot;, function () &#123; pm.expect(pm.response.text().length).to.be.at.most(500);&#125;); 39. Verify that the response body does not contain a specific key:1234pm.test(&quot;validating absence of key &#x27;password&#x27;&quot;, function () &#123; var jsonData = pm.response.json(); pm.expect(jsonData).to.not.have.property(&quot;password&quot;);&#125;); 40. Validate that an array field in the response body is sorted:12345pm.test(&quot;validating array is sorted&quot;, function () &#123; var jsonData = pm.response.json(); var sortedArray = [...jsonData.data].sort((a, b) =&gt; a - b); pm.expect(sortedArray).to.eql(jsonData.data);&#125;);","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"dev_side","slug":"dev-side","permalink":"https://ooge0.github.io/hexo-blog/tags/dev-side/"},{"name":"postman","slug":"postman","permalink":"https://ooge0.github.io/hexo-blog/tags/postman/"},{"name":"api_testing","slug":"api-testing","permalink":"https://ooge0.github.io/hexo-blog/tags/api-testing/"},{"name":"qa","slug":"qa","permalink":"https://ooge0.github.io/hexo-blog/tags/qa/"},{"name":"API","slug":"API","permalink":"https://ooge0.github.io/hexo-blog/tags/API/"}]},{"title":"Kafka intro","slug":"post_dev_side__kafka_intro","date":"2023-01-02T09:52:05.000Z","updated":"2024-12-06T08:52:29.578Z","comments":true,"path":"2023/01/02/post_dev_side__kafka_intro/","permalink":"https://ooge0.github.io/hexo-blog/2023/01/02/post_dev_side__kafka_intro/","excerpt":"","text":"Understanding Kafka: A Message Broker OverviewMessage brokers play a critical role in distributed systems by enabling communication between different applications and services. Kafka, a leading distributed event streaming platform, has become a cornerstone for building scalable and fault-tolerant systems. 1. What is Kafka?Kafka is an open-source distributed event streaming platform designed for high-throughput and low-latency processing. Originally developed by LinkedIn, Kafka is now maintained by the Apache Software Foundation. It enables the storage, processing, and replays of event streams in real time. Key components include: Producers: Applications or services that send data to Kafka topics. Consumers: Applications or services that read data from Kafka topics. Brokers: Kafka servers that store data and serve client requests. Topics: Categories or feeds to which records are sent and stored. Kafka’s architecture ensures scalability and resilience, making it a preferred choice for modern data pipelines and streaming analytics. 2. Supported ProtocolsWhile Kafka uses its proprietary Kafka Protocol, it also supports integrations with other messaging standards. Common protocols include: AMQP (Advanced Message Queuing Protocol): Primarily used by traditional message brokers like RabbitMQ, Kafka connects through connectors for compatibility. HTTP&#x2F;REST: Tools like Kafka REST Proxy enable HTTP-based communication for producing and consuming messages. gRPC: Can be integrated for client-server communication in microservices. These protocols enhance Kafka’s flexibility, allowing seamless interaction with diverse ecosystems. 3. UI Interfaces for KafkaSeveral tools provide user-friendly interfaces to manage and monitor Kafka clusters: Confluent Control Center: A comprehensive UI for Kafka monitoring and management. Kafdrop: A lightweight, open-source web UI for browsing Kafka topics, consumers, and brokers. Lens(lenses.io): A developer-friendly interface focused on debugging Kafka streams. AKHQ: A modern UI for topic management, consumer group monitoring, and ACL administration. These interfaces simplify Kafka management tasks for developers and administrators. 4. Implementations in Different Programming LanguagesKafka’s robust client libraries and APIs enable seamless integration across popular programming languages: Java: The official Kafka client library (org.apache.kafka) supports full functionality. Python: Libraries like kafka-python and confluent-kafka-python are widely used. C#: The Confluent.Kafka NuGet package provides a high-performance Kafka client. Node.js: Libraries like kafka-node and kafkajs enable JavaScript-based applications to integrate with Kafka. Go: sarama and confluent-kafka-go are popular libraries for Go developers. These libraries support producing, consuming, and managing Kafka messages efficiently. 5. Testing Kafka in QATesting Kafka can be challenging due to its asynchronous nature and real-time processing. QA teams can consider the following approaches: Unit Tests: Mock Kafka producers&#x2F;consumers using libraries like MockKafka (Java) or pytest-kafka (Python). Integration Tests: Use in-memory Kafka clusters (e.g., EmbeddedKafka for Java) to simulate real scenarios. Performance Testing: Tools like Apache JMeter and Gatling support Kafka-specific plugins for load testing. Consumer Lag Monitoring: Verify that consumers process messages without significant delays using metrics. QA teams should focus on data consistency, scalability, and latency metrics during testing. 6. Kafka vs. Azure Service Bus Feature Kafka Azure Service Bus Type Distributed Event Streaming Platform Cloud-Based Message Broker Protocols Kafka Protocol, REST, AMQP AMQP, HTTPS Persistence Durable by default Optional, based on queues&#x2F;topics Scaling Horizontal scaling with partitions Auto-scaling in Azure environment Use Case High-throughput, real-time data streams Enterprise workflows, hybrid setups Setup Self-managed or Confluent Cloud Fully managed by Azure Latency Sub-millisecond (with tuning) Typically higher 7. Footnotes Apache Kafka Documentation: Explore Kafka’s official documentation for detailed insights. Confluent Kafka: Learn more about managed Kafka solutions at Confluent.io. Comparison Details: Azure Service Bus documentation can be found here. Kafka Testing Tips: Check out MockKafka for Java testing here. Multimedia book about Kafka (Cartoon story) : https://www.gentlydownthe.stream/ My personal project for checkin basic Kafka features. Impletentation in Python. GitHub repo: https://github.com/ooge0/kafka-app-demo Web post: Apache Kafka Architecture. https://www.javatpoint.com/apache-kafka-architecture Deploy Kafka UI tool | Medium Fix for issue: Running Kafka on Windows 10 fails: The system cannot find the path specified Book: Mastering Kafka Streams and ksqlDB: Building Real-Time Data Systems by Example Mitch Seymour. Read on coursesidekick.com","categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"}],"tags":[{"name":"dev_side","slug":"dev-side","permalink":"https://ooge0.github.io/hexo-blog/tags/dev-side/"},{"name":"kafka","slug":"kafka","permalink":"https://ooge0.github.io/hexo-blog/tags/kafka/"}]},{"title":"Other papers","slug":"notes/other_papers","date":"2010-10-10T08:11:11.000Z","updated":"2024-11-20T19:12:45.983Z","comments":true,"path":"2010/10/10/notes/other_papers/","permalink":"https://ooge0.github.io/hexo-blog/2010/10/10/notes/other_papers/","excerpt":"","text":"Economics Confessions of an Economic Hit Man | John Perkins Psychology Методика семантического дифференциала. Психосемантический подход к исследованию индивидуального сознания. Т.Е. Косаревская, Р.Р. Кутькина. Philosophy: Введение в неклассическую философию.Гаспарян Д.Э. Введение в политическую философию. Гаджиев К.С. Political science Политология Гаджиев К.С. ! Глава 15. ОСНОВНЫЕ ТЕЧЕНИЯ ИДЕЙНО-ПОЛИТИЧЕСКОЙ МЫСЛИ Social and public policy A Brief History of the Future. Jacques Attali. Allen &amp; Unwin, 2009. 291 pages Краткая история будущего. Мир в ближайшие 50 лет. Жак Аттали. Blockchain Blockchain Networks:Token Design and Management Overview.Loïc Lesavre. Priam Varin. Dylan Yaga. 2021 DOI: https://doi.org/10.6028/NIST.IR.8301 Read on nvlpubs.nist.gov","categories":[{"name":"Notes","slug":"Notes","permalink":"https://ooge0.github.io/hexo-blog/categories/Notes/"}],"tags":[{"name":"papers","slug":"papers","permalink":"https://ooge0.github.io/hexo-blog/tags/papers/"},{"name":"tutorial","slug":"tutorial","permalink":"https://ooge0.github.io/hexo-blog/tags/tutorial/"},{"name":"my_contribution","slug":"my-contribution","permalink":"https://ooge0.github.io/hexo-blog/tags/my-contribution/"}]},{"title":"Favorite books","slug":"notes/favorite_books","date":"2010-10-09T21:00:00.000Z","updated":"2024-11-14T07:24:59.831Z","comments":true,"path":"2010/10/10/notes/favorite_books/","permalink":"https://ooge0.github.io/hexo-blog/2010/10/10/notes/favorite_books/","excerpt":"","text":"IMPORTANT!!! All information that you can find here is published as is.This section contains list of my favorite books and some list of resources where you can get them in pdf&#x2F;epub&#x2F;fb2 formats.Feel free to contact me if you have any concerns. Books on GitHubMy GitHub repository with books","categories":[{"name":"Notes","slug":"Notes","permalink":"https://ooge0.github.io/hexo-blog/categories/Notes/"}],"tags":[{"name":"tutorial","slug":"tutorial","permalink":"https://ooge0.github.io/hexo-blog/tags/tutorial/"},{"name":"my_contribution","slug":"my-contribution","permalink":"https://ooge0.github.io/hexo-blog/tags/my-contribution/"},{"name":"books","slug":"books","permalink":"https://ooge0.github.io/hexo-blog/tags/books/"}]},{"title":"Info data","slug":"notes/info_data","date":"2010-10-09T21:00:00.000Z","updated":"2024-10-31T13:13:30.918Z","comments":true,"path":"2010/10/10/notes/info_data/","permalink":"https://ooge0.github.io/hexo-blog/2010/10/10/notes/info_data/","excerpt":"","text":"ventusky.com Satellite observations Sumy (UA) Kyiv (UA) Poltava (UA) Sofia (BG) Solar Flares and Storms Warnings for Today Forecast platforms meteoagent.com - SOLAR FLARES & STORMS FORECAST ventusky.com - Weather forecast foreca.com foreca.com | Ukraine, Sumy foreca.com | Ukraine, Kyiv swe.ssa.esa.int - Space weather dashboard from the SWE Network / European space agency. spaceweatherlive.com","categories":[{"name":"Notes","slug":"Notes","permalink":"https://ooge0.github.io/hexo-blog/categories/Notes/"}],"tags":[{"name":"info","slug":"info","permalink":"https://ooge0.github.io/hexo-blog/tags/info/"}]},{"title":"Favorite movies","slug":"notes/vaforite_movies","date":"2010-10-09T21:00:00.000Z","updated":"2024-12-04T09:22:51.630Z","comments":true,"path":"2010/10/10/notes/vaforite_movies/","permalink":"https://ooge0.github.io/hexo-blog/2010/10/10/notes/vaforite_movies/","excerpt":"","text":"Favorite movies list TakedownWarGamesTronHackersThe-Net footnotes Takedown Initial reliese: 2000 IMDB: https://www.imdb.com/title/tt0159784/ About: Based on the story of the capture of computer hacker ‘Kevin Mitnick’. However, when breaks into the computer of a security expert and an ex-hacker, he’s in for a shock and has more than he can handle. Based on: The book “Takeodwn” by Tsutomu Shimomura1 with John Markoff2 WarGames Initial release: 1983 IMDB: https://www.imdb.com/title/tt0086567/ About: A young computer whiz kid accidentally connects to a U.S. military supercomputer while searching for new video games. He unknowingly initiates a nuclear war simulation, sparking a real-world crisis. Starring: Matthew Broderick, Ally Sheedy Director: John Badham Tron Initial release: 1982 IMDB: https://www.imdb.com/title/tt0084827/ About: A computer programmer is transported inside a computer where he battles a malevolent software program. One of the earliest films to explore digital realms and hacker culture in a stylized setting. Starring: Jeff Bridges, Bruce Boxleitner Director: Steven Lisberger Hackers Initial release: 1995 IMDB: https://www.imdb.com/title/tt0113243/ About: A group of teenage hackers stumbles upon a massive corporate fraud conspiracy while trying to outwit an older, sinister hacker. Known for its vibrant visual style and soundtrack. Starring: Angelina Jolie, Jonny Lee Miller Director: Iain Softley Sneakers Initial release: 1992 IMDB: https://www.imdb.com/title/tt0105435/ About: A team of security system experts is blackmailed into stealing a top-secret decoder, but soon realize they are being used by the government. This thriller features an all-star cast and explores themes of surveillance and encryption. Starring: Robert Redford, Dan Aykroyd, Sidney Poitier Director: Phil Alden Robinson The Net Initial release: 1995 IMDB: https://www.imdb.com/title/tt0113957/ About: A computer programmer becomes entangled in a conspiracy after her identity is stolen. The film was notable for addressing online identity theft and digital security during the early days of the internet. Starring: Sandra Bullock Director: Irwin Winkler Electric Dreams Initial release: 1984 IMDB: https://www.imdb.com/title/tt0087197/ About: A love triangle develops between a man, a woman, and a computer. The film blends romance and sci-fi, illustrating the rising influence of computers in everyday life. Starring: Lenny von Dohlen, Virginia Madsen Director: Steve Barron The Lawnmower Man Initial release: 1992 IMDB: https://www.imdb.com/title/tt0104692/ About: A scientist conducts virtual reality experiments to enhance the intelligence of a simple gardener. The film explores the risks of technological enhancement and the ethical boundaries of science. Starring: Jeff Fahey, Pierce Brosnan Director: Brett Leonard Ghost in the Shell Initial release: 1995 IMDB: https://www.imdb.com/title/tt0113568/ About: In a futuristic society, a cyborg cop hunts a mysterious hacker known as the Puppet Master. This Japanese animated film influenced cyberpunk aesthetics and explores themes of identity and consciousness. Director: Mamoru Oshii Footnotes: 1.Tsutomu Shimomura. More deails about Tsutomu Shimomura you can find in BIO ↩2.John Markoff is a journalist best known for his work covering technology at The New York Times for 28 years until his retirement in 2016, and a book and series of articles about the 1990s pursuit and capture of hacker Kevin Mitnick. ↩","categories":[{"name":"Notes","slug":"Notes","permalink":"https://ooge0.github.io/hexo-blog/categories/Notes/"}],"tags":[{"name":"tutorial","slug":"tutorial","permalink":"https://ooge0.github.io/hexo-blog/tags/tutorial/"},{"name":"my_contribution","slug":"my-contribution","permalink":"https://ooge0.github.io/hexo-blog/tags/my-contribution/"},{"name":"movie","slug":"movie","permalink":"https://ooge0.github.io/hexo-blog/tags/movie/"}]}],"categories":[{"name":"Posts","slug":"Posts","permalink":"https://ooge0.github.io/hexo-blog/categories/Posts/"},{"name":"Notes","slug":"Notes","permalink":"https://ooge0.github.io/hexo-blog/categories/Notes/"},{"name":"Maintanance","slug":"Maintanance","permalink":"https://ooge0.github.io/hexo-blog/categories/Maintanance/"},{"name":"Tutorials","slug":"Tutorials","permalink":"https://ooge0.github.io/hexo-blog/categories/Tutorials/"},{"name":"Categories","slug":"Categories","permalink":"https://ooge0.github.io/hexo-blog/categories/Categories/"},{"name":"QA","slug":"QA","permalink":"https://ooge0.github.io/hexo-blog/categories/QA/"},{"name":"My contribution","slug":"My-contribution","permalink":"https://ooge0.github.io/hexo-blog/categories/My-contribution/"},{"name":"Personal information","slug":"Personal-information","permalink":"https://ooge0.github.io/hexo-blog/categories/Personal-information/"}],"tags":[{"name":"apps","slug":"apps","permalink":"https://ooge0.github.io/hexo-blog/tags/apps/"},{"name":"windows_os","slug":"windows-os","permalink":"https://ooge0.github.io/hexo-blog/tags/windows-os/"},{"name":"NLP","slug":"NLP","permalink":"https://ooge0.github.io/hexo-blog/tags/NLP/"},{"name":"linguistic","slug":"linguistic","permalink":"https://ooge0.github.io/hexo-blog/tags/linguistic/"},{"name":"ML","slug":"ML","permalink":"https://ooge0.github.io/hexo-blog/tags/ML/"},{"name":"AI","slug":"AI","permalink":"https://ooge0.github.io/hexo-blog/tags/AI/"},{"name":"NN","slug":"NN","permalink":"https://ooge0.github.io/hexo-blog/tags/NN/"},{"name":"LLM","slug":"LLM","permalink":"https://ooge0.github.io/hexo-blog/tags/LLM/"},{"name":"BART","slug":"BART","permalink":"https://ooge0.github.io/hexo-blog/tags/BART/"},{"name":"BERT","slug":"BERT","permalink":"https://ooge0.github.io/hexo-blog/tags/BERT/"},{"name":"GPT2","slug":"GPT2","permalink":"https://ooge0.github.io/hexo-blog/tags/GPT2/"},{"name":"NLTK","slug":"NLTK","permalink":"https://ooge0.github.io/hexo-blog/tags/NLTK/"},{"name":"NPF","slug":"NPF","permalink":"https://ooge0.github.io/hexo-blog/tags/NPF/"},{"name":"data_mining","slug":"data-mining","permalink":"https://ooge0.github.io/hexo-blog/tags/data-mining/"},{"name":"parsing","slug":"parsing","permalink":"https://ooge0.github.io/hexo-blog/tags/parsing/"},{"name":"python","slug":"python","permalink":"https://ooge0.github.io/hexo-blog/tags/python/"},{"name":"VADER","slug":"VADER","permalink":"https://ooge0.github.io/hexo-blog/tags/VADER/"},{"name":"sentiment_analysis","slug":"sentiment-analysis","permalink":"https://ooge0.github.io/hexo-blog/tags/sentiment-analysis/"},{"name":"lexic","slug":"lexic","permalink":"https://ooge0.github.io/hexo-blog/tags/lexic/"},{"name":"emotions","slug":"emotions","permalink":"https://ooge0.github.io/hexo-blog/tags/emotions/"},{"name":"prompt_engineering","slug":"prompt-engineering","permalink":"https://ooge0.github.io/hexo-blog/tags/prompt-engineering/"},{"name":"web","slug":"web","permalink":"https://ooge0.github.io/hexo-blog/tags/web/"},{"name":"Dashdevs","slug":"Dashdevs","permalink":"https://ooge0.github.io/hexo-blog/tags/Dashdevs/"},{"name":"text_classification","slug":"text-classification","permalink":"https://ooge0.github.io/hexo-blog/tags/text-classification/"},{"name":"nst","slug":"nst","permalink":"https://ooge0.github.io/hexo-blog/tags/nst/"},{"name":"glossary","slug":"glossary","permalink":"https://ooge0.github.io/hexo-blog/tags/glossary/"},{"name":"my_contribution","slug":"my-contribution","permalink":"https://ooge0.github.io/hexo-blog/tags/my-contribution/"},{"name":"tools","slug":"tools","permalink":"https://ooge0.github.io/hexo-blog/tags/tools/"},{"name":"online_tools","slug":"online-tools","permalink":"https://ooge0.github.io/hexo-blog/tags/online-tools/"},{"name":"html","slug":"html","permalink":"https://ooge0.github.io/hexo-blog/tags/html/"},{"name":"tutorial","slug":"tutorial","permalink":"https://ooge0.github.io/hexo-blog/tags/tutorial/"},{"name":"science","slug":"science","permalink":"https://ooge0.github.io/hexo-blog/tags/science/"},{"name":"knowledge","slug":"knowledge","permalink":"https://ooge0.github.io/hexo-blog/tags/knowledge/"},{"name":"blog","slug":"blog","permalink":"https://ooge0.github.io/hexo-blog/tags/blog/"},{"name":"hexo_io","slug":"hexo-io","permalink":"https://ooge0.github.io/hexo-blog/tags/hexo-io/"},{"name":"usb","slug":"usb","permalink":"https://ooge0.github.io/hexo-blog/tags/usb/"},{"name":"cmd","slug":"cmd","permalink":"https://ooge0.github.io/hexo-blog/tags/cmd/"},{"name":"unicode","slug":"unicode","permalink":"https://ooge0.github.io/hexo-blog/tags/unicode/"},{"name":"ASCII","slug":"ASCII","permalink":"https://ooge0.github.io/hexo-blog/tags/ASCII/"},{"name":"LaTeX","slug":"LaTeX","permalink":"https://ooge0.github.io/hexo-blog/tags/LaTeX/"},{"name":"acsii","slug":"acsii","permalink":"https://ooge0.github.io/hexo-blog/tags/acsii/"},{"name":"chatbot","slug":"chatbot","permalink":"https://ooge0.github.io/hexo-blog/tags/chatbot/"},{"name":"Llama","slug":"Llama","permalink":"https://ooge0.github.io/hexo-blog/tags/Llama/"},{"name":"global_knowledge","slug":"global-knowledge","permalink":"https://ooge0.github.io/hexo-blog/tags/global-knowledge/"},{"name":"resources","slug":"resources","permalink":"https://ooge0.github.io/hexo-blog/tags/resources/"},{"name":"statistics","slug":"statistics","permalink":"https://ooge0.github.io/hexo-blog/tags/statistics/"},{"name":"cheatsheets","slug":"cheatsheets","permalink":"https://ooge0.github.io/hexo-blog/tags/cheatsheets/"},{"name":"tags","slug":"tags","permalink":"https://ooge0.github.io/hexo-blog/tags/tags/"},{"name":"qa","slug":"qa","permalink":"https://ooge0.github.io/hexo-blog/tags/qa/"},{"name":"db","slug":"db","permalink":"https://ooge0.github.io/hexo-blog/tags/db/"},{"name":"note","slug":"note","permalink":"https://ooge0.github.io/hexo-blog/tags/note/"},{"name":"md_format","slug":"md-format","permalink":"https://ooge0.github.io/hexo-blog/tags/md-format/"},{"name":"my_medium_post","slug":"my-medium-post","permalink":"https://ooge0.github.io/hexo-blog/tags/my-medium-post/"},{"name":"my_linkedin_post","slug":"my-linkedin-post","permalink":"https://ooge0.github.io/hexo-blog/tags/my-linkedin-post/"},{"name":"test_tasks","slug":"test-tasks","permalink":"https://ooge0.github.io/hexo-blog/tags/test-tasks/"},{"name":"my_porjects","slug":"my-porjects","permalink":"https://ooge0.github.io/hexo-blog/tags/my-porjects/"},{"name":"examples","slug":"examples","permalink":"https://ooge0.github.io/hexo-blog/tags/examples/"},{"name":"markdown","slug":"markdown","permalink":"https://ooge0.github.io/hexo-blog/tags/markdown/"},{"name":"papers","slug":"papers","permalink":"https://ooge0.github.io/hexo-blog/tags/papers/"},{"name":"ReadTheDocs","slug":"ReadTheDocs","permalink":"https://ooge0.github.io/hexo-blog/tags/ReadTheDocs/"},{"name":"[object Object]","slug":"object-Object","permalink":"https://ooge0.github.io/hexo-blog/tags/object-Object/"},{"name":"DocPublishing","slug":"DocPublishing","permalink":"https://ooge0.github.io/hexo-blog/tags/DocPublishing/"},{"name":"about_me","slug":"about-me","permalink":"https://ooge0.github.io/hexo-blog/tags/about-me/"},{"name":"ux_ui","slug":"ux-ui","permalink":"https://ooge0.github.io/hexo-blog/tags/ux-ui/"},{"name":"qa_check_list","slug":"qa-check-list","permalink":"https://ooge0.github.io/hexo-blog/tags/qa-check-list/"},{"name":"augmentation","slug":"augmentation","permalink":"https://ooge0.github.io/hexo-blog/tags/augmentation/"},{"name":"text_generation","slug":"text-generation","permalink":"https://ooge0.github.io/hexo-blog/tags/text-generation/"},{"name":"network_protocols","slug":"network-protocols","permalink":"https://ooge0.github.io/hexo-blog/tags/network-protocols/"},{"name":"protocols","slug":"protocols","permalink":"https://ooge0.github.io/hexo-blog/tags/protocols/"},{"name":"test_design","slug":"test-design","permalink":"https://ooge0.github.io/hexo-blog/tags/test-design/"},{"name":"mobile_application","slug":"mobile-application","permalink":"https://ooge0.github.io/hexo-blog/tags/mobile-application/"},{"name":"dev_side","slug":"dev-side","permalink":"https://ooge0.github.io/hexo-blog/tags/dev-side/"},{"name":"loguru","slug":"loguru","permalink":"https://ooge0.github.io/hexo-blog/tags/loguru/"},{"name":"logger","slug":"logger","permalink":"https://ooge0.github.io/hexo-blog/tags/logger/"},{"name":"API","slug":"API","permalink":"https://ooge0.github.io/hexo-blog/tags/API/"},{"name":"ubuntu","slug":"ubuntu","permalink":"https://ooge0.github.io/hexo-blog/tags/ubuntu/"},{"name":"unix","slug":"unix","permalink":"https://ooge0.github.io/hexo-blog/tags/unix/"},{"name":"postman","slug":"postman","permalink":"https://ooge0.github.io/hexo-blog/tags/postman/"},{"name":"api_testing","slug":"api-testing","permalink":"https://ooge0.github.io/hexo-blog/tags/api-testing/"},{"name":"kafka","slug":"kafka","permalink":"https://ooge0.github.io/hexo-blog/tags/kafka/"},{"name":"books","slug":"books","permalink":"https://ooge0.github.io/hexo-blog/tags/books/"},{"name":"info","slug":"info","permalink":"https://ooge0.github.io/hexo-blog/tags/info/"},{"name":"movie","slug":"movie","permalink":"https://ooge0.github.io/hexo-blog/tags/movie/"}]}