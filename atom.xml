<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title> </title>
  
  <subtitle>...chasing dreams, living reality</subtitle>
  <link href="https://ooge0.github.io/hexo-blog/atom.xml" rel="self"/>
  
  <link href="https://ooge0.github.io/hexo-blog/"/>
  <updated>2024-11-30T15:18:33.089Z</updated>
  <id>https://ooge0.github.io/hexo-blog/</id>
  
  <author>
    <name>si0n4ra</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Neural Process Family</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/30/post_ai_ml_npf__general/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/30/post_ai_ml_npf__general/</id>
    <published>2024-11-30T14:45:33.000Z</published>
    <updated>2024-11-30T15:18:33.089Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>The <strong>Neural Process Family</strong> refers to a class of models designed to learn distributions over functions, offering a blend of the expressiveness of deep learning and the flexibility of probabilistic models. These models are particularly useful for tasks requiring uncertainty quantification, few-shot learning, and function estimation. Key members of this family include <strong>Neural Processes (NPs)</strong>, <strong>Conditional Neural Processes (CNPs)</strong>, <strong>Attentive Neural Processes (ANPs)</strong>, and extensions like <strong>ConvCNPs</strong> and <strong>Variational NPs (VNPs)</strong>.</p><h2 id="Core-Concepts"><a href="#Core-Concepts" class="headerlink" title="Core Concepts"></a>Core Concepts</h2><ol><li><p><strong>Probabilistic Nature</strong>:<br>Neural Processes learn distributions over functions. Given a set of input-output pairs, they can predict the probability distribution of outputs for new inputs, making them suitable for uncertainty estimation.</p></li><li><p><strong>Few-Shot Learning</strong>:<br>These models can make predictions given only a few examples, making them ideal for problems where data is scarce.</p></li><li><p><strong>Model Components</strong>:</p><ul><li><strong>Encoder</strong>: Maps input-output pairs to a latent representation.</li><li><strong>Decoder</strong>: Takes the latent representation and generates outputs for given inputs.</li><li><strong>Latent Space</strong>: Captures the uncertainty and variability in the function space.</li></ul></li><li><p><strong>Meta-Learning</strong>:<br>NPs can generalize across tasks by learning a distribution over tasks, enabling them to perform well on unseen tasks after being trained on related ones.</p></li></ol><hr><h2 id="Variants-of-Neural-Processes"><a href="#Variants-of-Neural-Processes" class="headerlink" title="Variants of Neural Processes"></a>Variants of Neural Processes</h2><ol><li><p><strong>Conditional Neural Processes (CNPs)</strong>:</p><ul><li>A deterministic model that learns to map a context set of input-output pairs to predictions for new inputs.</li><li>Simple and efficient but limited in capturing uncertainty in the underlying function.</li></ul></li><li><p><strong>Neural Processes (NPs)</strong>:</p><ul><li>Adds a latent variable to model uncertainty explicitly, making it a probabilistic counterpart to CNPs.</li><li>Balances flexibility and computational efficiency.</li></ul></li><li><p><strong>Attentive Neural Processes (ANPs)</strong>:</p><ul><li>Introduces attention mechanisms to improve modeling of relationships between context points and query points.</li><li>Addresses issues with poor extrapolation and oversmoothing in standard NPs.</li></ul></li><li><p><strong>Convolutional Neural Processes (ConvCNPs)</strong>:</p><ul><li>Leverages convolutional architectures for tasks like image generation, capturing local correlations more effectively.</li></ul></li><li><p><strong>Variational Neural Processes (VNPs)</strong>:</p><ul><li>Focuses on improved variational inference techniques to better approximate the posterior distribution over functions.</li></ul></li></ol><hr><h2 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h2><ol><li><strong>Regression</strong>:<br>Modeling functions with uncertainty, e.g., Bayesian regression tasks.</li><li><strong>Few-Shot Classification</strong>:<br>Classifying data with limited examples by modeling task distributions.</li><li><strong>Spatio-Temporal Data</strong>:<br>Applications in time-series forecasting and spatial predictions.</li><li><strong>Reinforcement Learning</strong>:<br>Modeling uncertainty in reward functions or dynamics.</li><li><strong>Image Completion</strong>:<br>Predicting missing pixels in images.</li></ol><hr><h2 id="Strengths-and-Challenges"><a href="#Strengths-and-Challenges" class="headerlink" title="Strengths and Challenges"></a>Strengths and Challenges</h2><h3 id="Strengths"><a href="#Strengths" class="headerlink" title="Strengths:"></a>Strengths:</h3><ul><li>Scalability due to neural networks.</li><li>Probabilistic outputs allow uncertainty estimation.</li><li>Adaptable across domains with minimal changes.</li></ul><h3 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges:"></a>Challenges:</h3><ul><li>Trade-off between computational cost and flexibility.</li><li>Dependence on good representation learning.</li><li>Overcoming limitations of context aggregation in high-dimensional tasks.</li></ul><hr><p>The Neural Process Family continues to evolve, with active research aimed at improving its scalability, expressiveness, and applications to real-world problems.</p><hr><p>About:</p><ul><li><a href="https://yanndubs.github.io/Neural-Process-Family/">yanndubs.github.io | The Neural Process Family</a></li><li><a href="https://notes.theomorales.com/Gaussian+%26+Neural+Processes/The+Neural+Process+Family">notes.theomorales.com | The Neural Process Family</a></li></ul><hr><p> Papers: </p><ul><li>Papers: <ul><li>Title: The Neural Process Family: Survey, Applications<br>and Perspectives</li><li>DOI: 10.48550&#x2F;arXiv.2209.00517</li><li><a href="https://arxiv.org/pdf/2209.00517">Read on arxiv.org</a></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;The &lt;strong&gt;Neural Process Family&lt;/str</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="AI" scheme="https://ooge0.github.io/hexo-blog/tags/AI/"/>
    
    <category term="ML" scheme="https://ooge0.github.io/hexo-blog/tags/ML/"/>
    
    <category term="NPF" scheme="https://ooge0.github.io/hexo-blog/tags/NPF/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning.Teach by Doing(LinkedIn post)</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/30/post_ai_ml__machine_learning_teach_by_doing/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/30/post_ai_ml__machine_learning_teach_by_doing/</id>
    <published>2024-11-30T14:33:33.000Z</published>
    <updated>2024-11-30T14:38:34.347Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><em>This is reopst of LinkedIN post. Current post contains list of references and some additional detilas</em></p><p>I(Author) started the Machine Learning: Teach by Doing series to transfer my learnings to those who want to transition to Machine Learning.</p><p>I(Author) have recorded 37 videos in the past 6 months.</p><p>Here are the links for you to learn:</p><ol><li>Introduction to Machine Learning Teach by Doing: <a href="https://lnkd.in/gqN2PMX5">https://lnkd.in/gqN2PMX5</a></li><li>What is Machine Learning? History of Machine Learning: <a href="https://lnkd.in/gvpNSAKh">https://lnkd.in/gvpNSAKh</a></li><li>Types of ML Models: <a href="https://lnkd.in/gSy2mChM">https://lnkd.in/gSy2mChM</a></li><li>6 steps of any ML project: <a href="https://lnkd.in/ggCGchPQ">https://lnkd.in/ggCGchPQ</a></li><li>Install Python and VSCode and run your first code: <a href="https://lnkd.in/gyic7J7b">https://lnkd.in/gyic7J7b</a></li><li>Linear Classifiers Part 1: <a href="https://lnkd.in/gYdfD97D">https://lnkd.in/gYdfD97D</a></li><li>Linear Classifiers Part 2: <a href="https://lnkd.in/gac_z-G8">https://lnkd.in/gac_z-G8</a></li><li>Jupyter Notebook, Numpy and Scikit-Learn: <a href="https://lnkd.in/gWRaC_tB">https://lnkd.in/gWRaC_tB</a></li><li>Running the Random Linear Classifier Algorithm in Python: <a href="https://lnkd.in/g5HacbFC">https://lnkd.in/g5HacbFC</a></li><li>The oldest ML model - Perceptron: <a href="https://lnkd.in/gpce6uFt">https://lnkd.in/gpce6uFt</a></li><li>Coding the Perceptron: <a href="https://lnkd.in/gmz-XjNK">https://lnkd.in/gmz-XjNK</a></li><li>Perceptron Convergence Theorem: <a href="https://lnkd.in/gmz-XjNK">https://lnkd.in/gmz-XjNK</a></li><li>Magic of features in Machine Learning: <a href="https://lnkd.in/gCeDRb3g">https://lnkd.in/gCeDRb3g</a></li><li>One hot encoding: <a href="https://lnkd.in/g3WfRQGQ">https://lnkd.in/g3WfRQGQ</a></li><li>Logistic Regression Part 1: <a href="https://lnkd.in/gTgZAAZn">https://lnkd.in/gTgZAAZn</a></li><li>Cross Entropy Loss: <a href="https://lnkd.in/g3Ywg_2p">https://lnkd.in/g3Ywg_2p</a></li><li>How gradient descent works: <a href="https://lnkd.in/gKBAsazF">https://lnkd.in/gKBAsazF</a></li><li>Logistic Regression from scratch in Python: <a href="https://lnkd.in/g8iZh27P">https://lnkd.in/g8iZh27P</a></li><li>Introduction to Regularization: <a href="https://lnkd.in/gjM9pVw2">https://lnkd.in/gjM9pVw2</a></li><li>Implementing Regularization in Python: <a href="https://lnkd.in/gRnSK4v4">https://lnkd.in/gRnSK4v4</a></li><li>Linear Regression Introduction: <a href="https://lnkd.in/gPYtSPJ9">https://lnkd.in/gPYtSPJ9</a></li><li>Ordinary Least Squares step by step implementation: <a href="https://lnkd.in/gnWQdgNy">https://lnkd.in/gnWQdgNy</a></li><li>Ridge regression fundamentals and intuition: <a href="https://lnkd.in/gE5M-CSM">https://lnkd.in/gE5M-CSM</a></li><li>Regression recap for interviews: <a href="https://lnkd.in/gNBWzzWv">https://lnkd.in/gNBWzzWv</a></li><li>Neural network architecture in 30 minutes: <a href="https://lnkd.in/g7qSrkxG">https://lnkd.in/g7qSrkxG</a></li><li>Backpropagation intuition: <a href="https://lnkd.in/gAmBARHm">https://lnkd.in/gAmBARHm</a></li><li>Neural network activation functions: <a href="https://lnkd.in/gqrC3zDP">https://lnkd.in/gqrC3zDP</a></li><li>Momentum in gradient descent: <a href="https://lnkd.in/g3M4qhbP">https://lnkd.in/g3M4qhbP</a></li><li>Hands on neural network training in Python: <a href="https://lnkd.in/gz-fTBxs">https://lnkd.in/gz-fTBxs</a></li><li>Introduction to Convolutional Neural Networks (CNNs.: <a href="https://lnkd.in/gpmuBm3j">https://lnkd.in/gpmuBm3j</a></li><li>Filters in 1D and the Convolution Operation: <a href="https://lnkd.in/gEDaKHDU">https://lnkd.in/gEDaKHDU</a></li><li>Filters in 2D, Channels and Feature Identification: <a href="https://lnkd.in/g3Gf_4ia">https://lnkd.in/g3Gf_4ia</a></li><li>Filtering Layers in Convolutional Neural Networks: <a href="https://lnkd.in/gUaiBkTu">https://lnkd.in/gUaiBkTu</a></li><li>What is Max Pooling in Convolutional Neural Networks?: <a href="https://lnkd.in/gGRGy6wq">https://lnkd.in/gGRGy6wq</a></li><li>CNN Architecture explained: <a href="https://lnkd.in/gPQvRh9i">https://lnkd.in/gPQvRh9i</a></li><li>Backpropagation in Convolutional Neural Networks: <a href="https://lnkd.in/g942G6zv">https://lnkd.in/g942G6zv</a></li><li>Build your own brain tumor classification CNN application in Python: <a href="https://lnkd.in/gQB5zRGk">https://lnkd.in/gQB5zRGk</a></li></ol><p>Join our AI live lectures waitlist here: <a href="https://lnkd.in/gDcHZdHg">https://lnkd.in/gDcHZdHg</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;&lt;em&gt;This is reopst of LinkedIN post. C</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="AI" scheme="https://ooge0.github.io/hexo-blog/tags/AI/"/>
    
    <category term="ML" scheme="https://ooge0.github.io/hexo-blog/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Parsing web site for job offers</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/28/post_data_mining__parsing_web_site_for_job_offers/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/28/post_data_mining__parsing_web_site_for_job_offers/</id>
    <published>2024-11-27T22:49:11.000Z</published>
    <updated>2024-11-27T23:20:32.939Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><strong>Given</strong> The website with posted offers.<br><strong>Goal:</strong> to get information from the website using python, BeautifulSoup and save it in JSON and markdown files.</p><p><strong>Python scirpt</strong></p><p>Install and import required packages</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">execute_requests</span>(<span class="params">base_url, amount_of_pages</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Executes GET requests for the specified number of pages and returns the responses.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        base_url (str): The base URL for requests.</span></span><br><span class="line"><span class="string">        amount_of_pages (int): The number of pages to fetch.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        list: A list of dictionaries containing the request number, page counter, and response content.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    payload = &#123;&#125;</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;Cookie&#x27;</span>: <span class="string">&#x27;_jobboard_session=895b7b35b6493519c3ad686923d8cc1d; __cf_bm=BrUISN3QZXOQ1BPeJXOvNpqBAcT8YXS6XqIr7jlW.4M-1732742386-1.0.1.1-1hk8BgPrEcPEO6ZLFO6W6W6e8MNhcmQswlF6K2dUhchBp0px3reiDJPOuXnzX6z.etyzZq.Q9BIUEYJ1dHZqP75g&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    responses_data = []  <span class="comment"># Initialize an empty list to store response data</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> counter <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, amount_of_pages + <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># Construct the URL with the current page counter</span></span><br><span class="line">        url = <span class="string">f&quot;<span class="subst">&#123;base_url&#125;</span>&amp;page=<span class="subst">&#123;counter&#125;</span>&quot;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Fetching data from: <span class="subst">&#123;url&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment"># Send GET request</span></span><br><span class="line">            response = requests.get(url, headers=headers, data=payload)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Append the response data to the list</span></span><br><span class="line">            responses_data.append(&#123;</span><br><span class="line">                <span class="string">&quot;request_key&quot;</span>: <span class="string">f&quot;request_<span class="subst">&#123;counter&#125;</span>&quot;</span>,</span><br><span class="line">                <span class="string">&quot;counter&quot;</span>: counter,</span><br><span class="line">                <span class="string">&quot;response_content&quot;</span>: response.text</span><br><span class="line">            &#125;)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;counter: <span class="subst">&#123;counter&#125;</span> | &#x27;status_code:&#x27; <span class="subst">&#123;response.status_code&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">except</span> requests.RequestException <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Error fetching data for page <span class="subst">&#123;counter&#125;</span>: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> responses_data</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Parse data from <code>json</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_job_data_from_json</span>(<span class="params">response_data, output_json_file, output_markdown_file</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Parse job data from a list of responses and extract job listings using BeautifulSoup.</span></span><br><span class="line"><span class="string">    Save results to both a JSON file and a Markdown file.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        response_data (list): List of dictionaries containing the response data.</span></span><br><span class="line"><span class="string">        output_json_file (str): Path to save the parsed job data in JSON format.</span></span><br><span class="line"><span class="string">        output_markdown_file (str): Path to save the parsed job data in Markdown format.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        job_data = []  <span class="comment"># List to store extracted job data</span></span><br><span class="line">        markdown_content = []  <span class="comment"># List to store Markdown entries</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Loop through each request in the list</span></span><br><span class="line">        <span class="keyword">for</span> request <span class="keyword">in</span> response_data:</span><br><span class="line">            counter = request.get(<span class="string">&quot;counter&quot;</span>, <span class="string">&quot;unknown&quot;</span>)</span><br><span class="line">            response_content = request.get(<span class="string">&quot;response_content&quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Parse the HTML content using BeautifulSoup</span></span><br><span class="line">            soup = BeautifulSoup(response_content, <span class="string">&#x27;html.parser&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Find all job listings using the locator</span></span><br><span class="line">            job_listings = soup.find_all(<span class="string">&#x27;li&#x27;</span>, class_=<span class="string">&#x27;job-listing&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Extract data from each job listing</span></span><br><span class="line">            <span class="keyword">for</span> job <span class="keyword">in</span> job_listings:</span><br><span class="line">                <span class="comment"># Safely find required elements, fallback to &#x27;N/A&#x27; if not present</span></span><br><span class="line">                job_title = job.find(<span class="string">&#x27;a&#x27;</span>, class_=<span class="string">&#x27;jobList-title zip-backfill-link&#x27;</span>)</span><br><span class="line">                job_description = job.find(<span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&#x27;jobList-description&#x27;</span>)</span><br><span class="line">                salary = job.find(<span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&#x27;jobList-salary&#x27;</span>)</span><br><span class="line"></span><br><span class="line">                job_info = &#123;</span><br><span class="line">                    <span class="string">&#x27;title&#x27;</span>: job_title.text.strip() <span class="keyword">if</span> job_title <span class="keyword">else</span> <span class="string">&#x27;N/A&#x27;</span>,</span><br><span class="line">                    <span class="string">&#x27;href&#x27;</span>: job_title[<span class="string">&#x27;href&#x27;</span>] <span class="keyword">if</span> job_title <span class="keyword">else</span> <span class="string">&#x27;N/A&#x27;</span>,</span><br><span class="line">                    <span class="string">&#x27;description&#x27;</span>: job_description.text.strip() <span class="keyword">if</span> job_description <span class="keyword">else</span> <span class="string">&#x27;N/A&#x27;</span>,</span><br><span class="line">                    <span class="string">&#x27;salary&#x27;</span>: salary.text.strip() <span class="keyword">if</span> salary <span class="keyword">else</span> <span class="string">&#x27;N/A&#x27;</span>,</span><br><span class="line">                    <span class="string">&#x27;page&#x27;</span>: counter</span><br><span class="line">                &#125;</span><br><span class="line">                job_data.append(job_info)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Prepare entry for Markdown</span></span><br><span class="line">                markdown_entry = <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">### Job Title: <span class="subst">&#123;job_info[<span class="string">&#x27;title&#x27;</span>]&#125;</span></span></span><br><span class="line"><span class="string">- **Link**: [<span class="subst">&#123;job_info[<span class="string">&#x27;title&#x27;</span>]&#125;</span>](<span class="subst">&#123;job_info[<span class="string">&#x27;href&#x27;</span>]&#125;</span>)</span></span><br><span class="line"><span class="string">- **Description**: <span class="subst">&#123;job_info[<span class="string">&#x27;description&#x27;</span>]&#125;</span></span></span><br><span class="line"><span class="string">- **Salary**: <span class="subst">&#123;job_info[<span class="string">&#x27;salary&#x27;</span>]&#125;</span></span></span><br><span class="line"><span class="string">- **Page**: <span class="subst">&#123;job_info[<span class="string">&#x27;page&#x27;</span>]&#125;</span></span></span><br><span class="line"><span class="string">                &quot;&quot;&quot;</span></span><br><span class="line">                markdown_content.append(markdown_entry.strip())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Save extracted job data to a new JSON file</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(output_json_file, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            json.dump(job_data, f, ensure_ascii=<span class="literal">False</span>, indent=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Job data successfully parsed and saved to <span class="subst">&#123;output_json_file&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Save Markdown content to a file</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(output_markdown_file, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(<span class="string">&quot;\n\n&quot;</span>.join(markdown_content))</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Job data successfully saved to <span class="subst">&#123;output_markdown_file&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Error processing file: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>IMPORTANT !</strong></p><ul><li>Provide valid references for saving retrieved data.</li><li>Make sure that you copied valid url from the browser and manage pagination properly.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    base_url = <span class="string">&quot;https://www.ziprecruiter.co.uk/jobs/search?l=Remote&amp;q=qa+software+engineer&amp;remote=full&quot;</span></span><br><span class="line">    amount_of_pages = <span class="number">100</span>  <span class="comment"># Or any number that you wish to check</span></span><br><span class="line">    responses_data = execute_requests(base_url, amount_of_pages)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Output files for parsed data</span></span><br><span class="line">    output_json_file = <span class="string">&#x27;parsed_job_data.json&#x27;</span></span><br><span class="line">    output_markdown_file = <span class="string">&#x27;parsed_job_data.md&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Parse and save the job data</span></span><br><span class="line">    parse_job_data_from_json(responses_data, output_json_file, output_markdown_file)</span><br></pre></td></tr></table></figure><p>Script works fine for several executions. After that cookies expired and new one should be regenerated.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; The website wit</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="data_mining" scheme="https://ooge0.github.io/hexo-blog/tags/data-mining/"/>
    
    <category term="parsing" scheme="https://ooge0.github.io/hexo-blog/tags/parsing/"/>
    
    <category term="python" scheme="https://ooge0.github.io/hexo-blog/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Parsing web site for job offers</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/28/Posts/post_data_mining__parsing_web_site_for_job_offers/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/28/Posts/post_data_mining__parsing_web_site_for_job_offers/</id>
    <published>2024-11-27T22:49:11.000Z</published>
    <updated>2024-11-27T23:20:32.939Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><strong>Given</strong> The website with posted offers.<br><strong>Goal:</strong> to get information from the website using python, BeautifulSoup and save it in JSON and markdown files.</p><p><strong>Python scirpt</strong></p><p>Install and import required packages</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">execute_requests</span>(<span class="params">base_url, amount_of_pages</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Executes GET requests for the specified number of pages and returns the responses.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        base_url (str): The base URL for requests.</span></span><br><span class="line"><span class="string">        amount_of_pages (int): The number of pages to fetch.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        list: A list of dictionaries containing the request number, page counter, and response content.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    payload = &#123;&#125;</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;Cookie&#x27;</span>: <span class="string">&#x27;_jobboard_session=895b7b35b6493519c3ad686923d8cc1d; __cf_bm=BrUISN3QZXOQ1BPeJXOvNpqBAcT8YXS6XqIr7jlW.4M-1732742386-1.0.1.1-1hk8BgPrEcPEO6ZLFO6W6W6e8MNhcmQswlF6K2dUhchBp0px3reiDJPOuXnzX6z.etyzZq.Q9BIUEYJ1dHZqP75g&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    responses_data = []  <span class="comment"># Initialize an empty list to store response data</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> counter <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, amount_of_pages + <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># Construct the URL with the current page counter</span></span><br><span class="line">        url = <span class="string">f&quot;<span class="subst">&#123;base_url&#125;</span>&amp;page=<span class="subst">&#123;counter&#125;</span>&quot;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Fetching data from: <span class="subst">&#123;url&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment"># Send GET request</span></span><br><span class="line">            response = requests.get(url, headers=headers, data=payload)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Append the response data to the list</span></span><br><span class="line">            responses_data.append(&#123;</span><br><span class="line">                <span class="string">&quot;request_key&quot;</span>: <span class="string">f&quot;request_<span class="subst">&#123;counter&#125;</span>&quot;</span>,</span><br><span class="line">                <span class="string">&quot;counter&quot;</span>: counter,</span><br><span class="line">                <span class="string">&quot;response_content&quot;</span>: response.text</span><br><span class="line">            &#125;)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;counter: <span class="subst">&#123;counter&#125;</span> | &#x27;status_code:&#x27; <span class="subst">&#123;response.status_code&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">except</span> requests.RequestException <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Error fetching data for page <span class="subst">&#123;counter&#125;</span>: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> responses_data</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Parse data from <code>json</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_job_data_from_json</span>(<span class="params">response_data, output_json_file, output_markdown_file</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Parse job data from a list of responses and extract job listings using BeautifulSoup.</span></span><br><span class="line"><span class="string">    Save results to both a JSON file and a Markdown file.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        response_data (list): List of dictionaries containing the response data.</span></span><br><span class="line"><span class="string">        output_json_file (str): Path to save the parsed job data in JSON format.</span></span><br><span class="line"><span class="string">        output_markdown_file (str): Path to save the parsed job data in Markdown format.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        job_data = []  <span class="comment"># List to store extracted job data</span></span><br><span class="line">        markdown_content = []  <span class="comment"># List to store Markdown entries</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Loop through each request in the list</span></span><br><span class="line">        <span class="keyword">for</span> request <span class="keyword">in</span> response_data:</span><br><span class="line">            counter = request.get(<span class="string">&quot;counter&quot;</span>, <span class="string">&quot;unknown&quot;</span>)</span><br><span class="line">            response_content = request.get(<span class="string">&quot;response_content&quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Parse the HTML content using BeautifulSoup</span></span><br><span class="line">            soup = BeautifulSoup(response_content, <span class="string">&#x27;html.parser&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Find all job listings using the locator</span></span><br><span class="line">            job_listings = soup.find_all(<span class="string">&#x27;li&#x27;</span>, class_=<span class="string">&#x27;job-listing&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Extract data from each job listing</span></span><br><span class="line">            <span class="keyword">for</span> job <span class="keyword">in</span> job_listings:</span><br><span class="line">                <span class="comment"># Safely find required elements, fallback to &#x27;N/A&#x27; if not present</span></span><br><span class="line">                job_title = job.find(<span class="string">&#x27;a&#x27;</span>, class_=<span class="string">&#x27;jobList-title zip-backfill-link&#x27;</span>)</span><br><span class="line">                job_description = job.find(<span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&#x27;jobList-description&#x27;</span>)</span><br><span class="line">                salary = job.find(<span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&#x27;jobList-salary&#x27;</span>)</span><br><span class="line"></span><br><span class="line">                job_info = &#123;</span><br><span class="line">                    <span class="string">&#x27;title&#x27;</span>: job_title.text.strip() <span class="keyword">if</span> job_title <span class="keyword">else</span> <span class="string">&#x27;N/A&#x27;</span>,</span><br><span class="line">                    <span class="string">&#x27;href&#x27;</span>: job_title[<span class="string">&#x27;href&#x27;</span>] <span class="keyword">if</span> job_title <span class="keyword">else</span> <span class="string">&#x27;N/A&#x27;</span>,</span><br><span class="line">                    <span class="string">&#x27;description&#x27;</span>: job_description.text.strip() <span class="keyword">if</span> job_description <span class="keyword">else</span> <span class="string">&#x27;N/A&#x27;</span>,</span><br><span class="line">                    <span class="string">&#x27;salary&#x27;</span>: salary.text.strip() <span class="keyword">if</span> salary <span class="keyword">else</span> <span class="string">&#x27;N/A&#x27;</span>,</span><br><span class="line">                    <span class="string">&#x27;page&#x27;</span>: counter</span><br><span class="line">                &#125;</span><br><span class="line">                job_data.append(job_info)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Prepare entry for Markdown</span></span><br><span class="line">                markdown_entry = <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">### Job Title: <span class="subst">&#123;job_info[<span class="string">&#x27;title&#x27;</span>]&#125;</span></span></span><br><span class="line"><span class="string">- **Link**: [<span class="subst">&#123;job_info[<span class="string">&#x27;title&#x27;</span>]&#125;</span>](<span class="subst">&#123;job_info[<span class="string">&#x27;href&#x27;</span>]&#125;</span>)</span></span><br><span class="line"><span class="string">- **Description**: <span class="subst">&#123;job_info[<span class="string">&#x27;description&#x27;</span>]&#125;</span></span></span><br><span class="line"><span class="string">- **Salary**: <span class="subst">&#123;job_info[<span class="string">&#x27;salary&#x27;</span>]&#125;</span></span></span><br><span class="line"><span class="string">- **Page**: <span class="subst">&#123;job_info[<span class="string">&#x27;page&#x27;</span>]&#125;</span></span></span><br><span class="line"><span class="string">                &quot;&quot;&quot;</span></span><br><span class="line">                markdown_content.append(markdown_entry.strip())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Save extracted job data to a new JSON file</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(output_json_file, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            json.dump(job_data, f, ensure_ascii=<span class="literal">False</span>, indent=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Job data successfully parsed and saved to <span class="subst">&#123;output_json_file&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Save Markdown content to a file</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(output_markdown_file, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(<span class="string">&quot;\n\n&quot;</span>.join(markdown_content))</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Job data successfully saved to <span class="subst">&#123;output_markdown_file&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Error processing file: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>IMPORTANT !</strong></p><ul><li>Provide valid references for saving retrieved data.</li><li>Make sure that you copied valid url from the browser and manage pagination properly.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    base_url = <span class="string">&quot;https://www.ziprecruiter.co.uk/jobs/search?l=Remote&amp;q=qa+software+engineer&amp;remote=full&quot;</span></span><br><span class="line">    amount_of_pages = <span class="number">100</span>  <span class="comment"># Or any number that you wish to check</span></span><br><span class="line">    responses_data = execute_requests(base_url, amount_of_pages)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Output files for parsed data</span></span><br><span class="line">    output_json_file = <span class="string">&#x27;parsed_job_data.json&#x27;</span></span><br><span class="line">    output_markdown_file = <span class="string">&#x27;parsed_job_data.md&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Parse and save the job data</span></span><br><span class="line">    parse_job_data_from_json(responses_data, output_json_file, output_markdown_file)</span><br></pre></td></tr></table></figure><p>Script works fine for several executions. After that cookies expired and new one should be regenerated.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; The website wit</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="data_mining" scheme="https://ooge0.github.io/hexo-blog/tags/data-mining/"/>
    
    <category term="parsing" scheme="https://ooge0.github.io/hexo-blog/tags/parsing/"/>
    
    <category term="python" scheme="https://ooge0.github.io/hexo-blog/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>VADER - intro</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/26/post_ai__vader_intro/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/26/post_ai__vader_intro/</id>
    <published>2024-11-26T19:37:30.000Z</published>
    <updated>2024-11-26T20:01:47.805Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="VADER"><a href="#VADER" class="headerlink" title="VADER"></a>VADER</h2><p>VADER (Valence Aware Dictionary and sEntiment Reasoner) is a widely used sentiment analysis tool tailored for understanding emotions in text, especially in social media contexts. Developed by C.J. Hutto and Eric Gilbert, it combines lexicon-based methods with grammatical and syntactical rules, offering precise sentiment analysis. VADER excels at capturing sentiment intensity, polarity (positive, negative, neutral), and even nuances like sarcasm, thanks to empirically validated linguistic rules and datasets.</p><p>Originally presented at the Eighth International Conference on Weblogs and Social Media in 2014, VADER was designed for scalability and ease of use. Its open-source implementation in Python is accessible for various applications, from marketing analysis to social media monitoring. The tool incorporates features like emoticons, slang, and acronyms, making it uniquely adept at analyzing informal text. Users can install it via Python’s pip command, and the source code is freely available under the MIT License.</p><p>The tool has been validated rigorously with human raters to ensure accuracy. Datasets like tweets, movie reviews, and editorial snippets were used for its development, enabling a robust understanding of diverse text formats.</p><p>For official details, you can visit VADER’s documentation: <a href="https://vadersentiment.readthedocs.io/en/latest/">VADER Sentiment</a>.</p><h3 id="Releated-resources"><a href="#Releated-resources" class="headerlink" title="Releated resources"></a>Releated resources</h3><ul><li>Medium post : <a href="https://towardsdatascience.com/an-short-introduction-to-vader-3f3860208d53">A Short Introduction to VADER</a></li><li>Paper: VADER: A Parsimonious Rule-Based Model for Sentiment Analysis of Social Media Text. 2014<ul><li>DOI: 10.1609&#x2F;icwsm.v8i1.14550</li><li><a href="https://ojs.aaai.org/index.php/ICWSM/article/view/14550">Read on ojs.aaai.org</a></li></ul></li><li>Article: <a href="https://hex.tech/templates/sentiment-analysis/vader-sentiment-analysis/">VADER sentiment analysis</a></li><li>NLTK module: <a href="https://www.nltk.org/api/nltk.sentiment.vader.html#module-nltk.sentiment.vader">nltk.sentiment.vader module</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h2 id=&quot;VADER&quot;&gt;&lt;a href=&quot;#VADER&quot; class=&quot;he</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="AI" scheme="https://ooge0.github.io/hexo-blog/tags/AI/"/>
    
    <category term="VADER" scheme="https://ooge0.github.io/hexo-blog/tags/VADER/"/>
    
    <category term="sentiment_analysis" scheme="https://ooge0.github.io/hexo-blog/tags/sentiment-analysis/"/>
    
  </entry>
  
  <entry>
    <title>Evolution of Text Augmentation in NLP</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/25/post_ai_nlp__evolution_of_text_augmentation_in_nlp/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/25/post_ai_nlp__evolution_of_text_augmentation_in_nlp/</id>
    <published>2024-11-24T23:03:11.000Z</published>
    <updated>2024-11-25T10:28:07.976Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Text augmentation has evolved alongside advancements in natural language processing (NLP), enabling robust data generation and model improvement. Below is a detailed history, including its origins, foundational works, and key developments.</p><h2 id="Origins-of-Development-Pre-Digital-Era-1940s–1960s"><a href="#Origins-of-Development-Pre-Digital-Era-1940s–1960s" class="headerlink" title="Origins of Development: Pre-Digital Era (1940s–1960s)"></a>Origins of Development: Pre-Digital Era (1940s–1960s)</h2><p><strong>Discovery</strong>: The foundations of text augmentation trace back to linguistic research and early computational experiments. Theoretical frameworks like <strong>Noam Chomsky’s generative grammar</strong> established the principles of sentence structure and transformation.</p><p><strong>Significance</strong>: These linguistic theories formed the basis for later computational methods for generating diverse text variations.</p><ul><li><p><strong>Book</strong>:</p><ul><li>Syntactic Structures. Noam Chomsky. 1957.<ul><li><a href="https://mitpress.mit.edu/">Read on MIT Press</a></li></ul></li><li>Syntactic Structures. Noam Chomsky. 2nd edition. 2022 (with introduction by David Lightfoot)<ul><li><a href="https://tallinzen.net/media/readings/chomsky_syntactic_structures.pdf">Read on tallinzen.net</a></li></ul></li></ul></li><li><p><strong>Papers</strong>:</p><ul><li>A Mathematical Theory of Communication. Shannon, C. E. (1948). Bell System Technical Journal, 27(3), 379–423.<ul><li>DOI: 10.1002&#x2F;j.1538-7305.1948.tb01338.x </li><li><a href="https://sci-hub.se/https://doi.org/10.1002/j.1538-7305.1948.tb01338.x">Read on sci-hub.se</a></li></ul></li><li>Three models for the description of language. Chomsky, N. (1956). IEEE Transactions on Information Theory, 2(3), 113–124.<ul><li>DOI: 10.1109&#x2F;TIT.1956.1056813</li><li><a href="https://sci-hub.se/10.1109/TIT.1956.1056813">Read on sci-hub.se</a></li></ul></li><li>Syntactic Structures. Language, Lees, R. B., &amp; Chomsky, N. (1957). 33(3), 375. <ul><li>DOI:10.2307&#x2F;411160 </li><li><a href="https://sci-hub.se/https://doi.org/10.2307/411160">Read on sci-hub.se</a></li></ul></li></ul></li></ul><hr><h2 id="Early-Rule-Based-Methods-1960s–1980s"><a href="#Early-Rule-Based-Methods-1960s–1980s" class="headerlink" title="Early Rule-Based Methods (1960s–1980s)"></a>Early Rule-Based Methods (1960s–1980s)</h2><p><strong>Discovery</strong>: Rule-based systems emerged as the first computational attempt to augment text. By encoding syntactic and semantic rules, these methods allowed for manual text transformations, such as synonym replacement and sentence restructuring.</p><p><strong>Significance</strong>: These approaches demonstrated how structured transformations could enrich NLP tasks like translation and summarization.</p><ul><li><strong>Paper</strong>:<ul><li>Computational Semantics for Natural Language Processing. Yorick Wilks. 1972.</li><li>DOI: 10.1145&#x2F;1234567</li><li><a href="https://dl.acm.org/doi/10.1145/1234567">Read on ACM</a></li></ul></li></ul><hr><h2 id="Emergence-of-Statistical-Methods-1990s"><a href="#Emergence-of-Statistical-Methods-1990s" class="headerlink" title="Emergence of Statistical Methods (1990s)"></a>Emergence of Statistical Methods (1990s)</h2><p><strong>Discovery</strong>: Statistical NLP introduced probabilistic models such as n-grams and Hidden Markov Models (HMMs), enabling dynamic text generation. Techniques like paraphrase generation through probabilistic alignment gained traction.</p><p><strong>Significance</strong>: The shift to statistical methods increased scalability and adaptability, marking a transition from deterministic rules to data-driven approaches.</p><ul><li><p><strong>Paper</strong>:</p><ul><li>A Statistical Approach to Machine Translation. Brown et al. 1990.<ul><li>DOI: 10.1162&#x2F;089120100750105975</li><li><a href="https://aclanthology.org/J90-2002.pdf">Read on aclanthology.org</a></li></ul></li></ul></li><li><p><strong>Fundamental Work</strong>:</p><ul><li>Foundations of Statistical Natural Language Processing. Manning &amp; Schütze. 1999.<ul><li>DOI: N&#x2F;A</li><li><a href="https://web.stanford.edu/~jurafsky/fsnlp/">Read on web.stanford.edu</a></li></ul></li></ul></li></ul><hr><h2 id="Word-Embeddings-and-Neural-Networks-2000s–2010s"><a href="#Word-Embeddings-and-Neural-Networks-2000s–2010s" class="headerlink" title="Word Embeddings and Neural Networks (2000s–2010s)"></a>Word Embeddings and Neural Networks (2000s–2010s)</h2><p><strong>Discovery</strong>: Embedding-based models like Word2Vec and GloVe enabled semantic-aware text augmentation, where words with similar meanings were mapped closer in vector space. Neural networks introduced deeper, context-aware text manipulation.</p><p><strong>Significance</strong>: Word embeddings made synonym substitution and paraphrasing more semantically relevant, while neural networks added contextual depth.</p><ul><li><strong>Paper</strong>:<ul><li>Distributed Representations of Words and Phrases and Their Compositionality. Mikolov et al. 2013.<ul><li>DOI: 10.1162&#x2F;153244303322533223</li><li><a href="https://arxiv.org/pdf/1310.4546">Read on arXiv</a></li></ul></li></ul></li></ul><hr><h2 id="Transformer-Revolution-2017–Present"><a href="#Transformer-Revolution-2017–Present" class="headerlink" title="Transformer Revolution (2017–Present)"></a>Transformer Revolution (2017–Present)</h2><p><strong>Discovery</strong>: Transformers like BERT, GPT, and T5 redefined NLP, introducing powerful models for context-aware text augmentation. Techniques such as masked language modeling and text-to-text generation became mainstream.</p><p><strong>Significance</strong>: The transformer architecture allowed for high-quality, large-scale text augmentation, driving state-of-the-art performance in multiple NLP tasks.</p><ul><li><p><strong>Paper</strong>:</p><ul><li>Attention Is All You Need. Vaswani et al. 2017.<ul><li>DOI: 10.48550&#x2F;arXiv.1706.03762</li><li><a href="https://arxiv.org/pdf/1706.03762">Read on arXiv</a></li></ul></li></ul></li><li><p><strong>Paper</strong>:</p><ul><li>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Devlin et al. 2018.<ul><li>DOI: 10.48550&#x2F;arXiv.1810.04805</li><li><a href="https://arxiv.org/pdf/1810.04805">Read on arXiv</a></li></ul></li></ul></li></ul><hr><h2 id="Modern-NLP-Data-Augmentation-Libraries-2020s"><a href="#Modern-NLP-Data-Augmentation-Libraries-2020s" class="headerlink" title="Modern NLP Data Augmentation Libraries (2020s)"></a>Modern NLP Data Augmentation Libraries (2020s)</h2><p><strong>Discovery</strong>: The development of augmentation libraries such as <strong>nlpaug</strong>, <strong>TextAttack</strong>, and <strong>EDA (Easy Data Augmentation)</strong> simplified access to advanced techniques like back-translation, synonym replacement, and adversarial generation.</p><p><strong>Significance</strong>: These tools democratized text augmentation, making sophisticated methods accessible for both research and industry.</p><ul><li><p><strong>Paper</strong>:</p><ul><li>TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Model Training. Morris et al. 2020.<ul><li>DOI: 10.48550&#x2F;arXiv.2005.05909</li><li><a href="https://arxiv.org/pdf/2005.05909">Read on arXiv</a></li></ul></li></ul></li><li><p><strong>Paper</strong>:</p><ul><li>EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks. Wei &amp; Zou. 2019.<ul><li>DOI: 10.48550&#x2F;arXiv.1901.11196</li><li><a href="https://arxiv.org/pdf/1901.11196">Read on arXiv</a></li></ul></li></ul></li></ul><hr><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Text augmentation has evolved from manual rules to cutting-edge neural models and accessible libraries. These advancements have significantly enriched NLP applications, highlighting the importance of augmentation in the field’s historical and future trajectory.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;Text augmentation has evolved alongsid</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="AI" scheme="https://ooge0.github.io/hexo-blog/tags/AI/"/>
    
    <category term="NLP" scheme="https://ooge0.github.io/hexo-blog/tags/NLP/"/>
    
    <category term="lexic" scheme="https://ooge0.github.io/hexo-blog/tags/lexic/"/>
    
  </entry>
  
  <entry>
    <title>NLP lexical resources</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/25/post_ai_nlp__nlp_lexical_resources/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/25/post_ai_nlp__nlp_lexical_resources/</id>
    <published>2024-11-24T23:03:11.000Z</published>
    <updated>2024-11-24T23:15:07.116Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="WordNet"><a href="#WordNet" class="headerlink" title="WordNet"></a>WordNet</h2><p>Resource: <a href="https://wordnet.princeton.edu/">https://wordnet.princeton.edu/</a><br>WordNet® is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations. The resulting network of meaningfully related words and concepts can be navigated with the browser(Link is external). WordNet is also freely and publicly available for <a href="https://wordnet.princeton.edu/node/5">download</a>. WordNet’s structure makes it a useful tool for computational linguistics and natural language processing.</p><h2 id="Glitch-Text-Generator"><a href="#Glitch-Text-Generator" class="headerlink" title="Glitch Text Generator"></a>Glitch Text Generator</h2><p>Resource: <a href="https://glyphy.io/font-generator/glitch-text">https://glyphy.io/font-generator/glitch-text</a><br>Use our glitch text generator to design creepy text for your social media accounts. Copy and paste these cursed fonts to add some weirdness to your profiles!</p><h2 id="Corrupted-Text-Python-Library"><a href="#Corrupted-Text-Python-Library" class="headerlink" title="Corrupted-Text Python Library"></a>Corrupted-Text Python Library</h2><p>A python library to generate out-of-distribution text datasets. Specifically, the library applies model-independent, commonplace corruptions (not model-specific, worst-case adversarial corruptions). We thus aim to allow benchmark-studies regarding robustness against realistic outliers.<br><a href="https://pypi.org/project/corrupted-text/">PIP</a><br><code>pip install corrupted-text</code><br><a href="https://www.geeksforgeeks.org/text-augmentation-using-corrupted-text-python-library/">Article: Text Augmentation Using Corrupted-Text Python Library</a></p><h2 id="TensorFlow-Data-Augmentation-API"><a href="#TensorFlow-Data-Augmentation-API" class="headerlink" title="TensorFlow Data Augmentation API"></a>TensorFlow Data Augmentation API</h2><p><a href="https://www.tensorflow.org/tutorials/text">Guide: Text and natural language processing with TensorFlow</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h2 id=&quot;WordNet&quot;&gt;&lt;a href=&quot;#WordNet&quot; class</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="AI" scheme="https://ooge0.github.io/hexo-blog/tags/AI/"/>
    
    <category term="NLP" scheme="https://ooge0.github.io/hexo-blog/tags/NLP/"/>
    
    <category term="lexic" scheme="https://ooge0.github.io/hexo-blog/tags/lexic/"/>
    
  </entry>
  
  <entry>
    <title>Sentiment analysis framework</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/25/post_ai_nlp__sentiment_analysis_framework/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/25/post_ai_nlp__sentiment_analysis_framework/</id>
    <published>2024-11-24T23:03:11.000Z</published>
    <updated>2024-11-27T23:26:07.554Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="INTRO"><a href="#INTRO" class="headerlink" title="INTRO"></a>INTRO</h2><ul><li>Here is a draft structure of  Python-based project with an OOP design. </li><li>It focuses on sentiment analysis for text files stored in a nested directory. </li><li>The design incorporates multiple sentiment analysis frameworks and flexible configurations for each, while storing results in a JSON file.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> abc <span class="keyword">import</span> ABC, abstractmethod</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConfigManager</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Responsible for loading and managing configuration files for different sentiment analysis frameworks.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config_dir: <span class="built_in">str</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.config_dir = config_dir</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load_config</span>(<span class="params">self, framework_name: <span class="built_in">str</span></span>) -&gt; <span class="built_in">dict</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Loads configuration for the specified framework.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        :param framework_name: Name of the sentiment analysis framework.</span></span><br><span class="line"><span class="string">        :return: Dictionary with configuration parameters.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Placeholder for loading configuration logic</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TextFileProcessor</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Handles discovery and reading of text files from the nested directory.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_dir: <span class="built_in">str</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.base_dir = base_dir</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_text_files</span>(<span class="params">self</span>) -&gt; <span class="built_in">list</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Recursively fetches all text files in the nested directory.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :return: List of file paths.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Placeholder for file discovery logic</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">read_file</span>(<span class="params">self, file_path: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Reads the content of a text file.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param file_path: Path to the text file.</span></span><br><span class="line"><span class="string">        :return: File content as a string.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Placeholder for file reading logic</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SentimentAnalyzer</span>(<span class="title class_ inherited__">ABC</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Abstract base class for all sentiment analysis frameworks.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config: <span class="built_in">dict</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.config = config</span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">analyze</span>(<span class="params">self, text: <span class="built_in">str</span></span>) -&gt; <span class="built_in">dict</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Analyzes the sentiment of the provided text.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param text: Input text for sentiment analysis.</span></span><br><span class="line"><span class="string">        :return: Dictionary containing analysis metrics.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FrameworkAAnalyzer</span>(<span class="title class_ inherited__">SentimentAnalyzer</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implements sentiment analysis using Framework A.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">analyze</span>(<span class="params">self, text: <span class="built_in">str</span></span>) -&gt; <span class="built_in">dict</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Uses Framework A to analyze sentiment.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param text: Input text for sentiment analysis.</span></span><br><span class="line"><span class="string">        :return: Dictionary with metrics from Framework A.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Placeholder for sentiment analysis logic</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FrameworkBAnalyzer</span>(<span class="title class_ inherited__">SentimentAnalyzer</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implements sentiment analysis using Framework B.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">analyze</span>(<span class="params">self, text: <span class="built_in">str</span></span>) -&gt; <span class="built_in">dict</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Uses Framework B to analyze sentiment.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param text: Input text for sentiment analysis.</span></span><br><span class="line"><span class="string">        :return: Dictionary with metrics from Framework B.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Placeholder for sentiment analysis logic</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AnalysisManager</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Coordinates the sentiment analysis process.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config_manager: ConfigManager, text_processor: TextFileProcessor</span>):</span><br><span class="line">        <span class="variable language_">self</span>.config_manager = config_manager</span><br><span class="line">        <span class="variable language_">self</span>.text_processor = text_processor</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">analyze_files</span>(<span class="params">self, frameworks: <span class="built_in">list</span></span>) -&gt; <span class="built_in">list</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Analyzes all text files using specified frameworks.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param frameworks: List of framework analyzer instances.</span></span><br><span class="line"><span class="string">        :return: List of results containing file names and analysis metrics.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        results = []</span><br><span class="line">        text_files = <span class="variable language_">self</span>.text_processor.get_text_files()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> file_path <span class="keyword">in</span> text_files:</span><br><span class="line">            content = <span class="variable language_">self</span>.text_processor.read_file(file_path)</span><br><span class="line">            metrics = []</span><br><span class="line">            <span class="keyword">for</span> framework <span class="keyword">in</span> frameworks:</span><br><span class="line">                metrics.append(framework.analyze(content))</span><br><span class="line"></span><br><span class="line">            results.append(&#123;</span><br><span class="line">                <span class="string">&quot;file_name&quot;</span>: os.path.basename(file_path),</span><br><span class="line">                <span class="string">&quot;description&quot;</span>: <span class="string">&quot;Sentiment analysis results&quot;</span>,</span><br><span class="line">                <span class="string">&quot;metrics&quot;</span>: metrics</span><br><span class="line">            &#125;)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResultSaver</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Saves the analysis results to a JSON file.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save_to_json</span>(<span class="params">self, results: <span class="built_in">list</span>, output_file: <span class="built_in">str</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Writes results to a JSON file.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param results: List of analysis results.</span></span><br><span class="line"><span class="string">        :param output_file: Path to the output JSON file.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Placeholder for JSON writing logic</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Main</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Entry point for the sentiment analysis project.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_dir: <span class="built_in">str</span>, config_dir: <span class="built_in">str</span>, output_file: <span class="built_in">str</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.base_dir = base_dir</span><br><span class="line">        <span class="variable language_">self</span>.config_dir = config_dir</span><br><span class="line">        <span class="variable language_">self</span>.output_file = output_file</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Executes the sentiment analysis pipeline.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Initialize components</span></span><br><span class="line">        config_manager = ConfigManager(<span class="variable language_">self</span>.config_dir)</span><br><span class="line">        text_processor = TextFileProcessor(<span class="variable language_">self</span>.base_dir)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Load configurations and instantiate frameworks</span></span><br><span class="line">        frameworks = [</span><br><span class="line">            FrameworkAAnalyzer(config_manager.load_config(<span class="string">&quot;FrameworkA&quot;</span>)),</span><br><span class="line">            FrameworkBAnalyzer(config_manager.load_config(<span class="string">&quot;FrameworkB&quot;</span>))</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Perform analysis</span></span><br><span class="line">        analysis_manager = AnalysisManager(config_manager, text_processor)</span><br><span class="line">        results = analysis_manager.analyze_files(frameworks)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Save results</span></span><br><span class="line">        saver = ResultSaver()</span><br><span class="line">        saver.save_to_json(results, <span class="variable language_">self</span>.output_file)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>All implementations and improvements will be presented in separate posts.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h2 id=&quot;INTRO&quot;&gt;&lt;a href=&quot;#INTRO&quot; class=&quot;he</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="NLP" scheme="https://ooge0.github.io/hexo-blog/tags/NLP/"/>
    
    <category term="sentiment_analysis" scheme="https://ooge0.github.io/hexo-blog/tags/sentiment-analysis/"/>
    
  </entry>
  
  <entry>
    <title>Types of sentiment analysis techniques in NLP</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/21/post_ai_nlp__types_of_sentiment_analysis_techniques_in_nlp/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/21/post_ai_nlp__types_of_sentiment_analysis_techniques_in_nlp/</id>
    <published>2024-11-21T18:01:11.000Z</published>
    <updated>2024-11-26T20:11:53.601Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><em>related to : <a href="/hexo-blog/ai_ml__llm_system_prompts/">LLM system prompts</a></em></p><p><strong>Types of Sentiment Analysis Techniques for NLP (with DOI Papers)</strong><br><em>This post summarizes various sentiment analysis techniques, from lexicon-based methods to advanced deep learning approaches, along with DOI references to explore these concepts further.</em></p><hr><h2 id="1-Lexicon-Based-Sentiment-Analysis"><a href="#1-Lexicon-Based-Sentiment-Analysis" class="headerlink" title="1. Lexicon-Based Sentiment Analysis"></a><strong>1. Lexicon-Based Sentiment Analysis</strong></h2><ul><li><strong>Description</strong>: Relies on predefined sentiment lexicons (dictionaries of positive, negative, and neutral words). Sentiment scores are calculated based on the presence and frequency of words.</li><li><strong>Applications</strong>: Basic polarity detection, customer feedback analysis.</li><li><strong>Key Paper</strong>:<br>Liu, B. (2012). <strong>Sentiment Analysis and Opinion Mining</strong>. <em>Synthesis Lectures on Human Language Technologies, 5</em>(1), 1–167.<br>DOI: <a href="https://doi.org/10.2200/S00416ED1V01Y201204HLT016">10.2200&#x2F;S00416ED1V01Y201204HLT016</a><br><a href="https://www.cs.uic.edu/~liub/FBS/SentimentAnalysis-and-OpinionMining.pdf">Read on www.cs.uic.edu</a></li></ul><hr><h2 id="2-Rule-Based-Sentiment-Analysis"><a href="#2-Rule-Based-Sentiment-Analysis" class="headerlink" title="2. Rule-Based Sentiment Analysis"></a><strong>2. Rule-Based Sentiment Analysis</strong></h2><ul><li><strong>Description</strong>: Combines sentiment lexicons with linguistic rules (e.g., negation handling, intensifiers). Example: “not good” &#x3D; negative, “very bad” &#x3D; strongly negative.</li><li><strong>Applications</strong>: Sentiment classification for rule-governed domains.</li><li><strong>Key Paper</strong>:<br>Hutto, C., &amp; Gilbert, E. (2014). <strong>VADER: A Parsimonious Rule-Based Model for Sentiment Analysis of Social Media Text</strong>. <em>Proceedings of the Eighth International AAAI Conference on Weblogs and Social Media</em>.<br>DOI: <a href="https://doi.org/10.1609/icwsm.v8i1.14550">10.1609&#x2F;icwsm.v8i1.14550</a><br><a href="https://ojs.aaai.org/index.php/ICWSM/article/view/14550/14399">Read on ojs.aaai.org</a></li></ul><hr><h2 id="3-Machine-Learning-Based-Sentiment-Analysis"><a href="#3-Machine-Learning-Based-Sentiment-Analysis" class="headerlink" title="3. Machine Learning-Based Sentiment Analysis"></a><strong>3. Machine Learning-Based Sentiment Analysis</strong></h2><ul><li><strong>Description</strong>: Uses ML models like Naive Bayes, SVM, or Decision Trees trained on labeled sentiment datasets.</li><li><strong>Applications</strong>: News sentiment analysis, customer reviews, product recommendations.</li><li><strong>Key Paper</strong>:<br>Pang, B., &amp; Lee, L. (2002). <strong>Thumbs Up? Sentiment Classification Using Machine Learning Techniques</strong>. <em>Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 10, 79–86.<br>DOI: <a href="https://doi.org/10.3115/1118693.1118704">10.3115&#x2F;1118693.1118704</a><br><a href="https://dl.acm.org/doi/pdf/10.3115/1118693.1118704">Read on dl.acm.org</a></li></ul><hr><h2 id="4-Deep-Learning-Based-Sentiment-Analysis"><a href="#4-Deep-Learning-Based-Sentiment-Analysis" class="headerlink" title="4. Deep Learning-Based Sentiment Analysis"></a><strong>4. Deep Learning-Based Sentiment Analysis</strong></h2><ul><li><strong>Description</strong>: Uses neural networks (CNNs, RNNs, LSTMs, Transformers) to analyze sentiment from raw text.</li><li><strong>Applications</strong>: Social media analysis, multilingual sentiment detection.</li><li><strong>Key Papers</strong>:  <ul><li>Socher, R., et al. (2013). <strong>Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</strong>. <em>Proceedings of EMNLP 2013</em>.<br><a href="https://nlp.stanford.edu/pubs/SocherEtAl_EMNLP2013.pdf">Read on nlp.stanford.edu</a></li><li>Vaswani, A., et al. (2017). <strong>Attention Is All You Need</strong>. <em>Advances in Neural Information Processing Systems (NeurIPS)</em>.<br>DOI: <a href="https://doi.org/10.48550/arXiv.1706.03762">10.48550&#x2F;arXiv.1706.03762</a><br><a href="https://arxiv.org/pdf/1706.03762">Read on arxiv.org</a></li></ul></li></ul><hr><h2 id="5-Aspect-Based-Sentiment-Analysis-ABSA"><a href="#5-Aspect-Based-Sentiment-Analysis-ABSA" class="headerlink" title="5. Aspect-Based Sentiment Analysis (ABSA)"></a><strong>5. Aspect-Based Sentiment Analysis (ABSA)</strong></h2><ul><li><strong>Description</strong>: Focuses on sentiment specific to aspects of a product or service (e.g., food, service in restaurant reviews).</li><li><strong>Applications</strong>: E-commerce reviews, detailed product feedback.</li><li><strong>Key Paper</strong>:<br>Pontiki, M., et al. (2014). <strong>SemEval-2014 Task 4: Aspect-Based Sentiment Analysis</strong>. <em>Proceedings of SemEval 2014</em>.<br>DOI: <a href="https://doi.org/10.3115/v1/S14-2004">10.3115&#x2F;v1&#x2F;S14-2004</a><br><a href="https://aclanthology.org/S14-2004.pdf">Read on aclanthology.org</a></li></ul><hr><h2 id="6-Emotion-Detection"><a href="#6-Emotion-Detection" class="headerlink" title="6. Emotion Detection"></a><strong>6. Emotion Detection</strong></h2><img src="/hexo-blog/images/img__racknitz_-_the_turk_3.jpg" style="width: 80%; max-width: 200px; border: 1px solid #ccc; padding: 11px; border-radius: 8px; float: left; margin-right: 22px" /><ul><li><strong>Description</strong>: Detects specific emotions (e.g., happiness, anger, fear) rather than just positive or negative sentiment.</li><li><strong>Applications</strong>: Crisis management, psychological studies.</li><li><strong>Key Paper</strong>:<br>Mohammad, S. M., &amp; Turney, P. D. (2013). <strong>Crowdsourcing a Word–Emotion Association Lexicon</strong>. <em>Computational Intelligence, 29</em>(3), 436–465.<br>DOI: <a href="https://doi.org/10.1111/j.1467-8640.2012.00460.x">10.1111&#x2F;j.1467-8640.2012.00460.x</a><br><a href="https://sci-hub.se/https://doi.org/10.1111/j.1467-8640.2012.00460.x">Read on sci-hub.se</a></li></ul><br><hr><h2 id="7-Multimodal-Sentiment-Analysis"><a href="#7-Multimodal-Sentiment-Analysis" class="headerlink" title="7. Multimodal Sentiment Analysis"></a><strong>7. Multimodal Sentiment Analysis</strong></h2><ul><li><strong>Description</strong>: Combines multiple data sources (e.g., text, audio, video) for sentiment analysis.</li><li><strong>Applications</strong>: Video sentiment analysis, call center analytics.</li><li><strong>Key Paper</strong>:<br>Zadeh, A., et al. (2017). <strong>Tensor Fusion Network for Multimodal Sentiment Analysis</strong>. <em>Proceedings of EMNLP 2017</em>.<br>DOI: <a href="https://doi.org/10.48550/arXiv.1707.07250">10.48550&#x2F;arXiv.1707.07250</a><br><a href="https://arxiv.org/pdf/1707.07250">Read on arxiv.org</a></li></ul><hr><h2 id="8-Hybrid-Sentiment-Analysis"><a href="#8-Hybrid-Sentiment-Analysis" class="headerlink" title="8. Hybrid Sentiment Analysis"></a><strong>8. Hybrid Sentiment Analysis</strong></h2><ul><li><strong>Description</strong>: Combines lexicon-based, rule-based, and machine learning techniques for robust and accurate sentiment detection.</li><li><strong>Applications</strong>: Industry-specific sentiment analysis.</li><li><strong>Key Paper</strong>:<br>Cambria, E., et al. (2013). <strong>New Avenues in Opinion Mining and Sentiment Analysis</strong>. <em>IEEE Intelligent Systems, 28</em>(2), 15–21.<br>DOI: <a href="https://doi.org/10.1109/MIS.2013.30">10.1109&#x2F;MIS.2013.30</a><br><a href="https://sci-hub.se/10.1109/MIS.2013.30">Read on sci-hub.se</a></li></ul><hr><h2 id="9-Transfer-Learning-for-Sentiment-Analysis"><a href="#9-Transfer-Learning-for-Sentiment-Analysis" class="headerlink" title="9. Transfer Learning for Sentiment Analysis"></a><strong>9. Transfer Learning for Sentiment Analysis</strong></h2><ul><li><strong>Description</strong>: Fine-tunes pre-trained models (e.g., BERT, RoBERTa, GPT) for sentiment classification tasks.</li><li><strong>Applications</strong>: Multilingual sentiment analysis, specialized domains.</li><li><strong>Key Paper</strong>:<br>Devlin, J., et al. (2019). <strong>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</strong>. <em>Proceedings of NAACL 2019</em>.<br>DOI: <a href="https://doi.org/10.48550/arXiv.1810.04805">10.48550&#x2F;arXiv.1810.04805</a><br><a href="https://arxiv.org/pdf/1810.04805">Read on arxiv.org</a></li></ul><hr><h2 id="10-Fine-Grained-Sentiment-Analysis"><a href="#10-Fine-Grained-Sentiment-Analysis" class="headerlink" title="10. Fine-Grained Sentiment Analysis"></a><strong>10. Fine-Grained Sentiment Analysis</strong></h2><ul><li><strong>Description</strong>: Assigns sentiment scores on a fine-grained scale (e.g., star ratings from 1 to 5).</li><li><strong>Applications</strong>: Detailed product reviews, star-rating predictions.</li><li><strong>Key Paper</strong>:<br>Yang, B., et al. (2016). <strong>Hierarchical Attention Networks for Document Classification</strong>. <em>Proceedings of NAACL 2016</em>.<br>DOI: <a href="https://doi.org/10.18653/v1/N16-1174">10.18653&#x2F;v1&#x2F;N16-1174</a></li></ul><hr><h2 id="Other-resources"><a href="#Other-resources" class="headerlink" title="Other resources"></a>Other resources</h2><ol><li><a href="https://www.nice.com/info/top-sentiment-analysis-tools-and-techniques">https://www.nice.com/info/top-sentiment-analysis-tools-and-techniques</a> </li><li>Article: Opinion Mining and Sentiment Analysis. January 2008. Foundations and Trends® in Information Retrieval 2(1–2):1-135<ul><li>DOI:10.1561&#x2F;1500000011</li><li><a href="https://www.cs.cornell.edu/home/llee/omsa/omsa.pdf">Read on cs.cornell.edu</a></li></ul></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;&lt;em&gt;related to : &lt;a href=&quot;/hexo-blog/a</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="AI" scheme="https://ooge0.github.io/hexo-blog/tags/AI/"/>
    
    <category term="NLP" scheme="https://ooge0.github.io/hexo-blog/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>NLP in pictures</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/21/post_ai_nlp__nlp_in_pictures_1/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/21/post_ai_nlp__nlp_in_pictures_1/</id>
    <published>2024-11-21T17:21:11.000Z</published>
    <updated>2024-11-21T17:52:01.459Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="NLP-extracting-information-flow"><a href="#NLP-extracting-information-flow" class="headerlink" title="NLP extracting information flow"></a>NLP extracting information flow</h2><h3 id="Desired-logical-processes"><a href="#Desired-logical-processes" class="headerlink" title="Desired (logical processes)"></a>Desired (logical processes)</h3><p>Morphological analysis &gt;&gt; Syntax analysis  &gt;&gt; Semantic analysis  &gt;&gt; Extracting information</p><hr><h3 id="NLP-text-processing-pipeline-imagination-in-some-AI-ML-engineers-heads"><a href="#NLP-text-processing-pipeline-imagination-in-some-AI-ML-engineers-heads" class="headerlink" title="NLP text processing pipeline (imagination in some AI&#x2F;ML engineers heads)"></a>NLP text processing pipeline (imagination in some AI&#x2F;ML engineers heads)</h3><p><img src="/hexo-blog/images/nlp__text_processing_pipeline_ex_1_1.png" alt="NLP text processing pipeline" title="NLP text processing pipeline" style="width: 80%; max-width: 600px; border: 1px solid #ccc; padding: 5px; border-radius: 8px;" /></p></p><p><img src="/hexo-blog/images/nlp__text_processing_pipeline_ex_1.png" alt="NLP text processing pipeline - 2" title="NLP text processing pipeline - 2" style="width: 80%; max-width: 600px; border: 1px solid #ccc; padding: 5px; border-radius: 8px;" /></p></p><hr><h3 id="Megaputer-representation"><a href="#Megaputer-representation" class="headerlink" title="Megaputer representation"></a>Megaputer representation</h3><p>Megaputer </p><ul><li><a href="https://youtu.be/D8nXgHnPcB0">YouTube: Большая языковая модель MegaGPT + лингвистические правила: гибридный подход для анализа текстов</a>   </li><li>Презентация доступна по <a href="https://disk.yandex.ru/i/IVwBA2Oa2vzyCg">ссылке.</a>  </li><li><a href="https://www.megaputer.ru/">Сайт Мегапьютер</a>  </li><li><a href="https://www.megaputer.ru/project-gallery/">Галерея проектов</a>, разработанных в PolyAnalyst<br><img src="/hexo-blog/images/extracting_information_diagram_megaputer.png" alt="Megaputer diagramm-1" title="Megaputer diagramm-1" style="width: 80%; max-width: 600px; border: 1px solid #ccc; padding: 5px; border-radius: 8px;" /></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h2 id=&quot;NLP-extracting-information-flow&quot;&gt;</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="AI" scheme="https://ooge0.github.io/hexo-blog/tags/AI/"/>
    
    <category term="ML" scheme="https://ooge0.github.io/hexo-blog/tags/ML/"/>
    
    <category term="NLP" scheme="https://ooge0.github.io/hexo-blog/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>AI - Machine Learning + emotions</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/21/post_ai_ml__emotions/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/21/post_ai_ml__emotions/</id>
    <published>2024-11-21T00:08:12.000Z</published>
    <updated>2024-11-21T22:00:11.516Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><ol><li><p>Paper: <em>InstructERC: Reforming Emotion Recognition in Conversation with a Multi-task Retrieval-based LLMs Framework</em></p><ul><li>DOI:10.48550&#x2F;arXiv.2406.18088</li><li><a href="https://openreview.net/pdf/2c4c4157e9917665bbf110ffb8a87c3eba3ebed4.pdf">Read on openreview.net</a></li></ul></li><li><p>Paper: <em>InstructERC: Reforming Emotion Recognition in Conversation with Multi-task Retrieval-Augmented Large Language Models, 2024</em></p><ul><li>DOI: 10.48550&#x2F;arXiv.2309.11911</li><li><a href="https://arxiv.org/pdf/2309.11911">Read on arxiv.org</a></li></ul></li></ol><hr><h2 id="Other-resources-papers-books"><a href="#Other-resources-papers-books" class="headerlink" title="Other resources&#x2F;papers&#x2F;books"></a>Other resources&#x2F;papers&#x2F;books</h2><ol><li><p>Book: <em>Zinker J. Creative process in gestalt therapy. Vintage books. Random House, 1978</em></p></li><li><p>Book: <em>The Expressions of the Emotions in Man and Animals. Darwin. 1872</em></p><ul><li><a href="https://darwin-online.org.uk/content/frameset?viewtype=text&itemID=F1142&pageseq=1">Read book on darwin-online.org.uk</a></li><li><a href="https://archive.org/details/expressionofemot1872darw/">✰✰✰✰✰ Read book on archive.org </a></li><li><a href="https://web.seducoahuila.gob.mx/biblioweb/upload/the_expression_of_the_emotions_in_man_and_animals.pdf">✰ Read book on web.seducoahuila.gob.mx</a></li></ul></li><li><p>Paper: PyPlutchik: Visualising and comparing emotion-annotated corpora. 2021.</p><ul><li>DOI:10.1371&#x2F;journal.pone.0256503</li><li><a href="https://www.researchgate.net/publication/354295491_PyPlutchik_Visualising_and_comparing_emotion-annotated_corpora">✰✰✰✰✰ Read on researchgate.net</a><img src="/hexo-blog/images/plutchik_s_wheel_of_emotions.jpg" alt="Plutchik's wheel of emotions" title="Plutchik's wheel of emotions" href = "https://en.wikipedia.org/wiki/Robert_Plutchik#/media/File:Plutchik-wheel.svg" style="width: 20%; max-width: 1000px; border: 1px solid #ccc; padding: 5px; border-radius: 8px;" /></li></ul></li><li><p>Paper: <em>The Feeling Wheel.Willcox,G.(1982).Transactional Analysis Journal, 12(4), 274–276.</em></p><ul><li>DOI:10.1177&#x2F;036215378201200411</li><li><a href="https://sci-hub.se/10.1177/036215378201200411">Read on sc-hub.se</a><img src="/hexo-blog/images/feeling_wheel_willcox_g_1982.jpg" alt="The Feeling Wheel.Willcox,G.(1982).Transactional Analysis Journal, 12(4), 274–276." title="The Feeling Wheel.Willcox,G.(1982).Transactional Analysis Journal, 12(4), 274–276." style="width: 40%; max-width: 1300px; border: 1px solid #ccc; padding: 5px; border-radius: 8px;" /></li></ul> <img src="/hexo-blog/images/feeling_wheel_willcox_g_1982_ru.jpg" alt="The Feeling Wheel.Willcox,G _ru" title="The Feeling Wheel.Willcox,G _ru" style="width: 20%; max-width: 1000px; border: 1px solid #ccc; padding: 5px; border-radius: 8px;" /></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Paper: &lt;em&gt;InstructERC: Refor</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="AI" scheme="https://ooge0.github.io/hexo-blog/tags/AI/"/>
    
    <category term="ML" scheme="https://ooge0.github.io/hexo-blog/tags/ML/"/>
    
    <category term="emotions" scheme="https://ooge0.github.io/hexo-blog/tags/emotions/"/>
    
  </entry>
  
  <entry>
    <title>Fine-tuning vs. Feature-based Approaches</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/21/post_ai_ml__finetuning_vs_feature_based_approaches/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/21/post_ai_ml__finetuning_vs_feature_based_approaches/</id>
    <published>2024-11-21T00:08:12.000Z</published>
    <updated>2024-11-21T00:33:52.088Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Fine-tuning and feature-based approaches are two common techniques in transfer learning, a machine learning method where a pre-trained model is reused as a starting point for a new task.</p><p><strong>Fine-tuning</strong> involves adjusting the weights of a pre-trained model on a new dataset. This method is more computationally intensive but can lead to better performance, especially when the new task is similar to the original task.</p><p><strong>Feature-based</strong> approaches, on the other hand, extract features from a pre-trained model and use them as input for a new model. This method is less computationally intensive and can be effective when the new task is different from the original task.</p><p><strong>Key Differences:</strong></p><table><thead><tr><th>Feature</th><th>Fine-tuning</th><th>Feature-based</th></tr></thead><tbody><tr><td>Model Modification</td><td>Adjusts weights of pre-trained model</td><td>Extracts features, trains new model</td></tr><tr><td>Computational Cost</td><td>Higher</td><td>Lower</td></tr><tr><td>Task Similarity</td><td>Best for similar tasks</td><td>Can be used for diverse tasks</td></tr><tr><td>Data Requirements</td><td>More data needed</td><td>Less data needed</td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;Fine-tuning and feature-based approach</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="AI" scheme="https://ooge0.github.io/hexo-blog/tags/AI/"/>
    
    <category term="ML" scheme="https://ooge0.github.io/hexo-blog/tags/ML/"/>
    
    <category term="emotions" scheme="https://ooge0.github.io/hexo-blog/tags/emotions/"/>
    
  </entry>
  
  <entry>
    <title>Prompt Engineering - task_1.</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/21/post_ai_promt_engineer_task_1/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/21/post_ai_promt_engineer_task_1/</id>
    <published>2024-11-20T23:56:12.000Z</published>
    <updated>2024-11-25T10:57:32.983Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><strong>Prompt Evaluation Guide: Assessing Prompt and Response Quality</strong></p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>In this guide, we will evaluate the quality of prompts and their corresponding responses using a machine learning model. The goal is to determine whether the outputs align with specified criteria, improving the model’s prompt-handling capability through fine-tuning or adjustments. We’ll use <strong>OpenAI’s GPT-based models</strong> as our foundation, showcasing how to configure, evaluate, and visualize results locally. </p><p>This guide includes a step-by-step walkthrough for local deployment, fine-tuning, and performance assessment, complete with visualization of results to understand the quality and impact of changes.</p><hr><h2 id="2-Required-Tools"><a href="#2-Required-Tools" class="headerlink" title="2. Required Tools"></a>2. Required Tools</h2><h3 id="Tools-and-Libraries"><a href="#Tools-and-Libraries" class="headerlink" title="Tools and Libraries"></a>Tools and Libraries</h3><ul><li><strong>Hugging Face Transformers</strong>: For model training and configuration.</li><li><strong>Datasets Library</strong>: To load and preprocess prompt-response datasets.</li><li><strong>PyTorch</strong> or <strong>TensorFlow</strong>: Backend for model execution.</li><li><strong>Matplotlib</strong> and <strong>Seaborn</strong>: For data visualization.</li><li><strong>Python 3.8+</strong>: Required for compatibility with libraries.</li><li><strong>Evaluation Metrics</strong>:<ul><li><a href="../../../../../glossary-of-machine-learning-and-ai-terms#BLEU">BLEU</a> Score</li><li><a href="(../../../../../glossary-of-machine-learning-and-ai-terms#ROUGE-L">ROUGE-L</a></li><li>Perplexity</li></ul></li></ul><h3 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a>Resources</h3><ul><li><strong>Model</strong>: Pretrained GPT-2 or similar transformer-based model. Download from <a href="https://huggingface.co/models">Hugging Face Model Hub</a>.</li><li><strong>Dataset</strong>: Use datasets like <code>squad_v2</code> or create a custom prompt-response dataset.</li><li><strong>Environment</strong>: A local Python environment or virtual environment for isolation.</li></ul><hr><h2 id="3-Installation-Guide"><a href="#3-Installation-Guide" class="headerlink" title="3. Installation Guide"></a>3. Installation Guide</h2><p><strong>Clone the repository for local setup.</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/huggingface/transformers.git</span><br><span class="line"><span class="built_in">cd</span> transformers</span><br></pre></td></tr></table></figure><p><strong>Create virtual environment.</strong><br><em>To create a virtual environment, execute the following commands in the command line:</em></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install virtualenv</span><br></pre></td></tr></table></figure><p><strong>Activate the virtual environment:</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">venv\Scripts\activate</span><br></pre></td></tr></table></figure><p><strong>Create <code>requirements.txt</code> in the project root directory.</strong><br>Add there list of Python libraries as</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">transformers</span><br><span class="line">datasets</span><br><span class="line">torch</span><br><span class="line">matplotlib </span><br><span class="line">seaborn</span><br></pre></td></tr></table></figure><p><strong>Install required Python libraries from <code>requirements.txt</code>:</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><p>or if you are not using virtual env, execute</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install transformers datasets torch matplotlib seaborn</span><br></pre></td></tr></table></figure><h2 id="4-Configuration-Guide"><a href="#4-Configuration-Guide" class="headerlink" title="4. Configuration Guide"></a>4. Configuration Guide</h2><ol><li>Prepare Configuration File</li><li>Create a config.json with the following parameters:<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;model_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gpt-2&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;dataset_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;custom_dataset.json&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;max_length&quot;</span><span class="punctuation">:</span> <span class="number">256</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;batch_size&quot;</span><span class="punctuation">:</span> <span class="number">16</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;learning_rate&quot;</span><span class="punctuation">:</span> <span class="number">5e-5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;num_epochs&quot;</span><span class="punctuation">:</span> <span class="number">3</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;evaluation_metrics&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;bleu&quot;</span><span class="punctuation">,</span> <span class="string">&quot;rouge-l&quot;</span><span class="punctuation">,</span> <span class="string">&quot;perplexity&quot;</span><span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li><li>Dataset Preparation<br>Ensure your dataset is in JSONL format:<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;What is AI?&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;response&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Artificial Intelligence is...&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Define Machine Learning&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;response&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Machine Learning is...&quot;</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="5-Core-for-Evaluation-Task"><a href="#5-Core-for-Evaluation-Task" class="headerlink" title="5. Core for Evaluation Task"></a>5. Core for Evaluation Task</h2><p>Define the evaluation process:</p><ol><li>Load Dataset: Preprocess prompts and responses.</li><li>Fine-Tune Model: Train on specific tasks to enhance response relevance.</li><li>Evaluate metrics, measure:<ol><li>BLEU, </li><li>ROUGE-L, </li><li>perplexity scores for outputs.</li></ol></li></ol><h2 id="6-Guidelines-for-Prompt-Evaluation"><a href="#6-Guidelines-for-Prompt-Evaluation" class="headerlink" title="6. Guidelines for Prompt Evaluation"></a>6. Guidelines for Prompt Evaluation</h2><p><strong>Key Evaluation Areas:</strong></p><ol><li><strong>Relevance</strong>: Does the response match the expected answer?</li><li><strong>Clarity</strong>: Is the response clear and concise?</li><li><strong>Adaptability</strong>: Does the model adjust to different prompt complexities?</li><li><strong>Consistency</strong>: Are responses uniform in quality across test cases?</li></ol><p><strong>Complexity Consideration:</strong></p><ul><li>Simple prompts: Direct, factual queries.</li><li>Complex prompts: Context-based or multi-turn questions.</li></ul><h2 id="7-Main-Scripts"><a href="#7-Main-Scripts" class="headerlink" title="7. Main Scripts"></a>7. Main Scripts</h2><p><strong>Training Script</strong></p><p>Save as <code>train.py</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load model and tokenizer</span></span><br><span class="line">model_name = <span class="string">&quot;gpt-2&quot;</span></span><br><span class="line">model = GPT2LMHeadModel.from_pretrained(model_name)</span><br><span class="line">tokenizer = GPT2Tokenizer.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load and preprocess dataset</span></span><br><span class="line">dataset = load_dataset(<span class="string">&quot;json&quot;</span>, data_files=<span class="string">&quot;custom_dataset.json&quot;</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">batch</span>):</span><br><span class="line">    <span class="keyword">return</span> tokenizer(batch[<span class="string">&quot;prompt&quot;</span>], padding=<span class="string">&quot;max_length&quot;</span>, truncation=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">tokenized_data = dataset.<span class="built_in">map</span>(tokenize, batched=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training arguments</span></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">&quot;./results&quot;</span>,</span><br><span class="line">    evaluation_strategy=<span class="string">&quot;epoch&quot;</span>,</span><br><span class="line">    learning_rate=<span class="number">5e-5</span>,</span><br><span class="line">    per_device_train_batch_size=<span class="number">16</span>,</span><br><span class="line">    num_train_epochs=<span class="number">3</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Trainer</span></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=tokenized_data[<span class="string">&quot;train&quot;</span>]</span><br><span class="line">)</span><br><span class="line">trainer.train()</span><br></pre></td></tr></table></figure><p><strong>Evaluation Script</strong></p><p>Save as <code>evaluate.py</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load model</span></span><br><span class="line">model_name = <span class="string">&quot;./results&quot;</span></span><br><span class="line">evaluator = pipeline(<span class="string">&quot;text-generation&quot;</span>, model=model_name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluate prompts</span></span><br><span class="line">prompts = [<span class="string">&quot;What is AI?&quot;</span>, <span class="string">&quot;Define Machine Learning&quot;</span>]</span><br><span class="line">responses = [evaluator(prompt, max_length=<span class="number">50</span>) <span class="keyword">for</span> prompt <span class="keyword">in</span> prompts]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Metrics</span></span><br><span class="line">metric = load_metric(<span class="string">&quot;bleu&quot;</span>)</span><br><span class="line">metric_score = metric.compute(predictions=responses, references=[<span class="string">&quot;Artificial Intelligence is...&quot;</span>, <span class="string">&quot;Machine Learning is...&quot;</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;BLEU Score:&quot;</span>, metric_score)</span><br></pre></td></tr></table></figure><h2 id="8-Visualization-and-Explanation-of-Results"><a href="#8-Visualization-and-Explanation-of-Results" class="headerlink" title="8. Visualization and Explanation of Results"></a>8. Visualization and Explanation of Results</h2><p><strong>Visualization Script</strong></p><p>Save as <code>visualize.py</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example metric scores</span></span><br><span class="line">metrics = &#123;<span class="string">&quot;BLEU&quot;</span>: <span class="number">0.85</span>, <span class="string">&quot;ROUGE-L&quot;</span>: <span class="number">0.87</span>, <span class="string">&quot;Perplexity&quot;</span>: <span class="number">15.2</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">5</span>))</span><br><span class="line">sns.barplot(x=<span class="built_in">list</span>(metrics.keys()), y=<span class="built_in">list</span>(metrics.values()))</span><br><span class="line">plt.title(<span class="string">&quot;Evaluation Metrics&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Scores&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Metric&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h2><ol><li>BLEU &amp; ROUGE-L: Higher scores indicate better text generation quality.</li><li>Perplexity: Lower scores indicate improved language model comprehension.</li></ol><hr><p>To improve the model’s performance here is possible to focus on the following activities based on the evaluation metrics and the provided GPT-2 configuration:</p><h3 id="1-Data-Preprocessing-and-Augmentation"><a href="#1-Data-Preprocessing-and-Augmentation" class="headerlink" title="1. Data Preprocessing and Augmentation"></a>1. <strong>Data Preprocessing and Augmentation</strong></h3><ul><li><strong>Data Cleaning</strong>: Ensure the training data is clean, well-structured, and consistent. Remove noisy or irrelevant content that could negatively affect performance.</li><li><strong>Augment Data</strong>: Introduce more varied examples, especially for underrepresented topics. Adding more diverse sentence structures, word choices, and contexts can help improve model robustness.</li></ul><h3 id="2-Prompt-Optimization"><a href="#2-Prompt-Optimization" class="headerlink" title="2. Prompt Optimization"></a>2. <strong>Prompt Optimization</strong></h3><ul><li><strong>Refining Prompts</strong>: Work on crafting more precise and detailed prompts to guide the model towards generating more accurate responses.</li><li><strong>Incorporate Context</strong>: Provide context-rich prompts (e.g., multi-turn conversations) or detailed instructions to ensure the model outputs relevant and coherent responses.</li><li><strong>Temperature and Sampling</strong>: Adjust the <code>do_sample</code> setting and modify <code>top_k</code> or <code>top_p</code> parameters to control the randomness and creativity of the model’s output. A lower temperature (e.g., 0.7) can reduce randomness and produce more deterministic outputs.</li></ul><h3 id="3-Model-Hyperparameters-Adjustment"><a href="#3-Model-Hyperparameters-Adjustment" class="headerlink" title="3. Model Hyperparameters Adjustment"></a>3. <strong>Model Hyperparameters Adjustment</strong></h3><ul><li><strong>Increase Layers or Heads</strong>: If you’re able to fine-tune, consider experimenting with increasing the number of layers or attention heads to help the model learn more complex patterns.</li><li><strong>Experiment with <code>n_inner</code></strong>: Fine-tuning the <code>n_inner</code> parameter (which controls the size of the intermediate layer in the transformer) may yield better results for more complex tasks.</li></ul><h3 id="4-Fine-Tuning-GPT-2"><a href="#4-Fine-Tuning-GPT-2" class="headerlink" title="4. Fine-Tuning GPT-2"></a>4. <strong>Fine-Tuning GPT-2</strong></h3><ul><li><strong>Fine-Tuning with Task-Specific Data</strong>: Fine-tune the model on your specific domain or task using high-quality, labeled datasets. Fine-tuning will allow the model to learn task-specific patterns.</li><li><strong>Transfer Learning</strong>: Use transfer learning techniques by starting with a pre-trained GPT-2 model, and then train it on your task-specific corpus to improve the output quality.</li></ul><h3 id="5-Evaluation-Metric-Specific-Adjustments"><a href="#5-Evaluation-Metric-Specific-Adjustments" class="headerlink" title="5. Evaluation Metric-Specific Adjustments"></a>5. <strong>Evaluation Metric-Specific Adjustments</strong></h3><ul><li><strong>BLEU</strong>: Since BLEU is currently 0.0, which indicates poor overlap with reference texts, consider focusing on improving the lexical similarity by training on text data with high-quality references.</li><li><strong>ROUGE</strong>: Improve the recall and precision for ROUGE scores by providing more informative prompts that encourage the model to capture key content and key phrases.</li><li><strong>METEOR</strong>: Since METEOR considers synonyms and paraphrases, increasing the model’s understanding of semantic equivalence might improve this score. Use data augmentation or adversarial training to enhance this aspect.</li><li><strong>BERTScore</strong>: BERTScore evaluates embeddings, so improving model embeddings can significantly help. You can experiment with fine-tuning GPT-2 using BERT-based models (like <code>bert-base-uncased</code>) for better contextual word representations.</li></ul><h3 id="6-Regularization-Techniques"><a href="#6-Regularization-Techniques" class="headerlink" title="6. Regularization Techniques"></a>6. <strong>Regularization Techniques</strong></h3><ul><li><strong>Dropout Regularization</strong>: Experiment with adjusting <code>attn_pdrop</code>, <code>embd_pdrop</code>, and other dropout parameters to control overfitting and improve generalization.</li><li><strong>Layer Normalization</strong>: Ensure that layer normalization parameters (<code>layer_norm_epsilon</code>) are tuned properly to stabilize learning and avoid vanishing&#x2F;exploding gradients.</li></ul><h3 id="7-Model-Size-and-Parameters"><a href="#7-Model-Size-and-Parameters" class="headerlink" title="7. Model Size and Parameters"></a>7. <strong>Model Size and Parameters</strong></h3><ul><li><strong>Larger Models</strong>: If feasible, switch to larger models (e.g., GPT-2 Medium, GPT-2 Large, or GPT-3) for more capacity and better performance in complex tasks.</li><li><strong>Learning Rate and Optimizer Tuning</strong>: Adjust the learning rate for better convergence. Use learning rate schedulers to optimize training over time and avoid issues like vanishing gradients or poor local minima.</li></ul><h3 id="8-Loss-Function-Adjustments"><a href="#8-Loss-Function-Adjustments" class="headerlink" title="8. Loss Function Adjustments"></a>8. <strong>Loss Function Adjustments</strong></h3><ul><li><strong>Loss Function Tweaks</strong>: Investigate the loss function (cross-entropy in GPT-2) to ensure it’s optimized for your specific task. Sometimes, switching the loss function can help improve performance in tasks like summarization or question-answering.</li></ul><h3 id="9-Sampling-Strategies"><a href="#9-Sampling-Strategies" class="headerlink" title="9. Sampling Strategies"></a>9. <strong>Sampling Strategies</strong></h3><ul><li><strong>Top-k Sampling</strong>: Adjust the <code>top_k</code> parameter during text generation to sample from the top K most likely words. This can prevent repetitive or irrelevant generation.</li><li><strong>Nucleus Sampling</strong>: Adjust the <code>top_p</code> value to sample words from the cumulative probability distribution of the top P words, ensuring more diversity in the outputs.</li></ul><h3 id="10-Model-Evaluation-and-Iteration"><a href="#10-Model-Evaluation-and-Iteration" class="headerlink" title="10. Model Evaluation and Iteration"></a>10. <strong>Model Evaluation and Iteration</strong></h3><ul><li><strong>Cross-validation</strong>: Use cross-validation to evaluate different configurations and fine-tuned models to find the optimal setup.</li><li><strong>Hyperparameter Search</strong>: Perform a hyperparameter search (e.g., grid search, random search) to find the best set of hyperparameters for improving performance metrics.</li></ul><h3 id="Example-Implementation"><a href="#Example-Implementation" class="headerlink" title="Example Implementation:"></a>Example Implementation:</h3><pre><code class="python">from transformers import GPT2LMHeadModel, GPT2Tokenizerimport torch# Load pre-trained model and tokenizermodel = GPT2LMHeadModel.from_pretrained(&#39;gpt2&#39;)tokenizer = GPT2Tokenizer.from_pretrained(&#39;gpt2&#39;)# Refine the prompt to improve resultsprompt = &quot;Describe the importance of artificial intelligence in healthcare.&quot;# Generate text using refined promptinputs = tokenizer(prompt, return_tensors=&quot;pt&quot;)outputs = model.generate(**inputs, max_length=100, do_sample=True, top_p=0.95, top_k=60)# Decode and print the outputgenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)print(generated_text)</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;&lt;strong&gt;Prompt Evaluation Guide: Asses</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="AI" scheme="https://ooge0.github.io/hexo-blog/tags/AI/"/>
    
    <category term="ML" scheme="https://ooge0.github.io/hexo-blog/tags/ML/"/>
    
    <category term="prompt_engineering" scheme="https://ooge0.github.io/hexo-blog/tags/prompt-engineering/"/>
    
  </entry>
  
  <entry>
    <title>Web data handling</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/20/notes/notes_web_data_handling/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/20/notes/notes_web_data_handling/</id>
    <published>2024-11-20T19:38:30.000Z</published>
    <updated>2024-12-03T07:34:55.040Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="Get-text-from-DOM-by-specific-locator-via-DevTools"><a href="#Get-text-from-DOM-by-specific-locator-via-DevTools" class="headerlink" title="Get text from DOM by specific locator via DevTools"></a>Get text from DOM by specific locator via DevTools</h2><ol><li><strong>Task</strong>: Retrieve text from DOM for elements that have locator “.gfg-similar-read-item-heading” using devtools<br>Solution:<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Retrieve all elements matching the locator</span></span><br><span class="line"><span class="keyword">const</span> elements = <span class="variable language_">document</span>.<span class="title function_">querySelectorAll</span>(<span class="string">&quot;.gfg-similar-read-item-heading&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Extract text content from each element</span></span><br><span class="line"><span class="keyword">const</span> texts = <span class="title class_">Array</span>.<span class="title function_">from</span>(elements).<span class="title function_">map</span>(<span class="function"><span class="params">element</span> =&gt;</span> element.<span class="property">textContent</span>.<span class="title function_">trim</span>());</span><br><span class="line"></span><br><span class="line"><span class="comment">// Output the text content as an array</span></span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(texts);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Optional: Display the texts in format &#123;order_number&#125;&#123;text&#125;</span></span><br><span class="line">texts.<span class="title function_">forEach</span>(<span class="function">(<span class="params">text, index</span>) =&gt;</span> <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">`<span class="subst">$&#123;index + <span class="number">1</span>&#125;</span>: <span class="subst">$&#123;text&#125;</span>`</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// Optional: Display the texts in format &#123;text&#125;</span></span><br><span class="line">texts.<span class="title function_">forEach</span>(<span class="function">(<span class="params">text, index</span>) =&gt;</span> <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">`<span class="subst">$&#123;text&#125;</span>`</span>));</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h2 id=&quot;Get-text-from-DOM-by-specific-loc</summary>
      
    
    
    
    <category term="Notes" scheme="https://ooge0.github.io/hexo-blog/categories/Notes/"/>
    
    
    <category term="web" scheme="https://ooge0.github.io/hexo-blog/tags/web/"/>
    
  </entry>
  
  <entry>
    <title>Windows services and VM usage</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/20/notes/notes_windows_services_and_vm_usage/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/20/notes/notes_windows_services_and_vm_usage/</id>
    <published>2024-11-20T19:38:30.000Z</published>
    <updated>2024-12-03T07:27:09.503Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><ol><li><p><strong>Virtual Machine Setup</strong><br>Steps to create a virtual machine:</p><ol><li>Download a hypervisor:<ol><li>Go to <a href="https://www.virtualbox.org/wiki/Downloads">VirtualBox</a> or <a href="https://www.vmware.com/">VMware</a>. Download and install the software on your machine.</li><li>Obtain an OS image:<ol><li>Download the required ISO file for the operating system.</li></ol></li><li>Create the virtual machine:<ol><li>Open VirtualBox&#x2F;VMware app.</li><li>Create a new virtual machine according to requested specification.&#96;&#96;&#96;&#96;</li><li>Configure name, OS type, hardware settings like RAM, CPU, and disk space.</li></ol></li><li>Install the OS<ol><li>Start the virtual machine.</li><li>Follow the installation steps in the OS setup wizard.</li></ol></li><li>Connect to the VM:<ol><li>For Linux VMs: Enable SSH in the VM during installation or after setup.</li><li>Use SSH (on Linux&#x2F;Mac) or tools like PuTTY (on Windows) to connect.</li></ol></li><li>Additional steps:<ol><li>Windows VM Remote Desktop:<ol><li>Enable Remote Desktop in the Windows VM settings. Use the RDP client, configuration described here &gt;&gt; <a href="https://learn.microsoft.com/uk-ua/windows-server/remote/remote-desktop-services/clients/remote-desktop-clients">Remote Desktop clients for Remote Desktop Services and remote PCs</a> on your host machine to connect.</li><li>Connect RDP client to remote machine.</li></ol></li></ol></li></ol></li></ol></li><li><p><strong>Windows Services Management in PowerShell.</strong></p><p>Task: </p><ol><li>Print  list of services</li><li>Run specific service</li><li>Stop specific service</li><li>Restart specific service</li><li>Check status for specific service</li></ol><p> To get list of running services I will use :</p> <figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Get-Service</span> | <span class="built_in">Where-Object</span> &#123;<span class="variable">$_</span>.Status <span class="operator">-eq</span> <span class="string">&#x27;Running&#x27;</span>&#125;</span><br></pre></td></tr></table></figure><p> Assuming that I want to run <code>&quot;WSLService&quot;</code></p><p> To start <code>&quot;WSLService&quot;</code> service  I’m using :<br> <figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Start-Service</span> <span class="literal">-Name</span> <span class="string">&quot;WSLService&quot;</span></span><br></pre></td></tr></table></figure><br> To stop the service:<br> <figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Stop-Service</span> <span class="literal">-Name</span> <span class="string">&quot;WSLService&quot;</span></span><br></pre></td></tr></table></figure></p><p> For restarting service:</p> <figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Restart-Service</span> <span class="literal">-Name</span> <span class="string">&quot;WSLService&quot;</span></span><br></pre></td></tr></table></figure><p> For checking the status of monitored service:</p> <figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Get-Service</span> <span class="literal">-Name</span> <span class="string">&quot;WSLService&quot;</span></span><br></pre></td></tr></table></figure></li><li><p><strong>SSH connection to the Linux machine</strong> actions are below.</p><p>I added more information in advance because the original task does not contain information about the client machine from which the connection to the Linux machine will be made. There is 3 different cases:</p><ul><li>Case 1: From another Linux client to Linux host</li><li>Case 2: From a Windows client  to Linux host</li><li>Case 3: From macOS client to Linux host</li></ul><p>For Linux&#x2F;Mac: SSH is usually pre-installed but if it’s missing install an SSH client (e.g. on Ubuntu):<br>   <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt install openssh-client -y</span><br></pre></td></tr></table></figure><br>After installation, start the service on Linux machine:<br>   <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> systemctl start ssh</span><br></pre></td></tr></table></figure><br>If SSH service not started , checked its status on Linux machine:<br>    <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl status ssh</span><br></pre></td></tr></table></figure></p><p>Generate SSH keys (on the host machine):</p>  <figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh<span class="literal">-keygen</span> <span class="literal">-t</span> rsa <span class="literal">-b</span> <span class="number">2048</span></span><br></pre></td></tr></table></figure><p>  The keys will be stored in ~&#x2F;.ssh&#x2F;id_rsa (private) and ~&#x2F;.ssh&#x2F;id_rsa.pub (public).<br>  Copy the public key to the Linux server:</p>  <figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh<span class="literal">-copy-id</span> user@server_ip</span><br></pre></td></tr></table></figure><p>  Or manually append the content of id_rsa.pub to ~&#x2F;.ssh&#x2F;authorized_keys on the server.</p><p>  Connection via SSH for Linux:<br>   <figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh user@server_ip</span><br></pre></td></tr></table></figure></p></li><li><p>Optional Configurations:</p><p>Edit the SSH config file (~&#x2F;.ssh&#x2F;config) for aliases:</p>   <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Host server_alias  </span><br><span class="line">   HostName server_ip  </span><br><span class="line">   User username  </span><br><span class="line">   IdentityFile ~/.ssh/id_rsa  </span><br></pre></td></tr></table></figure><p>This will simplify the connection:</p>   <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh server_alias</span><br></pre></td></tr></table></figure></li></ol><p><strong>Case 1</strong>: From another Linux client to Linux host</p><p>  Verify SSH Client on the Local Machine:</p><ul><li>Most Linux distributions come with the ssh client pre-installed. Confirm by running:<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -V</span><br></pre></td></tr></table></figure> If it’s not installed, install it: <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt install openssh-client  </span><br></pre></td></tr></table></figure></li></ul><p>   Replace <code>&lt;username&gt;</code> with the remote machine’s username.</p><p>   Replace <code>&lt;remote_ip&gt;</code> with the target machine’s IP address.</p><ul><li>Use Public Key Authentication (Optional): <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure></li><li>Copy the public key to the remote machine:<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id &lt;username&gt;@&lt;remote_ip&gt;</span><br></pre></td></tr></table></figure></li><li>Now, log in without entering a password:<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh &lt;username&gt;@&lt;remote_ip&gt;</span><br></pre></td></tr></table></figure></li></ul><p><strong>Case 2</strong>: From a Windows client  to Linux host</p><ul><li><p>Install an SSH Client:</p><ul><li>Use the built-in OpenSSH client on Windows 10+. Open PowerShell or Command Prompt and check if SSH is installed:  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -V</span><br></pre></td></tr></table></figure><ul><li>For Windows alternatively install <a href="https://www.openssh.com/">OpenSSH</a> client from <a href="https://learn.microsoft.com/en-us/windows/client-management/client-tools/add-remove-hide-features?pivots=windows-11">“Optional Features”</a> or <a href="https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html">download PuTTY</a>( third-party SSH client).</li></ul></li></ul></li><li><p>Connect Using OpenSSH (Built-in).</p><p>Open PowerShell or Command Prompt and run:  </p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh &lt;username&gt;@&lt;remote_ip&gt;</span><br></pre></td></tr></table></figure></li><li><p>Connect Using PuTTY:</p><ul><li>Download and install PuTTY.</li><li>Open PuTTY and enter the hostname or IP address.</li><li>Set the Port to 22 and click Open.</li><li>Log in with your credentials.</li><li>Enable Key Authentication (Optional):<ul><li>Use <a href="https://www.puttygen.com/">puttygen</a> to generate a private&#x2F;public key pair.</li><li>Copy the public key to the Linux machine’s ~&#x2F;.ssh&#x2F;authorized_keys.</li><li>In PuTTY, configure the private key in Connection &gt; SSH &gt; Auth &gt; Browse Private Key.</li></ul></li></ul></li></ul><p><strong>Case 3</strong>: From macOS client to Linux host </p><ul><li><p>Verify SSH Client:</p><p> macOS includes an SSH client by default. Confirm it by running:</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -V</span><br></pre></td></tr></table></figure></li><li><p>Connect to the Linux Machine:</p><p>Open Terminal and run:</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh &lt;username&gt;@&lt;remote_ip&gt;</span><br></pre></td></tr></table></figure></li><li><p>Use Public Key Authentication (Optional):</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure></li><li><p>Copy the public key to the remote machine:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id &lt;username&gt;@&lt;remote_ip&gt;</span><br></pre></td></tr></table></figure></li><li><p>Now, I can log in without entering a password:</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh &lt;username&gt;@&lt;remote_ip&gt;</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Virtual Machine Setup</summary>
      
    
    
    
    <category term="Notes" scheme="https://ooge0.github.io/hexo-blog/categories/Notes/"/>
    
    
    <category term="Dashdevs" scheme="https://ooge0.github.io/hexo-blog/tags/Dashdevs/"/>
    
  </entry>
  
  <entry>
    <title>Text Classification. Historical overview, tools, and techniques.</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/19/post_ai_nlp__text_classification_intro_1/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/19/post_ai_nlp__text_classification_intro_1/</id>
    <published>2024-11-19T10:05:30.000Z</published>
    <updated>2024-11-19T10:06:14.446Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Text classification is a fundamental task in Natural Language Processing (NLP) that involves categorizing text into predefined categories. Its applications range from sentiment analysis to spam detection, and news categorization to intent recognition in conversational AI. This document provides a historical overview, a comparative analysis of tools, and details about modern approaches like Word2Vec, FastText, GloVe, and deep learning.</p><ol><li><p>Historical Overview of Text Classification</p><ol><li><strong>Traditional Methods (1950s - 2000s)</strong><ol><li><em>Bag of Words (BoW):</em><ul><li>Represents text as a frequency vector of words.</li><li>Pros: Simple and interpretable.</li><li>Cons: Ignores word order and semantics.</li></ul></li><li><em>TF-IDF (Term Frequency-Inverse Document Frequency):</em><ul><li>Improves BoW by weighting rare words higher.</li><li>Pros: Better at distinguishing important terms.</li><li>Cons: Still ignores context and word relationships.</li></ul></li><li><em>Naive Bayes:</em><ul><li>Commonly used with BoW or TF-IDF for classification.</li><li>Pros: Fast and robust for small datasets.</li><li>Cons: Assumes word independence, which rarely holds.</li></ul></li></ol></li></ol></li><li><p><strong>Emergence of Distributed Representations (2010s)</strong><br>The advent of distributed word representations marked a significant leap, addressing the limitations of sparse representations.</p><ol><li><em>Word2Vec (2013, Mikolov et al.):</em><ul><li>Generates dense vector embeddings using Skip-gram or CBOW.</li><li>Paper: Efficient Estimation of Word Representations in Vector Space<ul><li>DOI: <a href="https://doi.org/10.48550/arXiv.1301.3781">10.48550&#x2F;arXiv.1301.3781</a></li></ul></li><li>Pros: Captures semantic relationships and is computationally efficient.</li><li>Cons: Fixed-size embeddings and lacks out-of-vocabulary word handling.</li></ul></li><li><em>GloVe (2014, Pennington et al.):</em><br> Combines global word co-occurrence statistics with local context to produce embeddings.<ul><li>Paper: GloVe: Global Vectors for Word Representation<ul><li>DOI: <a href="https://aclanthology.org/D14-1162.pdf">10.3115&#x2F;v1&#x2F;D14-1162</a></li></ul></li><li>Pros: Effective at capturing statistical information.</li><li>Cons: Pre-trained on fixed corpora; inflexible for dynamic contexts.</li></ul></li><li><em>FastText (2016, Bojanowski et al.):</em><br>   Extends Word2Vec by representing words as n-grams of characters.</li></ol><ul><li>Paper:Enriching Word Vectors with Subword Information, 2016<ul><li>DOI: <a href="https://arxiv.org/pdf/1607.04606">10.48550&#x2F;arXiv.1607.04606</a></li></ul></li><li>Pros: Handles rare and out-of-vocabulary words better.</li><li>Cons: Increased computational cost compared to Word2Vec.</li></ul></li><li><p><strong>Deep Learning Era (Late 2010s - Present)</strong><br>The rise of neural networks transformed text classification. Key innovations include:</p><ol><li><em>Recurrent Neural Networks (RNNs):</em><br>Captures sequential dependencies but suffers from vanishing gradients.</li><li><em>Convolutional Neural Networks (CNNs):</em><br>Effective for capturing local patterns in text.<ul><li>Paper: An Introduction to Convolutional Neural Networks<ul><li>DOI: <a href="https://arxiv.org/pdf/1511.08458">10.48550&#x2F;arXiv.1511.08458</a></li></ul></li><li>Papper: A review of convolutional neural networks in computer<ul><li>Read the doc &gt;&gt;&gt; <a href="../../../../../docs/other/a_review_of_convolutional_neural_networks_in_computer_vision.pdf">A review of convolutional neural networks in computer</a></li></ul></li></ul></li><li><em>Transformers and Pre-trained Models (2018 - Present):</em><br>Models like:<ul><li>BERT<ul><li>Paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2018 <ul><li>DOI:<a href="https://doi.org/10.48550/arXiv.1810.04805">10.48550&#x2F;arXiv.1810.04805</a></li></ul></li></ul></li><li>GPT<ul><li>GPT-2<ul><li>Paper: Release Strategies and the Social Impacts of Language Models, 2019 </li><li>DOI:<a href="https://arxiv.org/pdf/1908.09203">10.48550&#x2F;arXiv.1908.09203</a></li></ul></li><li>GPT-3.5<ul><li>Paper: Language Models are Few-Shot Learners, 2020 </li><li>DOI:<a href="https://arxiv.org/pdf/2005.14165">10.48550&#x2F;arXiv.2005.14165</a></li></ul></li><li>GPT-4<ul><li>Paper: GPT-4 Technical Report, 2023 </li><li>DOI:<a href="https://doi.org/10.48550/arXiv.2303.08774">10.48550&#x2F;arXiv.2303.08774</a></li></ul></li></ul></li><li>and T5 revolutionized NLP by leveraging attention mechanisms and transfer learning.<ul><li>Paper: T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, 2019</li><li>DOI:<a href="https://arxiv.org/pdf/1910.10683">10.48550&#x2F;arXiv.1910.10683</a></li></ul></li></ul></li></ol></li></ol><h2 id="Footnotes"><a href="#Footnotes" class="headerlink" title="Footnotes"></a>Footnotes</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">GLUE - The General Language Understanding Evaluation (GLUE) benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">IMDB Dataset - Large Movie Review Dataset for Sentiment Analysis by Stanford.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">CoNLL-2003 - Dataset for Named Entity Recognition and other sequence modeling tasks.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">OntoNotes - Annotated dataset covering various linguistic phenomena.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">WMT - Workshop on Machine Translation datasets for translation tasks.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">OpenSubtitles - Large corpus of subtitles for multilingual tasks.<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">CNN/DailyMail - Dataset for abstractive summarization tasks.<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">XSum - Dataset for extreme summarization of news articles.<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">AG News - News topic classification dataset.<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Reuters-21578 - Text categorization benchmark dataset.<a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">SQuAD - Stanford Question Answering Dataset for reading comprehension tasks.<a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">TriviaQA - Dataset containing trivia questions and evidence passages.<a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">WikiText - Dataset for language modeling based on Wikipedia articles.<a href="#fnref:13" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Penn Treebank - Corpus for linguistic annotation and modeling.<a href="#fnref:14" rev="footnote"> ↩</a></span></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Universal Dependencies - Framework for consistent grammatical annotation across languages.<a href="#fnref:15" rev="footnote"> ↩</a></span></li><li id="fn:16"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">16.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">WSJ Corpus - Dataset from Wall Street Journal articles for POS tagging.<a href="#fnref:16" rev="footnote"> ↩</a></span></li><li id="fn:17"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">17.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Coreference resolution (CR) is the task of finding all linguistic expressions (called mentions) in a given text that refer to the same real-world entity.<a href="#fnref:17" rev="footnote"> ↩</a></span></li><li id="fn:18"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">18.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">CoNLL-2012 - Shared task on coreference resolution and other NLP challenges.<a href="#fnref:18" rev="footnote"> ↩</a></span></li><li id="fn:19"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">19.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Annotated dataset created to evaluate RoBERTa’s performance on coreference tasks, with a focus on contextual embeddings.<a href="#fnref:19" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;Text classification is a fundamental t</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="AI" scheme="https://ooge0.github.io/hexo-blog/tags/AI/"/>
    
    <category term="ML" scheme="https://ooge0.github.io/hexo-blog/tags/ML/"/>
    
    <category term="NLP" scheme="https://ooge0.github.io/hexo-blog/tags/NLP/"/>
    
    <category term="text_classification" scheme="https://ooge0.github.io/hexo-blog/tags/text-classification/"/>
    
  </entry>
  
  <entry>
    <title>Довідник для Лікаря Сімейної Медицини</title>
    <link href="https://ooge0.github.io/hexo-blog/nastusja/"/>
    <id>https://ooge0.github.io/hexo-blog/nastusja/</id>
    <published>2024-11-18T09:28:32.625Z</published>
    <updated>2024-11-18T09:52:44.381Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Цей довідник створений власними силами та за допомогою відкритих джерел для допомоги лікарям сімейної медицини, надаючи ключові ресурси, офіційні українські портали, посилання на довідники, глосарії та інші корисні інструменти.</p><hr><h2 id="Офіційні-Українські-Ресурси"><a href="#Офіційні-Українські-Ресурси" class="headerlink" title="Офіційні Українські Ресурси"></a>Офіційні Українські Ресурси</h2><ol><li><p><strong>Міністерство охорони здоров’я України (МОЗ)</strong>  </p><ul><li>Офіційний сайт: <a href="https://moz.gov.ua/">moz.gov.ua</a>  </li><li>Новини, нормативна база, протоколи лікування.</li></ul></li><li><p><strong>Національна служба здоров’я України (НСЗУ)</strong>  </p><ul><li>Офіційний сайт: <a href="https://nszu.gov.ua/">nszu.gov.ua</a>  </li><li>Інформація про медичні послуги, звіти, та декларації з лікарями.</li></ul></li><li><p><strong>Центр громадського здоров’я МОЗ України</strong>  </p><ul><li>Сайт: <a href="https://phc.org.ua/">phc.org.ua</a>  </li><li>Вакцинація, профілактика хвороб, епідеміологія.</li></ul></li></ol><hr><h2 id="Медичні-Довідники-та-Компедіуми"><a href="#Медичні-Довідники-та-Компедіуми" class="headerlink" title="Медичні Довідники та Компедіуми"></a>Медичні Довідники та Компедіуми</h2><ol><li><p><strong>Компендіум Лікарських Засобів</strong>  </p><ul><li>Сайт: <a href="https://compendium.com.ua/">compendium.com.ua</a>  </li><li>Інформація про ліки, інструкції, дозування.</li></ul></li><li><p><strong>Реєстр Ліків України</strong>  </p><ul><li>Сайт: <a href="https://www.drlz.com.ua/">drlz.com.ua</a>  </li><li>Офіційний перелік зареєстрованих ліків.</li></ul></li><li><p><strong>Клінічні протоколи лікування</strong>  </p><ul><li>МОЗ регулярно оновлює протоколи: <a href="https://guidelines.moz.gov.ua/">Протоколи МОЗ</a>.</li></ul></li></ol><hr><h2 id="Глосарії-та-Довідкові-Матеріали"><a href="#Глосарії-та-Довідкові-Матеріали" class="headerlink" title="Глосарії та Довідкові Матеріали"></a>Глосарії та Довідкові Матеріали</h2><ol><li><p><strong>Медичний Глосарій</strong>  </p><ul><li>Глосарій МОЗ: <a href="https://moz.gov.ua/uk/glosarij">moz.gov.ua</a>.</li><li>МОЗ Про затвердження Єдиного термінологічного словника (Глосарій) з питань управління  якості медичної допомоги: <a href="https://zakon.rada.gov.ua/rada/show/v0427282-11#Text">zakon.rada.gov.ua</a></li></ul></li><li><p><strong>Довідники для лікарів первинної ланки</strong>  </p><ul><li>Рекомендації для первинної допомоги: <a href="https://phc.org.ua/">phc.org.ua</a>.</li><li>Кишеньковий довідник лікаря первинної медичної допомоги для роботи з дітьми та підлітками: настанови щодо зміцнення здоров’я, профілактики та лікування захворювань від народження до підліткового віку: <a href="https://www.who.int/ukraine/uk/publications/9789289057622">who.int</a></li><li>Що входить до обов’язків лікаря первинної ланки?: <a href="https://moz.gov.ua/uk/scho-vhodit-do-obovjazkiv-likarja-pervinnoi-lanki">moz.gov.ua</a></li></ul></li></ol><hr><h2 id="Лабораторії-та-Діагностика"><a href="#Лабораторії-та-Діагностика" class="headerlink" title="Лабораторії та Діагностика"></a>Лабораторії та Діагностика</h2><ol><li><p><strong>Сінево Україна</strong>  </p><ul><li><a href="https://www.synevo.ua/">synevo.ua</a></li></ul></li><li><p><strong>Діла Лабораторія</strong>  </p><ul><li><a href="https://dila.ua/">dila.ua</a></li></ul></li><li><p><strong>Меділаб</strong>  </p><ul><li><a href="https://www.medilab.com.ua/">medilab.com.ua</a></li></ul></li></ol><hr><h2 id="Глобальні-Медичні-Ресурси"><a href="#Глобальні-Медичні-Ресурси" class="headerlink" title="Глобальні Медичні Ресурси"></a>Глобальні Медичні Ресурси</h2><ol><li><p><strong>Всесвітня організація охорони здоров’я (ВООЗ)</strong>  </p><ul><li>Сайт: <a href="https://www.who.int/">who.int</a>  </li><li>Міжнародні рекомендації та дослідження.</li></ul></li><li><p><strong>Медична Бібліотека США (PubMed)</strong>  </p><ul><li>Сайт: <a href="https://pubmed.ncbi.nlm.nih.gov/">pubmed.ncbi.nlm.nih.gov</a>  </li><li>Наукові статті з медицини.</li></ul></li><li><p><strong>CDC (Центри контролю та профілактики захворювань, США)</strong>  </p><ul><li>Сайт: <a href="https://www.cdc.gov/">cdc.gov</a>  </li><li>Профілактика, контроль хвороб.</li></ul></li></ol><hr><h2 id="Корисні-Інструменти"><a href="#Корисні-Інструменти" class="headerlink" title="Корисні Інструменти"></a>Корисні Інструменти</h2><ul><li><p><strong>Клінічні калькулятори</strong>: <a href="https://www.msdmanuals.com/uk/professional/pages-with-widgets/clinical-calculators?mode=list">msdmanuals.com</a>  </p></li><li><p><strong>Медичні калькулятори</strong>: <a href="https://medcalc.com.ua/">medcalc.com.ua</a>  </p></li><li><p><strong>Калькулятор маси тіла</strong>: <a href="https://cardioprostir.com.ua/applications-and-calculators/calculators/imt">cardioprostir.com.ua</a>  </p></li><li><p><strong>Калькулятор Калькулятор оцінки прихильності пацієнта</strong>: <a href="https://cardioprostir.com.ua/applications-and-calculators/calculators/goodwill">cardioprostir.com.ua</a>  </p></li><li><p><strong>Розрахунок дозувань</strong>: <a href="https://clincalc.com/">clincalc.com</a>  </p></li><li><p><strong>Медичний калькулятор: Дозування таблеток в залежності від маси тіла</strong>: <a href="https://testresult.org/ua/medical-calc/dozuvannia-tabletok-za-vahoiu">testresult.org</a>  </p></li><li><p><strong>Перевірка взаємодії препаратів</strong> Після переходу на сайт для перекладу натисніть правою кнопкою миші та оберіть “Перекласти”: <a href="https://reference.medscape.com/drug-interactionchecker">cardioprostir.com.ua</a></p></li></ul><hr><h2 id="Завантаження-та-Контакти"><a href="#Завантаження-та-Контакти" class="headerlink" title="Завантаження та Контакти"></a>Завантаження та Контакти</h2><p>Зберігайте цей список для швидкого доступу до корисних ресурсів! Якщо у вас є запитання чи пропозиції, будь ласка, зв’яжіться з автором через офіційні канали МОЗ або НСЗУ.</p><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;Цей довідник створений власними силами</summary>
      
    
    
    
    <category term="Notes" scheme="https://ooge0.github.io/hexo-blog/categories/Notes/"/>
    
    
    <category term="nst" scheme="https://ooge0.github.io/hexo-blog/tags/nst/"/>
    
  </entry>
  
  <entry>
    <title>Glossary of Machine Learning and AI Terms</title>
    <link href="https://ooge0.github.io/hexo-blog/glossary-of-machine-learning-and-ai-terms/"/>
    <id>https://ooge0.github.io/hexo-blog/glossary-of-machine-learning-and-ai-terms/</id>
    <published>2024-11-17T22:00:00.000Z</published>
    <updated>2024-11-30T14:55:03.372Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><em>Feel free to send me your thoughts, notes, other references. My contact details you can find on <a href="https://www.linkedin.com/in/romandenysenko/">LinkedIn</a></em></p><h3 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h3><p><a href="#A">A</a>  <a href="#B">B</a>  <a href="#C">C</a>  <a href="#D">D</a>  <a href="#E">E</a>  <a href="#F">F</a>  <a href="#G">G</a>  <a href="#H">H</a>  <a href="#I">I</a>  <a href="#J">J</a>  <a href="#K">K</a>  <a href="#L">L</a>  <a href="#M">M</a>  <a href="#N">N</a>  <a href="#O">O</a>  <a href="#P">P</a>  <a href="#Q">Q</a>  <a href="#R">R</a>  <a href="#S">S</a>  <a href="#T">T</a>  <a href="#U">U</a>  <a href="#V">V</a>  <a href="#W">W</a>  <a href="#X">X</a>  <a href="#Y">Y</a>  <a href="#Z">Z</a></p><h2 id="A"><a href="#A" class="headerlink" title="A"></a>A</h2><h3 id="Autoencoders"><a href="#Autoencoders" class="headerlink" title="Autoencoders"></a>Autoencoders</h3><p>Autoencoders are neural network architectures used for unsupervised learning. They are designed to compress input data into a lower-dimensional latent space (encoding) and then reconstruct the original data from this compressed representation (decoding). The primary objective is to minimize the reconstruction error, typically measured as the difference between input and output. </p><p><strong>Applications:</strong></p><ol><li><strong>Dimensionality Reduction</strong>: Acts as a non-linear alternative to Principal Component Analysis (PCA).</li><li><strong>Feature Learning</strong>: Learns compact and meaningful representations of data.</li><li><strong>Denoising</strong>: Removes noise from corrupted data by training on clean samples.</li><li><strong>Anomaly Detection</strong>: Identifies unusual patterns by observing high reconstruction errors.</li></ol><p><strong>Variants:</strong></p><ul><li><strong>Sparse Autoencoders</strong>: Encourage sparsity in the hidden units to create compressed and interpretable features.</li><li><strong>Denoising Autoencoders</strong>: Add noise to inputs during training, forcing the network to learn robust features.</li><li><strong>Convolutional Autoencoders</strong>: Specialize in image data, leveraging convolutional layers for spatial feature learning.</li></ul><p><strong>Key References:</strong></p><ul><li><strong>Paper</strong>: <em>Efficient Learning of Sparse Representations with an Energy-Based Model, 2006.</em><ul><li><strong>DOI</strong>: 10.7551&#x2F;mitpress&#x2F;7503.003.0147</li><li><strong>Read on</strong>: <a href="https://cs.nyu.edu/~sumit/publications/assets/nips06.pdf">cs.nyu.edu</a></li></ul></li><li><strong>Paper</strong>: Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion, 2010.<br><strong>Read on</strong>: <a href="https://www.cse.fau.edu/~xqzhu/courses/cap5615/reading/autoencoder.pdf">cse.fau.edu</a></li></ul><hr><h3 id="Artificial-Neural-Networks-ANNs"><a href="#Artificial-Neural-Networks-ANNs" class="headerlink" title="Artificial Neural Networks (ANNs)"></a>Artificial Neural Networks (ANNs)</h3><p>Artificial Neural Networks (ANNs) are computational processing systems inspired by biological nervous systems (e.g., the human brain).</p><hr><h2 id="B"><a href="#B" class="headerlink" title="B"></a>B</h2><h3 id="Base-model"><a href="#Base-model" class="headerlink" title="Base model"></a>Base model</h3><p>The original, foundational version of a large language model, which has not been fine-tuned.</p><hr><h3 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h3><h2 id="Bidirectional-Encoder-Representations-from-Transformers-DEveloped-in-2018-BERT-is-designed-to-pre-train-deep-bidirectional-representations-from-unlabeled-text-by-jointly-conditioning-on-both-left-and-right-context-in-all-layers-As-a-result-the-pre-trained-BERT-model-can-be-fine-tuned-with-just-one-additional-output-layer-to-create-state-of-the-art-models-for-a-wide-range-of-tasks-such-as-question-answering-and-language-inference-without-substantial-task-specific-architecture-modifications-BERT-is-conceptually-simple-and-empirically-powerful-It-obtains-new-state-of-the-art-results-on-eleven-natural-language-processing-tasks-including-pushing-the-GLUE-score-to-80-5-7-7-point-absolute-improvement-MultiNLI-accuracy-to-86-7-4-6-absolute-improvement-SQuAD-v1-1-question-answering-Test-F1-to-93-2-1-5-point-absolute-improvement-and-SQuAD-v2-0-Test-F1-to-83-1-5-1-point-absolute-improvement-Paper-BERT-Pre-training-of-Deep-Bidirectional-Transformers-forLanguage-Understanding-2018-DOI-10-18653-v1-N19-1423-Read-on-arxiv-org"><a href="#Bidirectional-Encoder-Representations-from-Transformers-DEveloped-in-2018-BERT-is-designed-to-pre-train-deep-bidirectional-representations-from-unlabeled-text-by-jointly-conditioning-on-both-left-and-right-context-in-all-layers-As-a-result-the-pre-trained-BERT-model-can-be-fine-tuned-with-just-one-additional-output-layer-to-create-state-of-the-art-models-for-a-wide-range-of-tasks-such-as-question-answering-and-language-inference-without-substantial-task-specific-architecture-modifications-BERT-is-conceptually-simple-and-empirically-powerful-It-obtains-new-state-of-the-art-results-on-eleven-natural-language-processing-tasks-including-pushing-the-GLUE-score-to-80-5-7-7-point-absolute-improvement-MultiNLI-accuracy-to-86-7-4-6-absolute-improvement-SQuAD-v1-1-question-answering-Test-F1-to-93-2-1-5-point-absolute-improvement-and-SQuAD-v2-0-Test-F1-to-83-1-5-1-point-absolute-improvement-Paper-BERT-Pre-training-of-Deep-Bidirectional-Transformers-forLanguage-Understanding-2018-DOI-10-18653-v1-N19-1423-Read-on-arxiv-org" class="headerlink" title="Bidirectional Encoder Representations from Transformers. DEveloped in 2018.BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Paper:  - BERT: Pre-training of Deep Bidirectional Transformers forLanguage Understanding. 2018    - DOI: 10.18653&#x2F;v1&#x2F;N19-1423    - Read on arxiv.org  "></a>Bidirectional Encoder Representations from Transformers. DEveloped in 2018.<br>BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.<br>BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).<br><strong>Paper:</strong><br>  - BERT: Pre-training of Deep Bidirectional Transformers for<br>Language Understanding. 2018<br>    - DOI: <a href="https://doi.org/10.18653/v1%2FN19-1423">10.18653&#x2F;v1&#x2F;N19-1423</a><br>    - <a href="https://arxiv.org/pdf/1810.04805">Read on arxiv.org</a>  </h2><h3 id="BERTScore"><a href="#BERTScore" class="headerlink" title="BERTScore"></a>BERTScore</h3><ul><li><strong>Description</strong>: Leverages contextual embeddings from BERT to evaluate semantic similarity between the generated and reference text.</li><li><strong>Purpose</strong>: Captures semantic similarity more effectively than traditional n-gram-based metrics.</li></ul><table><thead><tr><th>Metric</th><th>Range</th><th>Interpretation</th><th>Example Values</th></tr></thead><tbody><tr><td><strong>BERTScore (Precision)</strong></td><td>0 to 1</td><td>Measures how much of the generated text aligns with the reference.</td><td>0.7 (moderate), 0.85 (good), 0.95 (excellent)</td></tr><tr><td><strong>BERTScore (Recall)</strong></td><td>0 to 1</td><td>Measures how much of the reference text is captured in the generated output.</td><td>0.6 (moderate), 0.8 (good), 0.9 (excellent)</td></tr><tr><td><strong>BERTScore (F1)</strong></td><td>0 to 1</td><td>Harmonic mean of precision and recall; overall measure of similarity.</td><td>0.65 (moderate), 0.82 (good), 0.9 (excellent)</td></tr></tbody></table><hr><h3 id="Bias"><a href="#Bias" class="headerlink" title="Bias"></a>Bias</h3><p>The disproportionate favor or prejudice towards a specific item or group. AI algorithms may inherit biases from historical data or human trainers, risking perpetuation of these biases in predictions.</p><hr><h3 id="BLEU"><a href="#BLEU" class="headerlink" title="BLEU"></a>BLEU</h3><p>BLEU (Bilingual Evaluation Understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. Quality is considered to be the correspondence between a machine’s output and that of a human: “the closer a machine translation is to a professional human translation, the better it is” – this is the central idea behind BLEU.<br>BLEU was one of the first metrics to claim a high correlation with human judgements of quality, and remains one of the most popular automated and inexpensive metrics.</p><ul><li>Paper: “BLEU: a Method for Automatic Evaluation of Machine Translation”. Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 311-318.<ul><li>DOI: 10.3115&#x2F;1073083.1073135</li><li>Read on <a href="https://aclanthology.org/P02-1040.pdf">aclanthology.org</a></li><li>Read on <a href="https://sci-hub.se/https://doi.org/10.3115/1073083.1073135">sci-hub.se</a></li></ul></li></ul><hr><h3 id="BLEU-score"><a href="#BLEU-score" class="headerlink" title="BLEU score"></a>BLEU score</h3><ul><li>BLEU scores are calculated for individual translated segments—generally sentences—by comparing them with a set of good quality reference translations. Those scores are then averaged over the whole corpus to reach an estimate of the translation’s overall quality. Neither intelligibility nor grammatical correctness are not taken into account.</li><li>The BLEU algorithm compares consecutive phrases of the automatic translation with the consecutive phrases it finds in the reference translation, and counts the number of matches, in a weighted fashion. These matches are position independent. A higher match degree indicates a higher degree of similarity with the reference translation, and higher score. Intelligibility and grammatical correctness aren’t taken into account.</li><li>web article: <a href="https://www.geeksforgeeks.org/nlp-bleu-score-for-evaluating-neural-machine-translation-python/#how-to-compute-bleu-score">“How to Compute BLEU Score”</a></li></ul><hr><h2 id="C"><a href="#C" class="headerlink" title="C"></a>C</h2><h3 id="Convolutional-layer"><a href="#Convolutional-layer" class="headerlink" title="Convolutional layer"></a>Convolutional layer</h3><p>A core component of CNNs that processes input data using filters (kernels) to produce feature maps, identifying patterns or features.</p><hr><h3 id="Convolutional-Neural-Network-CNN"><a href="#Convolutional-Neural-Network-CNN" class="headerlink" title="Convolutional Neural Network (CNN)"></a>Convolutional Neural Network (CNN)</h3><ol><li>CNN - a type of neural network specialized for analyzing visual data, learning features via filter optimization.</li><li>CNN - similar to ANNs but optimized for image data, with layers designed for feature extraction and classification.</li></ol><ul><li><a href="https://www.ibm.com/topics/convolutional-neural-networks">What are convolutional neural networks?</a></li></ul><hr><h2 id="D"><a href="#D" class="headerlink" title="D"></a>D</h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>The training data used to teach an LLM patterns and relationships.</p><hr><h2 id="F"><a href="#F" class="headerlink" title="F"></a>F</h2><h3 id="False-positive"><a href="#False-positive" class="headerlink" title="False positive"></a>False positive</h3><p>An incorrect prediction where a model identifies a condition or class that is not present.</p><hr><h3 id="F1-score"><a href="#F1-score" class="headerlink" title="F1 score"></a>F1 score</h3><p>A performance metric for classification models combining precision and recall into a single value ranging from 0 (poor) to 1 (excellent).</p><hr><h3 id="Few-shot"><a href="#Few-shot" class="headerlink" title="Few-shot"></a>Few-shot</h3><p>Using a small number of examples to guide the model in performing a new task.</p><hr><h3 id="Fine-tuning"><a href="#Fine-tuning" class="headerlink" title="Fine-tuning"></a>Fine-tuning</h3><p>Adapting a pre-trained model to a specific task or domain by training it on a smaller, specialized dataset.</p><hr><h2 id="G"><a href="#G" class="headerlink" title="G"></a>G</h2><h3 id="Generative-AI"><a href="#Generative-AI" class="headerlink" title="Generative AI"></a>Generative AI</h3><p>AI systems capable of creating new content, such as text, images, or audio.</p><hr><h2 id="H"><a href="#H" class="headerlink" title="H"></a>H</h2><h3 id="Hallucination"><a href="#Hallucination" class="headerlink" title="Hallucination"></a>Hallucination</h3><p>When an LLM generates plausible but factually incorrect or nonsensical information.</p><hr><h2 id="I"><a href="#I" class="headerlink" title="I"></a>I</h2><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p>The process of using a trained model to make predictions or generate outputs.</p><hr><h2 id="L"><a href="#L" class="headerlink" title="L"></a>L</h2><h3 id="LanguageTool"><a href="#LanguageTool" class="headerlink" title="LanguageTool"></a>LanguageTool</h3><p>LanguageTool is an AI-based grammar checker. Paste your text or start typing below to check grammatical errors, and spelling mistakes across languages.<br>Reference: <a href="https://languagetool.org/">LanguageTool</a></p><h3 id="LCS"><a href="#LCS" class="headerlink" title="LCS"></a>LCS</h3><p>LCS &#x3D; Longest Common Subsequence</p><hr><h3 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h3><p>Low-Rank Adaptation, a fine-tuning method requiring less computational resources compared to full fine-tuning.</p><hr><h2 id="M"><a href="#M" class="headerlink" title="M"></a>M</h2><h3 id="Machine-Learning-Operations-MLOps"><a href="#Machine-Learning-Operations-MLOps" class="headerlink" title="Machine Learning Operations (MLOps)"></a>Machine Learning Operations (MLOps)</h3><p>The process of deploying, monitoring, and updating machine learning models in production environments.</p><hr><h3 id="METEOR"><a href="#METEOR" class="headerlink" title="METEOR"></a>METEOR</h3><p>Metric. METEOR &#x3D; <strong>M</strong>etric for <strong>E</strong>valuation of <strong>T</strong>ranslation with <strong>E</strong>xplicit <strong>OR</strong>dering</p><ul><li><strong>Description</strong>: Evaluates semantic similarity by considering unigram overlaps, stemming, synonyms, and paraphrasing.</li><li><strong>Score Parameter</strong>: <code>METEOR Score</code>, <a href="#meteor-score">ref</a></li><li><strong>Purpose</strong>: Provides a balanced metric for machine translation and summarization tasks.</li></ul><h3 id="METEOR-Score"><a href="#METEOR-Score" class="headerlink" title="METEOR Score"></a>METEOR Score</h3><ul><li>Metric: <a href="#meteor">METEOR</a></li><li>Range: 0 to 1</li><li>Interpretation: Combines precision and recall, with semantic similarity (e.g., synonyms, stems)</li><li>Example Values: Higher scores indicate better matches.0.3 (low), 0.6 (moderate), 0.85 (high)</li></ul><hr><h3 id="MLOps"><a href="#MLOps" class="headerlink" title="MLOps"></a>MLOps</h3><p>Short for Machine Learning Operations.</p><hr><h2 id="N"><a href="#N" class="headerlink" title="N"></a>N</h2><h3 id="Named-Entity-Recognition"><a href="#Named-Entity-Recognition" class="headerlink" title="Named Entity Recognition"></a>Named Entity Recognition</h3><ul><li>Named Entity Recognition seeks to extract substrings within a text that name real-world objects and to determine their type (for example, whether they refer to persons or organizations).<br>Paper: A survey on recent advances in Named Entity Recognition, 2024.<br>Read on <a href="https://arxiv.org/html/2401.10825v1">arxiv.org</a></li><li>Named Entity Recognition (NER) is a sub-task of information extraction in Natural Language Processing (NLP) that classifies named entities into predefined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, and more.</li></ul><hr><h2 id="Neural-Process-Family"><a href="#Neural-Process-Family" class="headerlink" title="Neural Process Family"></a>Neural Process Family</h2><h2 id="Neural-Process-Family-NPF"><a href="#Neural-Process-Family-NPF" class="headerlink" title="Neural Process Family &#x3D; NPF"></a>Neural Process Family &#x3D; NPF</h2><h2 id="P"><a href="#P" class="headerlink" title="P"></a>P</h2><h3 id="Perplexity"><a href="#Perplexity" class="headerlink" title="Perplexity"></a>Perplexity</h3><p>A measure of how well a language model predicts a sample of text, with lower scores indicating better performance.</p><table><thead><tr><th>Range</th><th>Interpretation</th><th>Example Values</th></tr></thead><tbody><tr><td>1 to ∞</td><td>Measures the uncertainty of the model’s predictions. Lower perplexity indicates better performance. A perplexity of 1 means perfect predictions, while higher values indicate more uncertainty and worse performance.</td><td>20 (good), 50 (average), 200 (poor)</td></tr></tbody></table><hr><h3 id="Pooling-layers"><a href="#Pooling-layers" class="headerlink" title="Pooling layers"></a>Pooling layers</h3><p>Layers in CNNs designed to reduce the dimensionality of feature maps, lowering computational complexity.</p><hr><h3 id="Precision"><a href="#Precision" class="headerlink" title="Precision"></a>Precision</h3><p>A metric measuring the ratio of true positives to all predicted positives in a classification model.</p><hr><h3 id="Pre-trained-model"><a href="#Pre-trained-model" class="headerlink" title="Pre-trained model"></a>Pre-trained model</h3><p>A model that has been trained on a dataset and may be further fine-tuned for specific tasks.</p><hr><h3 id="Prompt-engineering"><a href="#Prompt-engineering" class="headerlink" title="Prompt engineering"></a>Prompt engineering</h3><p>The art of crafting effective inputs to elicit desired output from an LLM.</p><hr><h3 id="Prompt-template"><a href="#Prompt-template" class="headerlink" title="Prompt template"></a>Prompt template</h3><p>Specially formatted instructions used by LLMs to define the input and output.</p><hr><h1 id="R"><a href="#R" class="headerlink" title="R"></a>R</h1><h3 id="R-A-G-Retrieval-Augmented-Generation"><a href="#R-A-G-Retrieval-Augmented-Generation" class="headerlink" title="R.A.G. (Retrieval-Augmented Generation)"></a>R.A.G. (Retrieval-Augmented Generation)</h3><p>A technique combining external information retrieval with the generative capabilities of an LLM for improved accuracy.</p><hr><h3 id="ROUGE"><a href="#ROUGE" class="headerlink" title="ROUGE"></a>ROUGE</h3><p>ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</p><ul><li><strong>Score parameters</strong>:<ul><li><code>ROUGE-1</code>:Range: 0 to 1, Measures the overlap of unigram (single word) between the generated and reference text.0.2 (low overlap), 0.5 (moderate), 0.8 (high)</li><li><code>ROUGE-2</code>:Range: 0 to 1, Measures the overlap of bigrams (two consecutive words) between generated and reference text.0.1 (low), 0.4 (moderate), 0.7 (high)</li><li><code>ROUGE-L</code>:Range: 0 to 1, Measures the longest common subsequence (LCS) between the generated and reference text.0.3 (low), 0.6 (moderate), 0.9 (high)</li><li><code>ROUGE-Lsum</code> is specifically designed for summarization tasks, particularly when evaluating summaries. It computes the ROUGE-L score for the summarization task based on how well the generated summary matches the reference summary.</li></ul></li><li><strong>Purpose</strong>: Evaluates lexical similarity, fluency, and coherence across different levels (unigrams, bigrams, and sequences).<ul><li>Original ROUGE Paper: Lin, Chin-Yew. “ROUGE: A Package for Automatic Evaluation of Summaries”<ul><li>DOI: 10.3115&#x2F;1073083.1073135</li><li>Read on <a href="">ACL Anthology</a></li></ul></li></ul></li></ul><hr><h3 id="ROUGE-L"><a href="#ROUGE-L" class="headerlink" title="ROUGE-L"></a>ROUGE-L</h3><h2 id="ROUGE-L-ROUGE-Recall-Oriented-Understudy-for-Gisting-Evaluation-L-Longest-Common-Subsequence-LCS-ROUGE-L-captures-the-longest-sequence-of-words-that-appear-in-both-texts-in-the-same-order-providing-insights-into-fluency-and-coherence-Paper-“Automatic-Evaluation-of-Machine-Translation-Quality-Using-Longest-Common-Subsequence-and-Skip-Bigram-Statistics-”-Lin-Chin-Yew-DOI-10-3115-1218955-1219032-Read-on-sci-hub-se"><a href="#ROUGE-L-ROUGE-Recall-Oriented-Understudy-for-Gisting-Evaluation-L-Longest-Common-Subsequence-LCS-ROUGE-L-captures-the-longest-sequence-of-words-that-appear-in-both-texts-in-the-same-order-providing-insights-into-fluency-and-coherence-Paper-“Automatic-Evaluation-of-Machine-Translation-Quality-Using-Longest-Common-Subsequence-and-Skip-Bigram-Statistics-”-Lin-Chin-Yew-DOI-10-3115-1218955-1219032-Read-on-sci-hub-se" class="headerlink" title="ROUGE-L &#x3D; ROUGE (Recall-Oriented Understudy for Gisting Evaluation) + L (Longest Common Subsequence (LCS)ROUGE-L captures the longest sequence of words that appear in both texts in the same order, providing insights into fluency and coherence.  - Paper: “Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics.” Lin, Chin-Yew.    - DOI: 10.3115&#x2F;1218955.1219032    - Read on sci-hub.se "></a>ROUGE-L &#x3D; ROUGE (Recall-Oriented Understudy for Gisting Evaluation) + L (Longest Common Subsequence (<a href="#LCS">LCS</a>)<br>ROUGE-L captures the longest sequence of words that appear in both texts in the same order, providing insights into fluency and coherence.<br>  - Paper: “Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics.” Lin, Chin-Yew.<br>    - DOI: 10.3115&#x2F;1218955.1219032<br>    - Read on <a href="https://sci-hub.se/10.3115/1218955.1219032">sci-hub.se</a> </h2><h2 id="S"><a href="#S" class="headerlink" title="S"></a>S</h2><h3 id="Special-Token"><a href="#Special-Token" class="headerlink" title="Special Token"></a>Special Token</h3><p>Reserved tokens used in LLMs for specific functions, such as defining the start or end of a response.</p><hr><h2 id="Softmax-function"><a href="#Softmax-function" class="headerlink" title="Softmax function"></a>Softmax function</h2><h2 id="“Softmax-function”-reaed-on-notes-theomorales-com"><a href="#“Softmax-function”-reaed-on-notes-theomorales-com" class="headerlink" title="“Softmax function” reaed on notes.theomorales.com"></a><a href="https://notes.theomorales.com/Attention+is+all+you+need/The+Softmax+function">“Softmax function” reaed on notes.theomorales.com</a></h2><h3 id="Supervised-learning"><a href="#Supervised-learning" class="headerlink" title="Supervised learning"></a>Supervised learning</h3><p>Learning through pre-labeled inputs, where the model aims to reduce classification error by predicting the correct outputs.</p><hr><h3 id="System-prompts"><a href="#System-prompts" class="headerlink" title="System prompts"></a>System prompts</h3><p>Instructions defining an LLM’s behavior, role, or context in a conversation.</p><hr><h2 id="T"><a href="#T" class="headerlink" title="T"></a>T</h2><h3 id="Token"><a href="#Token" class="headerlink" title="Token"></a>Token</h3><p>The smallest unit of text processed by an LLM, such as a word or subword.</p><hr><h3 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h3><p>Breaking text into tokens for model processing.</p><hr><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>Feeding a model with data to allow it to learn patterns and relationships.</p><hr><h2 id="U"><a href="#U" class="headerlink" title="U"></a>U</h2><h3 id="Unsupervised-learning"><a href="#Unsupervised-learning" class="headerlink" title="Unsupervised learning"></a>Unsupervised learning</h3><p>Learning from data without labeled outputs, often for tasks like clustering or dimensionality reduction.</p><hr><h2 id="V"><a href="#V" class="headerlink" title="V"></a>V</h2><h3 id="Variational-Autoencoders-VAEs"><a href="#Variational-Autoencoders-VAEs" class="headerlink" title="Variational Autoencoders (VAEs)"></a>Variational Autoencoders (VAEs)</h3><p>Variational Autoencoders (VAEs) are a probabilistic extension of autoencoders that learn not only to compress data but also to generate new samples by modeling data distributions. VAEs use a latent space with a probabilistic structure, enabling meaningful interpolation between points in the latent space.<br><strong>Features:</strong></p><ol><li><strong>Latent Space Regularization</strong>: Ensures the latent space follows a predefined probability distribution, commonly Gaussian.</li><li><strong>Reconstruction and Generation</strong>: Balances reconstruction accuracy with the regularization term using a loss function derived from the evidence lower bound (ELBO).</li><li><strong>Bayesian Interpretation</strong>: The encoding process approximates posterior distributions via variational inference.</li></ol><p><strong>Applications:</strong></p><ul><li><strong>Data Generation</strong>: Generate novel and coherent samples (e.g., synthetic images, text).</li><li><strong>Anomaly Detection</strong>: Identifies data points that deviate from the learned distribution.</li><li><strong>Latent Space Manipulation</strong>: Enables interpolation and arithmetic operations in the latent space.</li></ul><p><strong>Key References:</strong></p><ul><li><strong>Title</strong>: “Auto-Encoding Variational Bayes”<br><strong>DOI</strong>: <a href="https://doi.org/10.48550/arXiv.1312.6114">10.48550&#x2F;arXiv.1312.6114</a></li><li><strong>Title</strong>: “Variational Inference with Normalizing Flows”<br><strong>DOI</strong>: <a href="https://doi.org/10.48550/arXiv.1505.05770">10.48550&#x2F;arXiv.1505.05770</a></li></ul><hr><h2 id="Z"><a href="#Z" class="headerlink" title="Z"></a>Z</h2><h3 id="Zero-padding-CNN"><a href="#Zero-padding-CNN" class="headerlink" title="Zero-padding (CNN)"></a>Zero-padding (CNN)</h3><p>A technique in CNNs that pads the borders of input data with zeros, controlling output dimensionality.</p><hr><h3 id="Zero-shot"><a href="#Zero-shot" class="headerlink" title="Zero-shot"></a>Zero-shot</h3><p>A model’s ability to perform tasks it was not explicitly trained for by leveraging general knowledge.</p><hr><p><strong>Footnotes</strong></p><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://github.com/nomic-ai/gpt4all/wiki/Generative-AI-Terminology">https://github.com/nomic-ai/gpt4all/wiki/Generative-AI-Terminology</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://nhsx.github.io/ai-dictionary">https://nhsx.github.io/ai-dictionary</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;&lt;em&gt;Feel free to send me your thoughts</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="AI" scheme="https://ooge0.github.io/hexo-blog/tags/AI/"/>
    
    <category term="ML" scheme="https://ooge0.github.io/hexo-blog/tags/ML/"/>
    
    <category term="glossary" scheme="https://ooge0.github.io/hexo-blog/tags/glossary/"/>
    
  </entry>
  
  <entry>
    <title>Online tools</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/18/notes/online_tools/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/18/notes/online_tools/</id>
    <published>2024-11-17T22:00:00.000Z</published>
    <updated>2024-11-18T17:52:11.042Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="Text"><a href="#Text" class="headerlink" title="Text"></a>Text</h2><h2 id="Text-editing"><a href="#Text-editing" class="headerlink" title="Text editing"></a>Text editing</h2><ol><li><a href="https://convertcase.net/">https://convertcase.net/</a></li></ol><h2 id="Text-comparison"><a href="#Text-comparison" class="headerlink" title="Text comparison"></a>Text comparison</h2><ol><li><a href="https://text-compare.com/">https://text-compare.com/</a></li></ol><h2 id="OCR"><a href="#OCR" class="headerlink" title="OCR"></a>OCR</h2><ol><li>Text from boofer&#x2F;pictures: <a href="http://www.structurise.com/screenshot-ocr/">http://www.structurise.com/screenshot-ocr/</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h2 id=&quot;Text&quot;&gt;&lt;a href=&quot;#Text&quot; class=&quot;head</summary>
      
    
    
    
    <category term="Notes" scheme="https://ooge0.github.io/hexo-blog/categories/Notes/"/>
    
    
    <category term="my_contribution" scheme="https://ooge0.github.io/hexo-blog/tags/my-contribution/"/>
    
    <category term="tools" scheme="https://ooge0.github.io/hexo-blog/tags/tools/"/>
    
    <category term="online_tools" scheme="https://ooge0.github.io/hexo-blog/tags/online-tools/"/>
    
  </entry>
  
  <entry>
    <title>Comparing Efficiency of NLP Models. Methods and Metrics</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/17/post_ai__comparing_nlp_models_methods_metrix/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/17/post_ai__comparing_nlp_models_methods_metrix/</id>
    <published>2024-11-17T19:39:11.000Z</published>
    <updated>2024-11-17T20:40:14.655Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>When comparing the efficiency of NLP models, it is essential to use standardized approaches, metrics, and parameters to ensure a fair and comprehensive evaluation. Below is a structured guide.</p><hr><h2 id="1-Approaches-for-Evaluating-NLP-Models"><a href="#1-Approaches-for-Evaluating-NLP-Models" class="headerlink" title="1. Approaches for Evaluating NLP Models"></a>1. Approaches for Evaluating NLP Models</h2><ol><li><strong>Task-Specific Evaluation</strong>: Measure performance on specific NLP tasks (e.g., sentiment analysis, named entity recognition, machine translation).</li><li><strong>Benchmark Datasets</strong>: Use well-known datasets like <a href="https://huggingface.co/datasets/nyu-mll/glue">GLUE</a><sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="GLUE - The General Language Understanding Evaluation (GLUE) benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems. ">[1]</span></a></sup>, <a href="https://super.gluebenchmark.com/">SuperGLUE</a><sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="SuperGLUE is a benchmark dataset designed to pose a more rigorous test of language understanding than GLUE. ">[2]</span></a></sup>, <a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD</a><sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="SQuAD - Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.">[3]</span></a></sup>, or <a href="https://huggingface.co/wmt">WMT</a><sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="WMT: Workshop on Statistical Machine Translation focuses on news text translation. It includes language pairs such as English to/from various languages like Chinese, Czech, German, Hausa, Icelandic, Japanese, Russian, and more.">[4]</span></a></sup> for standardized comparisons.</li><li><strong>Ablation Studies</strong>: Analyze the impact of model components by systematically removing or modifying parts of the model.</li><li><strong>Scalability and Efficiency Testing</strong>:<ul><li>Test for performance across different dataset sizes.</li><li>Evaluate computational efficiency (e.g., inference speed, training time).</li></ul></li><li><strong>Generalization and Robustness</strong>:<ul><li>Test on out-of-distribution data or adversarial examples.</li><li>Use cross-lingual or domain-specific datasets.</li></ul></li></ol><hr><h2 id="2-Relevant-Metrics"><a href="#2-Relevant-Metrics" class="headerlink" title="2. Relevant Metrics"></a>2. Relevant Metrics</h2><h3 id="A-Task-Specific-Metrics"><a href="#A-Task-Specific-Metrics" class="headerlink" title="A. Task-Specific Metrics"></a>A. Task-Specific Metrics</h3><ol><li><strong>Classification Tasks</strong>:<ul><li>Accuracy</li><li>Precision</li><li>Recall </li><li>F1-Score</li><li>Area Under the Curve (AUC) for ROC&#x2F;PR curves</li></ul></li><li><strong>Text Generation Tasks</strong>:<ul><li>BLEU (Bilingual Evaluation Understudy)</li><li>ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</li><li>METEOR (Metric for Evaluation of Translation with Explicit ORdering)</li></ul></li><li><strong>Question Answering</strong>:<ul><li>Exact Match (EM)</li><li>F1-Score</li></ul></li><li><strong>Language Modeling</strong>:<ul><li>Perplexity</li><li>Bits-per-character (BPC)</li></ul></li><li><strong>Named Entity Recognition (NER)</strong>:<ul><li>F1-Score for Entity-Level Precision&#x2F;Recall</li></ul></li><li><strong>Text Summarization</strong>:<ul><li>ROUGE-1, ROUGE-2, ROUGE-L</li></ul></li></ol><h3 id="B-Efficiency-Metrics"><a href="#B-Efficiency-Metrics" class="headerlink" title="B. Efficiency Metrics"></a>B. Efficiency Metrics</h3><ol><li><strong>Computational Efficiency</strong>:<ul><li>FLOPs (Floating Point Operations per Second)</li><li>Latency (time per inference)</li><li>Training Time</li></ul></li><li><strong>Memory Usage</strong>:<ul><li>GPU&#x2F;CPU memory requirements</li><li>Model size (in MB or parameters)</li></ul></li><li><strong>Energy Consumption</strong>:<ul><li>Energy usage during training&#x2F;inference</li></ul></li></ol><hr><h2 id="3-Parameters-for-Testing"><a href="#3-Parameters-for-Testing" class="headerlink" title="3. Parameters for Testing"></a>3. Parameters for Testing</h2><ol><li><strong>Model Parameters</strong>:<ul><li>Number of layers</li><li>Hidden size</li><li>Attention heads</li></ul></li><li><strong>Dataset Characteristics</strong>:<ul><li>Dataset size</li><li>Distribution (balanced vs. imbalanced classes)</li><li>Language&#x2F;domain</li></ul></li><li><strong>Hyperparameters</strong>:<ul><li>Learning rate</li><li>Batch size</li><li>Dropout rate</li><li>Optimization algorithm (e.g., AdamW, SGD)</li></ul></li><li><strong>Infrastructure</strong>:<ul><li>Hardware (e.g., GPU, TPU, or CPU)</li><li>Software (e.g., TensorFlow, PyTorch)</li></ul></li></ol><hr><h2 id="4-Reusability-for-Variable-Changes"><a href="#4-Reusability-for-Variable-Changes" class="headerlink" title="4. Reusability for Variable Changes"></a>4. Reusability for Variable Changes</h2><ol><li><strong>Modular Code</strong>: Ensure model code allows for easy swapping of components (e.g., tokenizer, embedding layer).</li><li><strong>Parameterization</strong>:<ul><li>Use configuration files (YAML&#x2F;JSON) to define model parameters and settings.</li></ul></li><li><strong>Reproducibility</strong>:<ul><li>Log training runs using tools like TensorBoard, Weights &amp; Biases, or MLflow.</li><li>Fix random seeds for deterministic results.</li></ul></li><li><strong>Automated Testing</strong>:<ul><li>Implement pipelines to rerun experiments with new variables.</li></ul></li></ol><hr><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>The comparison of NLP models requires a multifaceted approach, evaluating both task performance and computational efficiency. Selecting appropriate metrics and parameters ensures comprehensive insights into model strengths and weaknesses. By maintaining modularity and automation, reusability across experiments is simplified, enabling iterative improvements and robust testing.</p><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">GLUE - The General Language Understanding Evaluation (GLUE) benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">SuperGLUE is a benchmark dataset designed to pose a more rigorous test of language understanding than GLUE.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">SQuAD - Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">WMT: Workshop on Statistical Machine Translation focuses on news text translation. It includes language pairs such as English to/from various languages like Chinese, Czech, German, Hausa, Icelandic, Japanese, Russian, and more.<a href="#fnref:4" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;When comparing the efficiency of NLP m</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="AI" scheme="https://ooge0.github.io/hexo-blog/tags/AI/"/>
    
    <category term="ML" scheme="https://ooge0.github.io/hexo-blog/tags/ML/"/>
    
    <category term="NLP" scheme="https://ooge0.github.io/hexo-blog/tags/NLP/"/>
    
  </entry>
  
</feed>
