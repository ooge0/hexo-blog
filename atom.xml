<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title> </title>
  
  <subtitle>...chasing dreams, living reality</subtitle>
  <link href="https://ooge0.github.io/hexo-blog/atom.xml" rel="self"/>
  
  <link href="https://ooge0.github.io/hexo-blog/"/>
  <updated>2024-12-13T07:33:16.529Z</updated>
  <id>https://ooge0.github.io/hexo-blog/</id>
  
  <author>
    <name>si0n4ra</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Windows OS apps</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/12/13/post_os__windows%20app/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/12/13/post_os__windows%20app/</id>
    <published>2024-12-13T07:18:22.000Z</published>
    <updated>2024-12-13T07:33:16.529Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="Monitorian"><a href="#Monitorian" class="headerlink" title="Monitorian"></a>Monitorian</h2><ul><li><strong>Monitorian app description</strong>: Monitorian is a desktop tool to adjust the brightness of multiple monitors with ease. The user can change the brightness of monitors, including external ones, either individually or in unison. In addition, the user can change the adjustable range of brightness and contrast for each monitor seamlessly.</br>To control an external monitor, the monitor must be DDC/CI compatible and the function enabled. If a monitor is connected through an converter, docking station or other device, such a device must be also compatible.</li><li><a href="https://github.com/emoacht/Monitorian">Monitorian home page</a></li><li><a href="https://apps.microsoft.com/detail/9nw33j738bl0?hl=en-GB&gl=UA">Monitorian Microsoft store download page</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h2 id=&quot;Monitorian&quot;&gt;&lt;a href=&quot;#Monitorian&quot;</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="apps" scheme="https://ooge0.github.io/hexo-blog/tags/apps/"/>
    
    <category term="windows_os" scheme="https://ooge0.github.io/hexo-blog/tags/windows-os/"/>
    
  </entry>
  
  <entry>
    <title>Text Generation Coherence vs. Text Generation Quality</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/12/12/post_linquistic__text_coherence_vs_text_quiality/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/12/12/post_linquistic__text_coherence_vs_text_quiality/</id>
    <published>2024-12-12T09:21:11.000Z</published>
    <updated>2024-12-12T10:54:20.189Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="1-Text-Generation-Coherence"><a href="#1-Text-Generation-Coherence" class="headerlink" title="1. Text Generation Coherence"></a>1. Text Generation Coherence</h2><p>Coherence refers to the logical and structural flow of the text. A coherent text feels connected and makes sense as a whole. It ensures that:</p><ul><li><strong>Logical Progression</strong>: Sentences and paragraphs follow each other in a natural order.</li><li><strong>Topic Consistency</strong>: The generated text remains focused on a single theme or idea.</li><li><strong>Contextual Relevance</strong>: Each part of the text relates appropriately to the previous and following parts.</li><li><strong>Absence of Contradictions</strong>: There are no logical inconsistencies or contradictory statements.</li></ul><h3 id="Example"><a href="#Example" class="headerlink" title="Example:"></a>Example:</h3><ul><li><p><strong>Coherent Text</strong>:</p><blockquote><p>“The sun was shining brightly in the clear blue sky. Birds chirped as children played in the park. It was a perfect day for a picnic.”</p></blockquote><ul><li>This is coherent because the sentences are connected and describe a single scene.</li></ul></li><li><p><strong>Incoherent Text</strong>:</p><blockquote><p>“The sun was shining. Suddenly, a spaceship landed in the park. Watermelons are tasty.”</p></blockquote><ul><li>This lacks coherence due to abrupt topic shifts and lack of logical flow.</li></ul></li></ul><p><strong>Key Challenge:</strong> Ensuring the generated text flows smoothly across different parts without jumping topics or introducing unrelated ideas.</p><hr><h2 id="2-Text-Generation-Quality"><a href="#2-Text-Generation-Quality" class="headerlink" title="2. Text Generation Quality"></a>2. Text Generation Quality</h2><p>Quality refers to how well the generated text meets overall standards of good writing. It is a broader measure that includes:</p><ul><li><strong>Grammar and Syntax</strong>: Free of grammatical errors and awkward sentence structures.</li><li><strong>Vocabulary Use</strong>: Appropriate word choice, richness of language, and precision.</li><li><strong>Creativity and Style</strong>: Ability to generate text that is engaging, varied, and stylistically appropriate for the context.</li><li><strong>Correctness</strong>: Facts and information presented are accurate.</li><li><strong>Relevance</strong>: Addresses the input or prompt directly and completely.</li></ul><h3 id="Example-1"><a href="#Example-1" class="headerlink" title="Example:"></a>Example:</h3><ul><li><p><strong>High-Quality Text</strong>:</p><blockquote><p>“Artificial intelligence has revolutionized many industries, offering unparalleled efficiency and innovation. From healthcare to finance, its impact is profound.”</p></blockquote><ul><li>This text is grammatically correct, well-structured, and relevant.</li></ul></li><li><p><strong>Low-Quality Text</strong>:</p><blockquote><p>“Artificial intelligance revolution in industries many, efficiently innovation offering. Healthcare finance profound impact is.”</p></blockquote><ul><li>This text is grammatically incorrect and poorly structured.</li></ul></li></ul><p><strong>Key Challenge:</strong> Balancing creativity, accuracy, and linguistic correctness to produce engaging and relevant content.</p><hr><h2 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h2><table><thead><tr><th><strong>Aspect</strong></th><th><strong>Text Generation Coherence</strong></th><th><strong>Text Generation Quality</strong></th></tr></thead><tbody><tr><td><strong>Focus</strong></td><td>Logical flow and connection between parts of the text.</td><td>Overall writing standards, including grammar, style, and relevance.</td></tr><tr><td><strong>Scope</strong></td><td>Specific to structural and contextual alignment.</td><td>Broader, encompassing coherence, grammar, creativity, etc.</td></tr><tr><td><strong>Evaluation</strong></td><td>Assesses transitions, logical progression, and focus.</td><td>Evaluates overall effectiveness, correctness, and engagement.</td></tr><tr><td><strong>Common Issues</strong></td><td>Topic shifts, contradictions, lack of context.</td><td>Grammatical errors, awkward phrasing, irrelevance.</td></tr></tbody></table><hr><h2 id="Real-World-Importance"><a href="#Real-World-Importance" class="headerlink" title="Real-World Importance"></a>Real-World Importance</h2><ul><li><strong>Coherence</strong> is critical for tasks requiring deep contextual understanding, such as writing long essays or technical documents.</li><li><strong>Quality</strong> is essential for creating polished and professional output, such as marketing content or customer communications.</li></ul><blockquote><p>While coherence is a subset of quality, high-quality text must always be coherent. However, coherent text might not necessarily be high-quality if it lacks creativity, relevance, or stylistic finesse.</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h2 id=&quot;1-Text-Generation-Coherence&quot;&gt;&lt;a h</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="NLP" scheme="https://ooge0.github.io/hexo-blog/tags/NLP/"/>
    
    <category term="linguistic" scheme="https://ooge0.github.io/hexo-blog/tags/linguistic/"/>
    
  </entry>
  
  <entry>
    <title>Fine-Tuning vs. Training Models for Specific Tasks</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/12/10/post_ai_ml_basiscs__fune_tuning_vs_training_models_for_specific_tasks/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/12/10/post_ai_ml_basiscs__fune_tuning_vs_training_models_for_specific_tasks/</id>
    <published>2024-12-10T17:30:33.000Z</published>
    <updated>2024-12-10T17:30:22.717Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Fine-tuning and training are two common approaches to adapting machine learning models for specific tasks. While training involves building a model from scratch or pre-trained weights, fine-tuning adapts an already trained model to perform well on a particular task. Below, we explore the nuances of these approaches for various popular architectures, including FFNs, CNNs, LSTMs, and Transformers.</p><hr><h2 id="Key-differences-between-fine-tuning-and-training"><a href="#Key-differences-between-fine-tuning-and-training" class="headerlink" title="Key differences between fine-tuning and training"></a>Key differences between fine-tuning and training</h2><table><thead><tr><th>Aspect</th><th>Fine-Tuning</th><th>Training</th></tr></thead><tbody><tr><td><strong>Starting Point</strong></td><td>Pre-trained model weights from a related task (e.g., ImageNet for vision, GPT for text).</td><td>Randomly initialized weights.</td></tr><tr><td><strong>Data Requirements</strong></td><td>Requires less data; leverages knowledge from the pre-trained model.</td><td>Requires large amounts of labeled data.</td></tr><tr><td><strong>Training Duration</strong></td><td>Generally shorter; only adjusts some layers.</td><td>Longer; all parameters are learned from scratch.</td></tr><tr><td><strong>Resource Needs</strong></td><td>Less compute-intensive due to reduced training scope.</td><td>High resource demands for extensive training.</td></tr><tr><td><strong>Use Case</strong></td><td>When data is limited or computational resources are constrained.</td><td>When custom architecture or large task-specific datasets are available.</td></tr></tbody></table><hr><h2 id="Baselines-for-fine-tuning-training"><a href="#Baselines-for-fine-tuning-training" class="headerlink" title="Baselines for fine-tuning&#x2F;training"></a>Baselines for fine-tuning&#x2F;training</h2><h3 id="1-Feedforward-Networks-FFNs"><a href="#1-Feedforward-Networks-FFNs" class="headerlink" title="1. Feedforward Networks (FFNs)"></a>1. Feedforward Networks (FFNs)</h3><ul><li><strong>Baseline</strong>: Fully connected layers with non-linear activations.</li><li><strong>Fine-Tuning</strong>: Modify output layers to match the target task and optionally freeze earlier layers.</li><li><strong>Training</strong>: Start with random initialization, requiring carefully tuned learning rates and weight initialization methods.</li></ul><h3 id="2-Convolutional-Neural-Networks-CNNs"><a href="#2-Convolutional-Neural-Networks-CNNs" class="headerlink" title="2. Convolutional Neural Networks (CNNs)"></a>2. Convolutional Neural Networks (CNNs)</h3><ul><li><strong>Baseline</strong>: Architectures like VGG, ResNet, or EfficientNet, pre-trained on ImageNet.</li><li><strong>Fine-Tuning</strong>: Replace the classification head to match target classes, often freezing earlier convolutional layers.</li><li><strong>Training</strong>: Train from scratch if the task involves entirely different visual domains (e.g., medical imaging).</li></ul><h3 id="3-Recurrent-Neural-Networks-RNNs-and-Variants-LSTMs-GRUs"><a href="#3-Recurrent-Neural-Networks-RNNs-and-Variants-LSTMs-GRUs" class="headerlink" title="3. Recurrent Neural Networks (RNNs) and Variants (LSTMs, GRUs)"></a>3. Recurrent Neural Networks (RNNs) and Variants (LSTMs, GRUs)</h3><ul><li><strong>Baseline</strong>: Pre-trained word embeddings (e.g., GloVe, Word2Vec) or models like ELMo.</li><li><strong>Fine-Tuning</strong>: Use pre-trained embeddings, fine-tune the LSTM layers on sequential tasks.</li><li><strong>Training</strong>: Train an LSTM network from scratch for language modeling or time-series tasks.</li></ul><h3 id="4-Transformers"><a href="#4-Transformers" class="headerlink" title="4. Transformers"></a>4. Transformers</h3><ul><li><strong>Baseline</strong>: Models like BERT, GPT, or T5.</li><li><strong>Fine-Tuning</strong>: Modify the decoder head, adjust hyperparameters like learning rate and layer freezing.</li><li><strong>Training</strong>: Start with pre-trained embeddings; for unique tasks, initialize the Transformer from scratch (high compute).</li></ul><hr><h2 id="Technical-details-of-popular-architectures"><a href="#Technical-details-of-popular-architectures" class="headerlink" title="Technical details of popular architectures"></a>Technical details of popular architectures</h2><table><thead><tr><th>Model</th><th>Baseline Task</th><th>Fine-Tuning Scope</th><th>Training Considerations</th></tr></thead><tbody><tr><td><strong>VGG16</strong></td><td>Image Classification (ImageNet)</td><td>Replace final dense layer, freeze initial layers.</td><td>High memory usage, simpler architecture to optimize.</td></tr><tr><td><strong>ResNet50</strong></td><td>Image Classification (ImageNet)</td><td>Adjust classification head; fine-tune deeper layers as needed.</td><td>Skip connections improve gradient flow.</td></tr><tr><td><strong>BERT</strong></td><td>Masked Language Model (MLM)</td><td>Modify for classification, QA, or summarization.</td><td>Pre-training requires MLM objectives.</td></tr><tr><td><strong>GPT-3</strong></td><td>Text Generation</td><td>Fine-tune specific tasks by updating decoder head.</td><td>Requires extensive GPU resources for full training.</td></tr><tr><td><strong>LSTM</strong></td><td>Sequential Data Modeling</td><td>Fine-tune on embeddings, adjust for target sequence length.</td><td>Long training times due to sequential processing.</td></tr><tr><td><strong>EfficientNet</strong></td><td>Image Classification</td><td>Replace head; scale input resolution for task-specific datasets.</td><td>Compound scaling optimizes trade-offs in performance.</td></tr></tbody></table><hr><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Fine-tuning and training models depend heavily on the task, available data, and computational resources. Fine-tuning is generally faster and resource-efficient, making it ideal for adapting large pre-trained models to specific tasks. On the other hand, training from scratch offers flexibility when creating custom architectures but demands extensive data and compute. By understanding these approaches, practitioners can select the most suitable method for their applications.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;Fine-tuning and training are two commo</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="ML" scheme="https://ooge0.github.io/hexo-blog/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Comparison of popular models and architectures</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/12/10/post_ai_nn__comparison_of_popular_models_and_architectures/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/12/10/post_ai_nn__comparison_of_popular_models_and_architectures/</id>
    <published>2024-12-10T17:30:33.000Z</published>
    <updated>2024-12-13T07:38:16.744Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Related to:</p><ul><li><a href="../../../../post_ai_nn__list_of_neural_network_models_architectures_and_basic_components">List of neural network models, architectures, and basic components</a></li></ul><hr><h2 id="Table-Single-models-with-options"><a href="#Table-Single-models-with-options" class="headerlink" title="Table  - Single models with options"></a>Table  - Single models with options</h2><h1 id="Detailed-Breakdown-of-Popular-Models-and-Architectures"><a href="#Detailed-Breakdown-of-Popular-Models-and-Architectures" class="headerlink" title="Detailed Breakdown of Popular Models and Architectures"></a>Detailed Breakdown of Popular Models and Architectures</h1><h2 id="Autoencoders"><a href="#Autoencoders" class="headerlink" title="Autoencoders"></a>Autoencoders</h2><table><thead><tr><th><strong>Category</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td><strong>Basic Components</strong></td><td>Encoder, Decoder, Latent Space</td></tr><tr><td><strong>Open-Source&#x2F; Forbidden</strong></td><td>Open-Source</td></tr><tr><td><strong>Criteria of Measuring Productivity</strong></td><td>Reconstruction Loss</td></tr><tr><td><strong>Model’s Parameters</strong></td><td>Weights (Changeable), Latent Dimensions (Fixed)</td></tr><tr><td><strong>Criteria of Measuring Parameter’s Productivity</strong></td><td>Reconstruction Accuracy</td></tr><tr><td><strong>Model’s Hyperparameters</strong></td><td>Learning Rate, Latent Dimension Size (Changeable)</td></tr><tr><td><strong>Criteria of Measuring Hyperparameter’s Productivity</strong></td><td>Lower Reconstruction Error</td></tr><tr><td><strong>Basic Components of Hyperparameter’s Productivity</strong></td><td>Effective Latent Space Size, Training Convergence Rate</td></tr></tbody></table><h2 id="CNN-Convolutional-Neural-Networks"><a href="#CNN-Convolutional-Neural-Networks" class="headerlink" title="CNN (Convolutional Neural Networks)"></a>CNN (Convolutional Neural Networks)</h2><table><thead><tr><th><strong>Category</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td><strong>Basic Components</strong></td><td>Convolution Layers, Pooling, Fully Connected Layers</td></tr><tr><td><strong>Open-Source&#x2F; Forbidden</strong></td><td>Open-Source</td></tr><tr><td><strong>Criteria of Measuring Productivity</strong></td><td>Accuracy, Precision, Recall, F1-Score</td></tr><tr><td><strong>Model’s Parameters</strong></td><td>Filter Weights (Changeable), Input Channels (Fixed)</td></tr><tr><td><strong>Criteria of Measuring Parameter’s Productivity</strong></td><td>Detection Accuracy</td></tr><tr><td><strong>Model’s Hyperparameters</strong></td><td>Kernel Size, Stride, Number of Filters (Changeable)</td></tr><tr><td><strong>Criteria of Measuring Hyperparameter’s Productivity</strong></td><td>Higher Feature Extraction Quality</td></tr><tr><td><strong>Basic Components of Hyperparameter’s Productivity</strong></td><td>Filter Efficiency, Computational Cost</td></tr></tbody></table><h2 id="RNN-Recurrent-Neural-Networks"><a href="#RNN-Recurrent-Neural-Networks" class="headerlink" title="RNN (Recurrent Neural Networks)"></a>RNN (Recurrent Neural Networks)</h2><table><thead><tr><th><strong>Category</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td><strong>Basic Components</strong></td><td>Recurrent Layers, Activation Functions</td></tr><tr><td><strong>Open-Source&#x2F; Forbidden</strong></td><td>Open-Source</td></tr><tr><td><strong>Criteria of Measuring Productivity</strong></td><td>Perplexity, Accuracy, BLEU Score (NLP)</td></tr><tr><td><strong>Model’s Parameters</strong></td><td>Hidden State (Changeable), Sequence Length (Fixed)</td></tr><tr><td><strong>Criteria of Measuring Parameter’s Productivity</strong></td><td>Temporal Pattern Capture Efficiency</td></tr><tr><td><strong>Model’s Hyperparameters</strong></td><td>Learning Rate, Hidden State Size (Changeable)</td></tr><tr><td><strong>Criteria of Measuring Hyperparameter’s Productivity</strong></td><td>Sequence Learning Performance</td></tr><tr><td><strong>Basic Components of Hyperparameter’s Productivity</strong></td><td>Effective Sequence Memory Size</td></tr></tbody></table><h2 id="LSTM-Long-Short-Term-Memory-Networks"><a href="#LSTM-Long-Short-Term-Memory-Networks" class="headerlink" title="LSTM (Long Short-Term Memory Networks)"></a>LSTM (Long Short-Term Memory Networks)</h2><table><thead><tr><th><strong>Category</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td><strong>Basic Components</strong></td><td>LSTM Cells (Input, Forget, Output Gates)</td></tr><tr><td><strong>Open-Source&#x2F; Forbidden</strong></td><td>Open-Source</td></tr><tr><td><strong>Criteria of Measuring Productivity</strong></td><td>Perplexity, Accuracy (Time-Series, NLP)</td></tr><tr><td><strong>Model’s Parameters</strong></td><td>Cell Weights (Changeable), Memory Cell (Fixed)</td></tr><tr><td><strong>Criteria of Measuring Parameter’s Productivity</strong></td><td>Long-Term Dependency Capture Efficiency</td></tr><tr><td><strong>Model’s Hyperparameters</strong></td><td>Learning Rate, Number of Layers (Changeable)</td></tr><tr><td><strong>Criteria of Measuring Hyperparameter’s Productivity</strong></td><td>Retention of Long-Term Dependencies</td></tr><tr><td><strong>Basic Components of Hyperparameter’s Productivity</strong></td><td>Sequence Retention and Gradient Stability</td></tr></tbody></table><h2 id="GNN-Graph-Neural-Networks"><a href="#GNN-Graph-Neural-Networks" class="headerlink" title="GNN (Graph Neural Networks)"></a>GNN (Graph Neural Networks)</h2><table><thead><tr><th><strong>Category</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td><strong>Basic Components</strong></td><td>Node Embeddings, Edge Features, Graph Convolutions</td></tr><tr><td><strong>Open-Source&#x2F; Forbidden</strong></td><td>Open-Source</td></tr><tr><td><strong>Criteria of Measuring Productivity</strong></td><td>Node Classification Accuracy, Link Prediction</td></tr><tr><td><strong>Model’s Parameters</strong></td><td>Edge Weights (Changeable), Node Attributes (Fixed)</td></tr><tr><td><strong>Criteria of Measuring Parameter’s Productivity</strong></td><td>Graph Feature Capture Efficiency</td></tr><tr><td><strong>Model’s Hyperparameters</strong></td><td>Number of Layers, Embedding Size (Changeable)</td></tr><tr><td><strong>Criteria of Measuring Hyperparameter’s Productivity</strong></td><td>Graph-Level Feature Generalization</td></tr><tr><td><strong>Basic Components of Hyperparameter’s Productivity</strong></td><td>Graph Topology Learning</td></tr></tbody></table><h2 id="BERT-Bidirectional-Encoder-Representations-from-Transformers"><a href="#BERT-Bidirectional-Encoder-Representations-from-Transformers" class="headerlink" title="BERT (Bidirectional Encoder Representations from Transformers)"></a>BERT (Bidirectional Encoder Representations from Transformers)</h2><table><thead><tr><th><strong>Category</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td><strong>Basic Components</strong></td><td>Encoder, Multi-Head Attention, Feedforward Layers</td></tr><tr><td><strong>Open-Source&#x2F; Forbidden</strong></td><td>Open-Source</td></tr><tr><td><strong>Criteria of Measuring Productivity</strong></td><td>F1-Score, Exact Match (QA), Perplexity</td></tr><tr><td><strong>Model’s Parameters</strong></td><td>Token Embeddings (Changeable), Vocabulary (Fixed)</td></tr><tr><td><strong>Criteria of Measuring Parameter’s Productivity</strong></td><td>Contextual Understanding Quality</td></tr><tr><td><strong>Model’s Hyperparameters</strong></td><td>Learning Rate, Batch Size, Sequence Length (Changeable)</td></tr><tr><td><strong>Criteria of Measuring Hyperparameter’s Productivity</strong></td><td>Contextual Embedding Accuracy</td></tr><tr><td><strong>Basic Components of Hyperparameter’s Productivity</strong></td><td>Attention Mechanism, Positional Encoding</td></tr></tbody></table><h2 id="BART-Bidirectional-and-Auto-Regressive-Transformers"><a href="#BART-Bidirectional-and-Auto-Regressive-Transformers" class="headerlink" title="BART (Bidirectional and Auto-Regressive Transformers)"></a>BART (Bidirectional and Auto-Regressive Transformers)</h2><table><thead><tr><th><strong>Category</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td><strong>Basic Components</strong></td><td>Encoder-Decoder, Multi-Head Attention, Feedforward</td></tr><tr><td><strong>Open-Source&#x2F; Forbidden</strong></td><td>Open-Source</td></tr><tr><td><strong>Criteria of Measuring Productivity</strong></td><td>Rouge Score, BLEU Score</td></tr><tr><td><strong>Model’s Parameters</strong></td><td>Attention Weights (Changeable), Vocabulary (Fixed)</td></tr><tr><td><strong>Criteria of Measuring Parameter’s Productivity</strong></td><td>Summarization and Translation Accuracy</td></tr><tr><td><strong>Model’s Hyperparameters</strong></td><td>Learning Rate, Number of Heads (Changeable)</td></tr><tr><td><strong>Criteria of Measuring Hyperparameter’s Productivity</strong></td><td>Text Generation Quality</td></tr><tr><td><strong>Basic Components of Hyperparameter’s Productivity</strong></td><td>Encoder-Decoder Consistency</td></tr></tbody></table><h2 id="T5-Text-to-Text-Transfer-Transformer"><a href="#T5-Text-to-Text-Transfer-Transformer" class="headerlink" title="T5 (Text-to-Text Transfer Transformer)"></a>T5 (Text-to-Text Transfer Transformer)</h2><table><thead><tr><th><strong>Category</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td><strong>Basic Components</strong></td><td>Encoder-Decoder, Attention Mechanisms, Feedforward</td></tr><tr><td><strong>Open-Source&#x2F; Forbidden</strong></td><td>Open-Source</td></tr><tr><td><strong>Criteria of Measuring Productivity</strong></td><td>Rouge Score, BLEU Score</td></tr><tr><td><strong>Model’s Parameters</strong></td><td>Token Embeddings (Changeable), Vocabulary (Fixed)</td></tr><tr><td><strong>Criteria of Measuring Parameter’s Productivity</strong></td><td>Text-to-Text Conversion Accuracy</td></tr><tr><td><strong>Model’s Hyperparameters</strong></td><td>Sequence Length, Beam Width (Changeable)</td></tr><tr><td><strong>Criteria of Measuring Hyperparameter’s Productivity</strong></td><td>Text Generation Coherence</td></tr><tr><td><strong>Basic Components of Hyperparameter’s Productivity</strong></td><td>Attention Span, Latent Representation Quality</td></tr></tbody></table><h2 id="LLAMA"><a href="#LLAMA" class="headerlink" title="LLAMA"></a>LLAMA</h2><table><thead><tr><th><strong>Category</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td><strong>Basic Components</strong></td><td>Transformer Layers, Feedforward Layers, Attention</td></tr><tr><td><strong>Open-Source&#x2F; Forbidden</strong></td><td>Restricted for Modifications</td></tr><tr><td><strong>Criteria of Measuring Productivity</strong></td><td>F1-Score, Rouge Score</td></tr><tr><td><strong>Model’s Parameters</strong></td><td>Attention Weights (Changeable), Vocabulary (Fixed)</td></tr><tr><td><strong>Criteria of Measuring Parameter’s Productivity</strong></td><td>Latent Representation Consistency</td></tr><tr><td><strong>Model’s Hyperparameters</strong></td><td>Number of Layers, Head Size (Changeable)</td></tr><tr><td><strong>Criteria of Measuring Hyperparameter’s Productivity</strong></td><td>Layer-to-Layer Weight Propagation</td></tr><tr><td><strong>Basic Components of Hyperparameter’s Productivity</strong></td><td>Transformer Block Efficiency</td></tr></tbody></table><h2 id="GPT-Generative-Pre-trained-Transformer"><a href="#GPT-Generative-Pre-trained-Transformer" class="headerlink" title="GPT (Generative Pre-trained Transformer)"></a>GPT (Generative Pre-trained Transformer)</h2><table><thead><tr><th><strong>Category</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td><strong>Basic Components</strong></td><td>Transformer Decoder, Feedforward Layers, Attention</td></tr><tr><td><strong>Open-Source&#x2F; Forbidden</strong></td><td>Open-Source</td></tr><tr><td><strong>Criteria of Measuring Productivity</strong></td><td>Perplexity, BLEU Score</td></tr><tr><td><strong>Model’s Parameters</strong></td><td>Attention Weights (Changeable), Vocabulary (Fixed)</td></tr><tr><td><strong>Criteria of Measuring Parameter’s Productivity</strong></td><td>Generative Text Coherence</td></tr><tr><td><strong>Model’s Hyperparameters</strong></td><td>Learning Rate, Model Depth, Token Limit (Changeable)</td></tr><tr><td><strong>Criteria of Measuring Hyperparameter’s Productivity</strong></td><td>Generative Text Quality</td></tr><tr><td><strong>Basic Components of Hyperparameter’s Productivity</strong></td><td>Token Context Understanding</td></tr></tbody></table><h2 id="ViT-Vision-Transformer"><a href="#ViT-Vision-Transformer" class="headerlink" title="ViT (Vision Transformer)"></a>ViT (Vision Transformer)</h2><table><thead><tr><th><strong>Category</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td><strong>Basic Components</strong></td><td>Patch Embedding, Transformer Layers, Attention</td></tr><tr><td><strong>Open-Source&#x2F; Forbidden</strong></td><td>Open-Source</td></tr><tr><td><strong>Criteria of Measuring Productivity</strong></td><td>Accuracy, Precision, Recall, F1-Score</td></tr><tr><td><strong>Model’s Parameters</strong></td><td>Patch Embeddings (Changeable), Image Size (Fixed)</td></tr><tr><td><strong>Criteria of Measuring Parameter’s Productivity</strong></td><td>Visual Feature Generalization</td></tr><tr><td><strong>Model’s Hyperparameters</strong></td><td>Patch Size, Attention Heads (Changeable)</td></tr><tr><td><strong>Criteria of Measuring Hyperparameter’s Productivity</strong></td><td>Patch Extraction Accuracy, Attention Span</td></tr><tr><td><strong>Basic Components of Hyperparameter’s Productivity</strong></td><td>Image Feature Learning Efficiency</td></tr></tbody></table><!-- ## Table - Models vs features| **Model/Architecture** | **Basic Components**                                    | **Open-Source/ Forbidden for Modifications** | **Criteria of Measuring Model Productivity** | **Model's Parameters (Changeable/Non-Changeable)** | **Criteria of Measuring Parameter's Productivity** | **Model's Hyperparameters (Changeable/Non-Changeable)** | **Criteria of Measuring Hyperparameter's Productivity** | **Basic Components of Hyperparameter's Productivity** ||-------------------------|--------------------------------------------------------|-----------------------------------------------|-----------------------------------------------|----------------------------------------------------|----------------------------------------------------|---------------------------------------------------------|---------------------------------------------------------|---------------------------------------------------------|| **Autoencoders**        | Encoder, Decoder, Latent Space                         | Open-Source                                   | Reconstruction Loss                            | Weights (Changeable), Latent Dimensions (Fixed)    | Reconstruction Accuracy                              | Learning Rate, Latent Dimension Size (Changeable)       | Lower Reconstruction Error                             | Effective Latent Space Size, Training Convergence Rate || **CNN**                 | Convolution Layers, Pooling, Fully Connected Layers    | Open-Source                                   | Accuracy, Precision, Recall, F1-Score         | Filter Weights (Changeable), Input Channels (Fixed)| Detection Accuracy                                   | Kernel Size, Stride, Number of Filters (Changeable)     | Higher Feature Extraction Quality                      | Filter Efficiency, Computational Cost                 || **RNN**                 | Recurrent Layers, Activation Functions                | Open-Source                                   | Perplexity, Accuracy, BLEU Score (NLP)        | Hidden State (Changeable), Sequence Length (Fixed) | Temporal Pattern Capture Efficiency                 | Learning Rate, Hidden State Size (Changeable)           | Sequence Learning Performance                          | Effective Sequence Memory Size                        || **FNN**                 | Neurons, Dense Layers, Activation Functions           | Open-Source                                   | Accuracy, Loss Reduction                       | Weights (Changeable), Bias (Fixed)                | Accuracy and Loss Reduction                          | Learning Rate, Number of Layers (Changeable)            | Loss Convergence Rate                                  | Layer Depth, Weight Initialization Strategy           || **LSTM**                | LSTM Cells (Input, Forget, Output Gates)              | Open-Source                                   | Perplexity, Accuracy (Time-Series, NLP)        | Cell Weights (Changeable), Memory Cell (Fixed)    | Long-Term Dependency Capture Efficiency             | Learning Rate, Number of Layers (Changeable)            | Retention of Long-Term Dependencies                   | Sequence Retention and Gradient Stability             || **GNN**                 | Node Embeddings, Edge Features, Graph Convolutions    | Open-Source                                   | Node Classification Accuracy, Link Prediction | Edge Weights (Changeable), Node Attributes (Fixed)| Graph Feature Capture Efficiency                    | Number of Layers, Embedding Size (Changeable)           | Graph-Level Feature Generalization                    | Graph Topology Learning                                || **BERT**                | Encoder, Multi-Head Attention, Feedforward Layers     | Open-Source                                   | F1-Score, Exact Match (QA), Perplexity         | Token Embeddings (Changeable), Vocabulary (Fixed) | Contextual Understanding Quality                    | Learning Rate, Batch Size, Sequence Length (Changeable)| Contextual Embedding Accuracy                         | Attention Mechanism, Positional Encoding              || **BART**                | Encoder-Decoder, Multi-Head Attention, Feedforward    | Open-Source                                   | Rouge Score, BLEU Score                        | Attention Weights (Changeable), Vocabulary (Fixed)| Summarization and Translation Accuracy              | Learning Rate, Number of Heads (Changeable)             | Text Generation Quality                               | Encoder-Decoder Consistency                           || **T5**                  | Encoder-Decoder, Attention Mechanisms, Feedforward    | Open-Source                                   | Rouge Score, BLEU Score                        | Token Embeddings (Changeable), Vocabulary (Fixed) | Text-to-Text Conversion Accuracy                    | Sequence Length, Beam Width (Changeable)                | Text Generation Coherence                             | Attention Span, Latent Representation Quality         || **LLAMA**               | Transformer Layers, Feedforward Layers, Attention     | Restricted for Modifications                  | F1-Score, Rouge Score                          | Attention Weights (Changeable), Vocabulary (Fixed)| Generative Output Accuracy                          | Number of Layers, Head Size (Changeable)                | Latent Representation Consistency                     | Layer-to-Layer Weight Propagation                     || **GPT**                 | Transformer Decoder, Feedforward Layers, Attention    | Open-Source                                   | Perplexity, BLEU Score                         | Attention Weights (Changeable), Vocabulary (Fixed)| Generative Text Coherence                            | Learning Rate, Model Depth, Token Limit (Changeable)    | Generative Text Quality                               | Token Context Understanding                           || **ViT (Vision Transformer)** | Patch Embedding, Transformer Layers, Attention        | Open-Source                                   | Accuracy, Precision, Recall, F1-Score         | Patch Embeddings (Changeable), Image Size (Fixed) | Image Feature Generalization                         | Patch Size, Attention Heads (Changeable)                | Visual Feature Learning Efficiency                    | Patch Extraction Accuracy, Attention Span             |---### Notes:1. **Open-Source vs. Forbidden for Modifications**: Models like LLAMA may have licensing restrictions preventing modification, while others are freely accessible for experimentation.2. **Criteria of Productivity**: Metrics vary by task, e.g., classification accuracy for vision tasks, BLEU scores for translation, or F1-scores for question-answering.3. **Parameters vs. Hyperparameters**: Parameters are learned during training (e.g., weights), whereas hyperparameters are manually set and adjusted (e.g., learning rate, number of layers).4. **Measuring Hyperparameter Productivity**: Includes monitoring loss convergence, validation performance, and computational efficiency.## Table - Features vs models# Comparison of Popular Models and Architectures| **Category**                               | **Autoencoders**                | **CNN**                            | **RNN**                             | **FNN**                             | **LSTM**                            | **GNN**                             | **BERT**                            | **BART**                            | **T5**                              | **LLAMA**                           | **GPT**                             | **ViT**                             ||--------------------------------------------|----------------------------------|-------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|| **Basic Components**                       | Encoder, Decoder, Latent Space  | Convolution Layers, Pooling, Fully Connected Layers | Recurrent Layers, Activation Functions | Neurons, Dense Layers, Activation Functions | LSTM Cells (Input, Forget, Output Gates) | Node Embeddings, Edge Features, Graph Convolutions | Encoder, Multi-Head Attention, Feedforward Layers | Encoder-Decoder, Multi-Head Attention, Feedforward | Encoder-Decoder, Attention Mechanisms, Feedforward | Transformer Layers, Feedforward Layers, Attention | Transformer Decoder, Feedforward Layers, Attention | Patch Embedding, Transformer Layers, Attention || **Open-Source/ Forbidden for Modifications** | Open-Source                     | Open-Source                        | Open-Source                          | Open-Source                          | Open-Source                          | Open-Source                          | Open-Source                          | Open-Source                          | Open-Source                          | Restricted for Modifications         | Open-Source                          | Open-Source                          || **Criteria of Measuring Model Productivity** | Reconstruction Loss             | Accuracy, Precision, Recall, F1-Score | Perplexity, Accuracy, BLEU Score (NLP) | Accuracy, Loss Reduction             | Perplexity, Accuracy (Time-Series, NLP) | Node Classification Accuracy, Link Prediction | F1-Score, Exact Match (QA), Perplexity | Rouge Score, BLEU Score             | Rouge Score, BLEU Score             | F1-Score, Rouge Score                | Perplexity, BLEU Score               | Accuracy, Precision, Recall, F1-Score || **Model's Parameters (Changeable/Non-Changeable)** | Weights (Changeable), Latent Dimensions (Fixed) | Filter Weights (Changeable), Input Channels (Fixed) | Hidden State (Changeable), Sequence Length (Fixed) | Weights (Changeable), Bias (Fixed)  | Cell Weights (Changeable), Memory Cell (Fixed) | Edge Weights (Changeable), Node Attributes (Fixed) | Token Embeddings (Changeable), Vocabulary (Fixed) | Attention Weights (Changeable), Vocabulary (Fixed) | Token Embeddings (Changeable), Vocabulary (Fixed) | Attention Weights (Changeable), Vocabulary (Fixed) | Attention Weights (Changeable), Vocabulary (Fixed) | Patch Embeddings (Changeable), Image Size (Fixed) || **Criteria of Measuring Parameter's Productivity** | Reconstruction Accuracy         | Detection Accuracy                 | Temporal Pattern Capture Efficiency  | Accuracy and Loss Reduction          | Long-Term Dependency Capture Efficiency | Graph Feature Capture Efficiency    | Contextual Understanding Quality     | Summarization and Translation Accuracy | Text-to-Text Conversion Accuracy    | Generative Output Accuracy           | Generative Text Coherence            | Image Feature Generalization         || **Model's Hyperparameters (Changeable/Non-Changeable)** | Learning Rate, Latent Dimension Size (Changeable) | Kernel Size, Stride, Number of Filters (Changeable) | Learning Rate, Hidden State Size (Changeable) | Learning Rate, Number of Layers (Changeable) | Learning Rate, Number of Layers (Changeable) | Number of Layers, Embedding Size (Changeable) | Learning Rate, Batch Size, Sequence Length (Changeable) | Learning Rate, Number of Heads (Changeable) | Sequence Length, Beam Width (Changeable) | Number of Layers, Head Size (Changeable) | Learning Rate, Model Depth, Token Limit (Changeable) | Patch Size, Attention Heads (Changeable) || **Criteria of Measuring Hyperparameter's Productivity** | Lower Reconstruction Error      | Higher Feature Extraction Quality  | Sequence Learning Performance        | Loss Convergence Rate                | Retention of Long-Term Dependencies  | Graph-Level Feature Generalization  | Contextual Embedding Accuracy        | Text Generation Quality              | Text Generation Coherence            | Latent Representation Consistency    | Generative Text Quality              | Visual Feature Learning Efficiency   || **Basic Components of Hyperparameter's Productivity** | Effective Latent Space Size, Training Convergence Rate | Filter Efficiency, Computational Cost | Effective Sequence Memory Size       | Layer Depth, Weight Initialization Strategy | Sequence Retention and Gradient Stability | Graph Topology Learning             | Attention Mechanism, Positional Encoding | Encoder-Decoder Consistency         | Attention Span, Latent Representation Quality | Layer-to-Layer Weight Propagation    | Token Context Understanding          | Patch Extraction Accuracy, Attention Span | -->]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;Related to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;../.</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="AI" scheme="https://ooge0.github.io/hexo-blog/tags/AI/"/>
    
    <category term="NN" scheme="https://ooge0.github.io/hexo-blog/tags/NN/"/>
    
  </entry>
  
  <entry>
    <title>List of neural network models, architectures, and basic components</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/12/10/post_ai_nn__list_of_neural_network_models_architectures_and_basic_components/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/12/10/post_ai_nn__list_of_neural_network_models_architectures_and_basic_components/</id>
    <published>2024-12-10T17:30:33.000Z</published>
    <updated>2024-12-12T11:40:09.755Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Related to </p><ol><li><a href="../../../../post_ai_nn__comparison_of_popular_models_and_architectures">Comparison of Popular Models and Architectures</a></li></ol><hr><p>This document provides a categorized list of common neural network (NN) models and architectures. It also outlines their basic components and how they fit into larger systems.</p><hr><h2 id="Neural-network-models-and-architectures"><a href="#Neural-network-models-and-architectures" class="headerlink" title="Neural network models and architectures"></a><strong>Neural network models and architectures</strong></h2><table><thead><tr><th><strong>Architecture</strong></th><th><strong>Model Examples</strong></th><th><strong>Purpose</strong></th></tr></thead><tbody><tr><td><strong>Feedforward Neural Network (FNN)</strong></td><td>Basic MLP (Multi-Layer Perceptron)</td><td>General-purpose model for regression and classification tasks.</td></tr><tr><td><strong>Convolutional Neural Networks (CNN)</strong></td><td>VGG, ResNet, AlexNet, EfficientNet</td><td>Designed for image processing tasks like classification, object detection, and segmentation.</td></tr><tr><td><strong>Recurrent Neural Networks (RNNs)</strong></td><td>Vanilla RNN, LSTM, GRU</td><td>Sequential data processing for tasks like language modeling and time-series prediction.</td></tr><tr><td><strong>Transformers</strong></td><td>BERT, GPT, T5, Vision Transformer (ViT)</td><td>State-of-the-art architecture for text, sequential, and image tasks.</td></tr><tr><td><strong>Autoencoders</strong></td><td>Variational Autoencoder (VAE), Denoising Autoencoder</td><td>Dimensionality reduction, feature extraction, and generative tasks.</td></tr><tr><td><strong>Generative Adversarial Networks (GANs)</strong></td><td>DCGAN, StyleGAN, CycleGAN</td><td>Generative tasks such as image synthesis and domain transfer.</td></tr><tr><td><strong>Graph Neural Networks (GNNs)</strong></td><td>GCN, GraphSAGE, GAT</td><td>Structured data learning tasks, e.g., on graphs or social networks.</td></tr></tbody></table><hr><h2 id="Basic-Components-of-Neural-Networks"><a href="#Basic-Components-of-Neural-Networks" class="headerlink" title="Basic Components of Neural Networks"></a><strong>Basic Components of Neural Networks</strong></h2><table><thead><tr><th><strong>Component</strong></th><th><strong>Description</strong></th><th><strong>Applications</strong></th></tr></thead><tbody><tr><td><strong>Neuron</strong></td><td>Basic computation unit applying a weighted sum followed by an activation function.</td><td>Foundational unit in all neural networks.</td></tr><tr><td><strong>Layer</strong></td><td>A collection of neurons; can be input, hidden, or output.</td><td>Used in all neural architectures.</td></tr><tr><td><strong>Activation Function</strong></td><td>Non-linear function applied to neurons, e.g., ReLU, Sigmoid, Tanh.</td><td>Enables learning of complex patterns.</td></tr><tr><td><strong>Dropout</strong></td><td>Regularization technique randomly dropping neurons during training.</td><td>Reduces overfitting in models.</td></tr><tr><td><strong>Encoder</strong></td><td>Part of the model that converts input data into a latent representation.</td><td>Used in Transformers, Autoencoders, BERT, and more.</td></tr><tr><td><strong>Decoder</strong></td><td>Converts latent representations back to an output format.</td><td>Used in Transformers, Autoencoders, and Seq2Seq models.</td></tr><tr><td><strong>Attention Mechanism</strong></td><td>Focuses on important parts of the input data, e.g., Self-Attention.</td><td>Essential in Transformers and attention-based architectures.</td></tr><tr><td><strong>Residual Block</strong></td><td>A module that adds shortcut connections to mitigate vanishing gradients.</td><td>Found in ResNet, Transformer architectures.</td></tr><tr><td><strong>Convolution Layer</strong></td><td>Applies convolutional operations to extract spatial features.</td><td>Used in CNNs for tasks like image and video analysis.</td></tr><tr><td><strong>Pooling Layer</strong></td><td>Reduces spatial dimensions using techniques like max-pooling or average pooling.</td><td>Used in CNNs to downsample feature maps.</td></tr><tr><td><strong>Recurrent Cell</strong></td><td>Core unit of RNNs, capable of maintaining temporal dependencies.</td><td>Used in RNNs, LSTMs, and GRUs for time-series and sequential data.</td></tr><tr><td><strong>Self-Attention Layer</strong></td><td>Computes relationships between all input tokens to capture global dependencies.</td><td>Core of Transformers.</td></tr><tr><td><strong>Feedforward Layer</strong></td><td>Dense layer applied after attention mechanisms in Transformers.</td><td>Processes token-wise transformations.</td></tr><tr><td><strong>Embedding Layer</strong></td><td>Converts categorical data or tokens into dense vectors.</td><td>Used in NLP, graph embeddings, and more.</td></tr><tr><td><strong>Latent Space</strong></td><td>Compressed representation of data, typically learned by encoders.</td><td>Found in Autoencoders, VAEs, and GANs.</td></tr></tbody></table><hr><h2 id="How-components-relate-to-models"><a href="#How-components-relate-to-models" class="headerlink" title="How components relate to models"></a><strong>How components relate to models</strong></h2><table><thead><tr><th><strong>Architecture</strong></th><th><strong>Key Components</strong></th></tr></thead><tbody><tr><td><strong>FNN</strong></td><td>Neurons, Layers, Activation Functions, Dropout.</td></tr><tr><td><strong>CNN</strong></td><td>Convolution Layers, Pooling Layers, Fully Connected Layers, Activation Functions.</td></tr><tr><td><strong>RNN (Vanilla)</strong></td><td>Recurrent Cells, Layers, Activation Functions.</td></tr><tr><td><strong>LSTM</strong></td><td>LSTM Cells (with Forget, Input, Output gates), Layers.</td></tr><tr><td><strong>Transformers</strong></td><td>Encoder, Decoder, Self-Attention, Multi-Head Attention, Feedforward Layers, Positional Embeddings.</td></tr><tr><td><strong>Autoencoders</strong></td><td>Encoder, Decoder, Latent Space, Reconstruction Loss.</td></tr><tr><td><strong>GANs</strong></td><td>Generator, Discriminator, Adversarial Loss.</td></tr><tr><td><strong>GNNs</strong></td><td>Node Embeddings, Edge Features, Graph Convolutions.</td></tr></tbody></table><hr><p>This table serves as a foundation for understanding how modern deep learning architectures are structured and utilized across a wide range of applications.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;Related to &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;../.</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="AI" scheme="https://ooge0.github.io/hexo-blog/tags/AI/"/>
    
    <category term="NN" scheme="https://ooge0.github.io/hexo-blog/tags/NN/"/>
    
  </entry>
  
  <entry>
    <title>BART configuration parameters overview</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/12/04/post_ai_llm__bart_configuration_parameters_overview/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/12/04/post_ai_llm__bart_configuration_parameters_overview/</id>
    <published>2024-12-04T20:08:12.000Z</published>
    <updated>2024-12-04T20:39:12.658Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><em>This post related to:</em></p><ol><li><a href="../../../../post_ai_llm__techniques_for_handling_context_in_ai_models">Techniques for Handling Context in AI Models</a></li></ol><hr><h2 id="Parameters-list-and-descriptions"><a href="#Parameters-list-and-descriptions" class="headerlink" title="Parameters list and descriptions"></a>Parameters list and descriptions</h2><table><thead><tr><th><strong>Parameter</strong></th><th><strong>Description</strong></th><th><strong>Data Type&#x2F;Options</strong></th></tr></thead><tbody><tr><td><code>max_position_embeddings</code></td><td>Maximum number of positions for input tokens.</td><td>Integer, e.g., <code>1024</code>.</td></tr><tr><td><code>d_model</code></td><td>Dimensionality of the model’s embeddings and hidden states.</td><td>Integer, e.g., <code>768</code>, <code>1024</code>.</td></tr><tr><td><code>encoder_layers</code></td><td>Number of layers in the encoder.</td><td>Integer, e.g., <code>6</code>, <code>12</code>.</td></tr><tr><td><code>decoder_layers</code></td><td>Number of layers in the decoder.</td><td>Integer, e.g., <code>6</code>, <code>12</code>.</td></tr><tr><td><code>encoder_attention_heads</code></td><td>Number of attention heads in the encoder.</td><td>Integer, e.g., <code>12</code>.</td></tr><tr><td><code>decoder_attention_heads</code></td><td>Number of attention heads in the decoder.</td><td>Integer, e.g., <code>12</code>.</td></tr><tr><td><code>vocab_size</code></td><td>Vocabulary size of the tokenizer. Represents the range of token IDs.</td><td>Integer, e.g., <code>50265</code>.</td></tr><tr><td><code>activation_function</code></td><td>Activation function used in feedforward layers.</td><td>String: <code>relu</code>, <code>gelu</code>, <code>tanh</code>, etc.</td></tr><tr><td><code>dropout</code></td><td>Dropout probability applied to various layers.</td><td>Float between <code>0.0</code> and <code>1.0</code>, typically <code>0.1</code>.</td></tr><tr><td><code>attention_dropout</code></td><td>Dropout probability in the attention mechanism.</td><td>Float between <code>0.0</code> and <code>1.0</code>, typically <code>0.1</code>.</td></tr><tr><td><code>init_std</code></td><td>Standard deviation for weight initialization.</td><td>Float, e.g., <code>0.02</code>.</td></tr><tr><td><code>encoder_ffn_dim</code></td><td>Dimensionality of the encoder feedforward layers.</td><td>Integer, e.g., <code>3072</code>.</td></tr><tr><td><code>decoder_ffn_dim</code></td><td>Dimensionality of the decoder feedforward layers.</td><td>Integer, e.g., <code>3072</code>.</td></tr><tr><td><code>scale_embedding</code></td><td>Whether to scale the embeddings by <code>sqrt(d_model)</code>.</td><td>Boolean, <code>true</code> or <code>false</code>.</td></tr><tr><td><code>use_cache</code></td><td>Whether to use cached key&#x2F;values for faster decoding.</td><td>Boolean, <code>true</code> or <code>false</code>.</td></tr><tr><td><code>pad_token_id</code></td><td>Token ID used for padding.</td><td>Integer, typically <code>0</code>.</td></tr><tr><td><code>bos_token_id</code></td><td>Token ID for the beginning-of-sequence token.</td><td>Integer, typically <code>0</code>.</td></tr><tr><td><code>eos_token_id</code></td><td>Token ID for the end-of-sequence token.</td><td>Integer, typically <code>2</code>.</td></tr></tbody></table><h2 id="Summary-of-parameter-impact"><a href="#Summary-of-parameter-impact" class="headerlink" title="Summary of parameter impact"></a>Summary of parameter impact</h2><h3 id="How-Changes-Reflect-on-Model-Behavior"><a href="#How-Changes-Reflect-on-Model-Behavior" class="headerlink" title="How Changes Reflect on Model Behavior:"></a>How Changes Reflect on Model Behavior:</h3><ol><li><p><strong>Model Complexity:</strong></p><ul><li>Increasing <code>encoder_layers</code>, <code>decoder_layers</code>, <code>d_model</code>, or attention heads enhances model capacity but increases computational requirements.</li></ul></li><li><p><strong>Regularization:</strong></p><ul><li>Dropout parameters (<code>dropout</code>, <code>attention_dropout</code>) control overfitting risks but may reduce performance if too high.</li></ul></li><li><p><strong>Encoder-Decoder interactions:</strong></p><ul><li><code>encoder_ffn_dim</code> and <code>decoder_ffn_dim</code> directly influence the learning ability of the model for complex patterns.</li></ul></li><li><p><strong>Efficiency:</strong></p><ul><li>Enabling <code>use_cache</code> improves inference time for autoregressive tasks.</li></ul></li></ol><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li>Paper: BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension<ul><li><a href="https://arxiv.org/pdf/1910.13461">Read on arxiv.org</a></li><li>DOI:10.48550&#x2F;arXiv.1910.13461</li></ul></li><li>Paper: Improving Sharpness-Aware Minimization with Fisher Mask for Better Generalization on Language Models<ul><li>DOI:10.48550&#x2F;arXiv.2210.05497</li><li><a href="https://arxiv.org/pdf/2210.05497">Read on arxiv.org</a></li></ul></li><li><a href="https://huggingface.co/docs/transformers/model_doc/bart">Hugging Face BART Documentation</a></li><li>Web article: <a href="https://medium.com/@nadirapovey/bart-model-architecture-8ac1cea0e877">BART Model Architecture | Medium</a></li><li>Web article: <a href="https://www.projectpro.io/article/transformers-bart-model-explained/553">Transformers BART Model Explained for Text Summarization | projectpro.io</a></li><li>Google colab notebook: <a href="https://colab.research.google.com/drive/1Cy27V-7qqYatqMA7fEqG2kgMySZXw9I4?usp=sharing&pli=1">BART Learns to Rap - Medium.ipynb</a></li></ul><h2 id="Challenges-and-reports-on-configuration"><a href="#Challenges-and-reports-on-configuration" class="headerlink" title="Challenges and reports on configuration"></a>Challenges and reports on configuration</h2><ul><li><p><strong>Report:</strong> </p><ul><li>Paper: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Raffel et al., 2020.<ul><li>DOI: 10.48550&#x2F;arXiv.1910.10683</li><li><a href="https://arxiv.org/pdf/1910.10683">Read on arxiv.org</a></li></ul></li></ul></li><li><p><strong>Challenges:</strong> Balancing fine-tuning for generative and discriminative tasks in sequence-to-sequence models.</p><ul><li>Paper: Unified Generative and Discriminative Training for Multi-modal Large Language Models<ul><li><a href="https://arxiv.org/html/2411.00304v1">Read on arxiv.org</a></li></ul></li></ul></li></ul><h2 id="Playgrounds-to-experiment"><a href="#Playgrounds-to-experiment" class="headerlink" title="Playgrounds to experiment"></a>Playgrounds to experiment</h2><ul><li><strong>Hugging Face Spaces:</strong> <a href="https://huggingface.co/spaces">https://huggingface.co/spaces</a></li><li><strong>Google Colab with Transformers:</strong> <a href="https://colab.research.google.com/">https://colab.research.google.com/</a></li><li><strong>OpenAI Playground for Generative Tasks:</strong> <a href="https://platform.openai.com/playground">https://platform.openai.com/playground</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;&lt;em&gt;This post related to:&lt;/em&gt;&lt;/p&gt;
&lt;ol</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="AI" scheme="https://ooge0.github.io/hexo-blog/tags/AI/"/>
    
    <category term="LLM" scheme="https://ooge0.github.io/hexo-blog/tags/LLM/"/>
    
    <category term="BART" scheme="https://ooge0.github.io/hexo-blog/tags/BART/"/>
    
  </entry>
  
  <entry>
    <title>BERT configuration parameters overview</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/12/04/post_ai_llm__bert_configuration_parameters_overview/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/12/04/post_ai_llm__bert_configuration_parameters_overview/</id>
    <published>2024-12-04T20:08:12.000Z</published>
    <updated>2024-12-04T20:38:06.482Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="This-post-related-to-1-Techniques-for-Handling-Context-in-AI-Models2-GPT2-configuration-parameters-overview3-BART-configuration-parameters-overview"><a href="#This-post-related-to-1-Techniques-for-Handling-Context-in-AI-Models2-GPT2-configuration-parameters-overview3-BART-configuration-parameters-overview" class="headerlink" title="This post related to:1. Techniques for Handling Context in AI Models2. GPT2 configuration parameters overview3. BART configuration parameters overview"></a><em>This post related to:</em><br>1. <a href="../../../../post_ai_llm__techniques_for_handling_context_in_ai_models">Techniques for Handling Context in AI Models</a><br>2. <a href="../../../../post_ai_llm__gpt2_configuration_parameters_overview">GPT2 configuration parameters overview</a><br>3. <a href="../../../../post_ai_llm__bart_configuration_parameters_overview">BART configuration parameters overview</a></h2><h2 id="Parameters-list-and-descriptions"><a href="#Parameters-list-and-descriptions" class="headerlink" title="Parameters list and descriptions"></a>Parameters list and descriptions</h2><table><thead><tr><th><strong>Parameter</strong></th><th><strong>Description</strong></th><th><strong>Data Type&#x2F;Options</strong></th></tr></thead><tbody><tr><td><code>hidden_size</code></td><td>Dimensionality of the hidden states and embeddings.</td><td>Integer, e.g., <code>768</code>, <code>1024</code>.</td></tr><tr><td><code>num_hidden_layers</code></td><td>Number of hidden layers in the transformer encoder.</td><td>Integer, e.g., <code>12</code>, <code>24</code>.</td></tr><tr><td><code>num_attention_heads</code></td><td>Number of attention heads per transformer layer.</td><td>Integer, typically a divisor of <code>hidden_size</code>.</td></tr><tr><td><code>vocab_size</code></td><td>Vocabulary size of the tokenizer. Represents the range of token IDs.</td><td>Integer, e.g., <code>30522</code>.</td></tr><tr><td><code>intermediate_size</code></td><td>Dimensionality of the feedforward layers.</td><td>Integer, e.g., <code>3072</code>.</td></tr><tr><td><code>hidden_dropout_prob</code></td><td>Dropout probability for fully connected layers in the encoder.</td><td>Float between <code>0.0</code> and <code>1.0</code>, typically <code>0.1</code>.</td></tr><tr><td><code>attention_probs_dropout_prob</code></td><td>Dropout probability in the attention mechanism.</td><td>Float between <code>0.0</code> and <code>1.0</code>, typically <code>0.1</code>.</td></tr><tr><td><code>max_position_embeddings</code></td><td>Maximum number of positions for input tokens.</td><td>Integer, e.g., <code>512</code>.</td></tr><tr><td><code>type_vocab_size</code></td><td>Size of the token type vocabulary for segment embeddings.</td><td>Integer, typically <code>2</code>.</td></tr><tr><td><code>initializer_range</code></td><td>Standard deviation for weight initialization.</td><td>Float, e.g., <code>0.02</code>.</td></tr><tr><td><code>layer_norm_eps</code></td><td>A small value added for numerical stability in layer normalization.</td><td>Float, typically <code>1e-5</code>.</td></tr><tr><td><code>output_hidden_states</code></td><td>Whether to output all hidden states from each layer.</td><td>Boolean, <code>true</code> or <code>false</code>.</td></tr><tr><td><code>output_attentions</code></td><td>Whether to output the attention weights.</td><td>Boolean, <code>true</code> or <code>false</code>.</td></tr></tbody></table><h2 id="Summary-of-parameter-impact"><a href="#Summary-of-parameter-impact" class="headerlink" title="Summary of parameter impact"></a>Summary of parameter impact</h2><h3 id="How-changes-reflect-on-model-behavior"><a href="#How-changes-reflect-on-model-behavior" class="headerlink" title="How changes reflect on model behavior:"></a>How changes reflect on model behavior:</h3><ol><li><p><strong>Capacity:</strong></p><ul><li>Increasing <code>hidden_size</code>, <code>num_hidden_layers</code>, or <code>num_attention_heads</code> allows the model to capture more complex patterns but increases resource usage.</li></ul></li><li><p><strong>Regularization:</strong></p><ul><li>Dropout probabilities (<code>hidden_dropout_prob</code>, <code>attention_probs_dropout_prob</code>) control overfitting risks but can hinder learning if set too high.</li></ul></li><li><p><strong>Pretraining vs. Fine-tuning:</strong></p><ul><li><code>type_vocab_size</code> is essential for tasks requiring segment embeddings (e.g., sentence pairs).</li></ul></li><li><p><strong>Stability and Efficiency:</strong></p><ul><li><code>layer_norm_eps</code> ensures stable training, while <code>initializer_range</code> affects convergence.</li></ul></li></ol><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li>Paper</li><li>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</li><li>DOI: 10.48550&#x2F;arXiv.1810.04805</li><li><a href="https://arxiv.org/abs/1810.04805">Read on </a></li><li><a href="https://huggingface.co/docs/transformers/model_doc/bert">Hugging Face BERT Documentation</a></li></ul><h2 id="Challenges-and-reports-on-configuration"><a href="#Challenges-and-reports-on-configuration" class="headerlink" title="Challenges and reports on configuration"></a>Challenges and reports on configuration</h2><ul><li><strong>Report:</strong> “On the Structural Properties of BERT Models” (Kovaleva et al., 2019).</li><li><strong>Challenges:</strong> Over-parameterization and inefficiency in fine-tuning for domain-specific tasks.</li></ul><h2 id="Playgrounds-to-experiment"><a href="#Playgrounds-to-experiment" class="headerlink" title="Playgrounds to experiment"></a>Playgrounds to experiment</h2><ul><li><strong>Hugging Face Spaces:</strong> <a href="https://huggingface.co/spaces">https://huggingface.co/spaces</a></li><li><strong>Google Colab with Transformers:</strong> <a href="https://colab.research.google.com/">https://colab.research.google.com/</a></li><li><strong>OpenAI Playground for Text Understanding:</strong> <a href="https://platform.openai.com/playground">https://platform.openai.com/playground</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h2 id=&quot;This-post-related-to-1-Techniques</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="AI" scheme="https://ooge0.github.io/hexo-blog/tags/AI/"/>
    
    <category term="LLM" scheme="https://ooge0.github.io/hexo-blog/tags/LLM/"/>
    
    <category term="BERT" scheme="https://ooge0.github.io/hexo-blog/tags/BERT/"/>
    
  </entry>
  
  <entry>
    <title>GPT-2 configuration parameters overview</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/12/04/post_ai_llm__gpt2_configuration_parameters_overview/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/12/04/post_ai_llm__gpt2_configuration_parameters_overview/</id>
    <published>2024-12-04T18:08:12.000Z</published>
    <updated>2024-12-12T10:55:43.394Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><em>This post related to:</em></p><ol><li><a href="../../../../post_ai_llm__techniques_for_handling_context_in_ai_models">Techniques for Handling Context in AI Models</a></li><li><a href="../../../../post_ai_llm__bart_configuration_parameters_overview">BART configuration parameters overview</a></li></ol><hr><h2 id="Parameters-List-and-Descriptions"><a href="#Parameters-List-and-Descriptions" class="headerlink" title="Parameters List and Descriptions"></a>Parameters List and Descriptions</h2><table><thead><tr><th><strong>Parameter</strong></th><th><strong>Description</strong></th><th><strong>Data Type&#x2F;Options</strong></th></tr></thead><tbody><tr><td><code>n_ctx</code></td><td>The context size or maximum length of input tokens the model can process.</td><td>Integer, e.g., <code>1024</code>, <code>2048</code>.</td></tr><tr><td><code>n_embd</code></td><td>Dimensionality of the embeddings. Determines the size of the word and positional embeddings.</td><td>Integer, typically <code>768</code>, <code>1024</code>, or <code>1280</code>.</td></tr><tr><td><code>n_layer</code></td><td>Number of transformer layers in the model. Dictates depth of the network.</td><td>Integer, e.g., <code>12</code>, <code>24</code>, <code>48</code>.</td></tr><tr><td><code>n_head</code></td><td>Number of attention heads in each transformer layer. Reflects parallel attention mechanisms.</td><td>Integer, typically a divisor of <code>n_embd</code> such as <code>12</code> or <code>16</code>.</td></tr><tr><td><code>vocab_size</code></td><td>Size of the vocabulary for the tokenizer. Represents the range of token IDs the model can handle.</td><td>Integer, e.g., <code>50257</code>.</td></tr><tr><td><code>activation_function</code></td><td>The activation function used in feedforward layers (e.g., “gelu”). Affects non-linearity.</td><td>String: <code>relu</code>, <code>gelu</code>, <code>tanh</code>, <code>sigmoid</code>, etc.</td></tr><tr><td><code>resid_pdrop</code></td><td>Dropout probability for residual connections. Adds regularization.</td><td>Float between <code>0.0</code> and <code>1.0</code>. Typically <code>0.1</code>.</td></tr><tr><td><code>embd_pdrop</code></td><td>Dropout probability for embeddings. Helps prevent overfitting.</td><td>Float between <code>0.0</code> and <code>1.0</code>. Typically <code>0.1</code>.</td></tr><tr><td><code>attn_pdrop</code></td><td>Dropout probability in the attention mechanism. Ensures robustness.</td><td>Float between <code>0.0</code> and <code>1.0</code>. Typically <code>0.1</code>.</td></tr><tr><td><code>initializer_range</code></td><td>The range of the uniform distribution for weight initialization.</td><td>Float, e.g., <code>0.02</code>.</td></tr><tr><td><code>layer_norm_epsilon</code></td><td>A small value added for numerical stability in layer normalization.</td><td>Float, typically <code>1e-5</code>.</td></tr><tr><td><code>use_cache</code></td><td>Whether the model uses cached key&#x2F;values for faster generation during inference.</td><td>Boolean, <code>true</code> or <code>false</code>.</td></tr><tr><td><code>bos_token_id</code></td><td>Token ID for the beginning-of-sequence token.</td><td>Integer, e.g., <code>50256</code>.</td></tr><tr><td><code>eos_token_id</code></td><td>Token ID for the end-of-sequence token.</td><td>Integer, e.g., <code>50256</code>.</td></tr><tr><td><code>scale_attn_weights</code></td><td>Whether to scale the attention weights.</td><td>Boolean, <code>true</code> or <code>false</code>.</td></tr><tr><td><code>output_hidden_states</code></td><td>Whether to return all hidden states from each layer during inference.</td><td>Boolean, <code>true</code> or <code>false</code>.</td></tr><tr><td><code>output_attentions</code></td><td>Whether to return the attention weights during inference.</td><td>Boolean, <code>true</code> or <code>false</code>.</td></tr><tr><td><code>tie_word_embeddings</code></td><td>Whether to tie the input and output word embeddings.</td><td>Boolean, <code>true</code> or <code>false</code>.</td></tr></tbody></table><hr><h2 id="Summary-of-Parameter-Impact"><a href="#Summary-of-Parameter-Impact" class="headerlink" title="Summary of Parameter Impact"></a>Summary of Parameter Impact</h2><h3 id="How-changes-reflect-on-model-behavior"><a href="#How-changes-reflect-on-model-behavior" class="headerlink" title="How changes reflect on model behavior:"></a>How changes reflect on model behavior:</h3><ol><li><p><strong>Model Size and Computation:</strong></p><ul><li>Increasing <code>n_layer</code>, <code>n_embd</code>, or <code>n_head</code> leads to larger and more computationally intensive models but potentially improves learning capacity.</li><li>Reducing <code>n_ctx</code> limits the model’s ability to process long inputs.</li></ul></li><li><p><strong>Regularization:</strong></p><ul><li>Dropout parameters (<code>resid_pdrop</code>, <code>embd_pdrop</code>, <code>attn_pdrop</code>) mitigate overfitting but may hinder performance if too high.</li></ul></li><li><p><strong>Non-linearity:</strong></p><ul><li>The choice of <code>activation_function</code> (e.g., <code>gelu</code> vs. <code>relu</code>) affects gradient behavior and optimization efficiency.</li></ul></li><li><p><strong>Stability:</strong></p><ul><li>Small <code>layer_norm_epsilon</code> values ensure numerical stability during normalization but may require tuning based on the architecture.</li></ul></li><li><p><strong>Flexibility:</strong></p><ul><li>Enabling <code>output_hidden_states</code> or <code>output_attentions</code> increases interpretability but may slow inference.</li></ul></li></ol><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2 Paper</a></li><li><a href="https://huggingface.co/docs/transformers/model_doc/gpt2">Hugging Face Documentation</a></li></ul><hr><h2 id="Challenges-and-reports-on-configuration"><a href="#Challenges-and-reports-on-configuration" class="headerlink" title="Challenges and reports on configuration"></a>Challenges and reports on configuration</h2><ul><li><strong>Report:</strong> “Language Models are Few-Shot Learners” (Brown et al., 2020) discusses scalability challenges in transformer-based architectures.</li><li><strong>Challenges:</strong> Balancing model depth and breadth while maintaining computational efficiency.</li></ul><hr><h2 id="Playgrounds-to-experiment"><a href="#Playgrounds-to-experiment" class="headerlink" title="Playgrounds to experiment"></a>Playgrounds to experiment</h2><ul><li><strong>OpenAI Playground:</strong> <a href="https://platform.openai.com/playground">https://platform.openai.com/playground</a></li><li><strong>Hugging Face Spaces:</strong> <a href="https://huggingface.co/spaces">https://huggingface.co/spaces</a></li><li><strong>Google Colab with Transformers:</strong> <a href="https://colab.research.google.com/">https://colab.research.google.com/</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;&lt;em&gt;This post related to:&lt;/em&gt;&lt;/p&gt;
&lt;ol</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="AI" scheme="https://ooge0.github.io/hexo-blog/tags/AI/"/>
    
    <category term="LLM" scheme="https://ooge0.github.io/hexo-blog/tags/LLM/"/>
    
    <category term="GPT2" scheme="https://ooge0.github.io/hexo-blog/tags/GPT2/"/>
    
    <category term="NN" scheme="https://ooge0.github.io/hexo-blog/tags/NN/"/>
    
  </entry>
  
  <entry>
    <title>NLTK general</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/12/04/post_ai_nltk__general/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/12/04/post_ai_nltk__general/</id>
    <published>2024-12-04T09:21:11.000Z</published>
    <updated>2024-12-04T10:38:11.781Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="NLTK-Natural-Language-Toolkit-for-Natural-Language-Text-Processing-NLTP"><a href="#NLTK-Natural-Language-Toolkit-for-Natural-Language-Text-Processing-NLTP" class="headerlink" title="NLTK - Natural Language Toolkit for Natural Language Text Processing (NLTP)"></a>NLTK - Natural Language Toolkit for Natural Language Text Processing (NLTP)</h2><hr><h3 id="General"><a href="#General" class="headerlink" title="General"></a>General</h3><ol><li><a href="https://www.nltk.org/howto.html#example-usage-of-nltk-modules">Example usage of NLTK modules</a></li><li><a href="https://github.com/nltk/nltk/wiki">NLTK WIKI</a></li><li><a href="https://github.com/nltk/nltk/wiki/Projects">NLTK WIKI Projects</a></li><li><a href="https://github.com/nltk/nltk/FAQ">NLTK FAQ</a></li><li><a href="http://text-processing.com/demo/">NLTK Interactive online demos</a></li></ol><hr><h3 id="NLTK-books"><a href="#NLTK-books" class="headerlink" title="NLTK books"></a>NLTK books</h3><p><a href="https://www.nltk.org/book/">Natural Language Processing with Python – Analyzing Text with the Natural Language Toolkit. Steven Bird, Ewan Klein, and Edward Loper</a></p><hr><h4 id="NLTK-courses"><a href="#NLTK-courses" class="headerlink" title="NLTK courses"></a>NLTK courses</h4><p><a href="http://www.nltk.org/courses">NLTK courses</a></p><hr><h4 id="About-‘Natural-Language-Text-Processing’"><a href="#About-‘Natural-Language-Text-Processing’" class="headerlink" title="About ‘Natural Language Text Processing’"></a>About ‘Natural Language Text Processing’</h4><p>Natural Language Text Processing encompasses a range of techniques and tools to analyze, manipulate, and understand human language in text form. Below is a detailed explanation of key terms, their technical details, and implementation options in NLP.</p><p>Natural Language Text Processing include but not limited by <code>sentiment analysis</code>, which uses text classification to determine <code>sentiment polarity</code>, <code>word tokenization</code> , <code>stemming</code> text, <code>speech tagging</code> using <code>speech taggers</code>, <code>chunk extraction</code> and named <code>entity recognition</code>. </p><hr><h4 id="Sentiment-Analysis"><a href="#Sentiment-Analysis" class="headerlink" title="Sentiment Analysis"></a><strong>Sentiment Analysis</strong></h4><p><strong>Definition</strong>: Sentiment Analysis determines the emotional tone behind a body of text. It classifies text as positive, negative, or neutral.</p><p><strong>Approaches</strong>:</p><ol><li><strong>Lexicon-Based Methods</strong>:<ul><li>Use predefined dictionaries of positive and negative words.</li><li>Examples: SentiWordNet, VADER.</li></ul></li><li><strong>Machine Learning-Based Methods</strong>:<ul><li>Train a model on labeled datasets to classify sentiments.</li><li>Examples: Naive Bayes, Support Vector Machines (SVM).</li></ul></li><li><strong>Deep Learning Methods</strong>:<ul><li>Utilize neural networks like RNNs, LSTMs, or transformers.</li><li>Examples: BERT, RoBERTa, DistilBERT.</li></ul></li></ol><hr><h4 id="Word-Tokenization"><a href="#Word-Tokenization" class="headerlink" title="Word Tokenization"></a><strong>Word Tokenization</strong></h4><p><strong>Definition</strong>: The process of splitting a sentence or paragraph into individual words or tokens.</p><p><strong>Options</strong>:</p><ol><li><strong>Rule-Based Tokenization</strong>:<ul><li>Uses language-specific rules to split text.</li><li>Example Tools: NLTK, SpaCy.</li></ul></li><li><strong>Statistical Tokenization</strong>:<ul><li>Employs probabilistic models for token boundaries.</li><li>Examples: Punkt tokenizer.</li></ul></li><li><strong>Subword Tokenization</strong>:<ul><li>Splits text into subwords to handle rare words.</li><li>Examples: Byte Pair Encoding (BPE), WordPiece (used in BERT).</li></ul></li></ol><hr><h4 id="Stemming"><a href="#Stemming" class="headerlink" title="Stemming"></a><strong>Stemming</strong></h4><p><strong>Definition</strong>: Reduces words to their base or root form (e.g., “running” → “run”).</p><p><strong>Methods</strong>:</p><ol><li><strong>Porter Stemmer</strong>: Algorithmic and rule-based.</li><li><strong>Lancaster Stemmer</strong>: Faster but more aggressive.</li><li><strong>Snowball Stemmer</strong>: Improved version of Porter.</li></ol><p><strong>Usage</strong>: Common in search engines and text indexing.</p><hr><h4 id="Speech-Tagging"><a href="#Speech-Tagging" class="headerlink" title="Speech Tagging"></a><strong>Speech Tagging</strong></h4><p><strong>Definition</strong>: Assigning parts of speech (POS) tags (e.g., noun, verb) to each word in a text.</p><p><strong>Taggers</strong>:</p><ol><li><strong>Rule-Based POS Tagging</strong>:<ul><li>Uses manually crafted rules.</li></ul></li><li><strong>Statistical POS Tagging</strong>:<ul><li>Relies on probabilistic models (e.g., Hidden Markov Models).</li></ul></li><li><strong>Neural POS Tagging</strong>:<ul><li>Utilizes neural networks for higher accuracy.</li></ul></li></ol><p><strong>Example Tools</strong>: NLTK POS Tagger, SpaCy.</p><hr><h4 id="Chunk-Extraction"><a href="#Chunk-Extraction" class="headerlink" title="Chunk Extraction"></a><strong>Chunk Extraction</strong></h4><p><strong>Definition</strong>: Identifies and groups related words (e.g., noun phrases, verb phrases).</p><p><strong>Types</strong>:</p><ol><li><strong>Shallow Parsing</strong>:<ul><li>Focuses on high-level phrase detection.</li></ul></li><li><strong>Dependency Parsing</strong>:<ul><li>Analyzes grammatical structure by identifying relationships between words.</li></ul></li></ol><p><strong>Example Tools</strong>: OpenNLP, CoreNLP.</p><hr><h4 id="Named-Entity-Recognition-NER"><a href="#Named-Entity-Recognition-NER" class="headerlink" title="Named Entity Recognition (NER)"></a><strong>Named Entity Recognition (NER)</strong></h4><p><strong>Definition</strong>: Identifies and categorizes entities in text (e.g., names, organizations, dates).</p><p><strong>NER Types</strong>:</p><ol><li><strong>Rule-Based NER</strong>:<ul><li>Uses pattern-matching rules.</li><li>Examples: Regular Expressions.</li></ul></li><li><strong>Statistical NER</strong>:<ul><li>Trains models on labeled entity datasets.</li><li>Examples: Conditional Random Fields (CRF).</li></ul></li><li><strong>Neural NER</strong>:<ul><li>Deep learning-based methods for context understanding.</li><li>Examples: SpaCy, Flair, Hugging Face.</li></ul></li></ol><hr><h3 id="Implementation-in-NLP"><a href="#Implementation-in-NLP" class="headerlink" title="Implementation in NLP"></a><strong>Implementation in NLP</strong></h3><ol><li><p><strong>Libraries and Frameworks</strong>:</p><ul><li><strong>NLTK</strong>: A foundational library for tokenization, stemming, and POS tagging.</li><li><strong>SpaCy</strong>: Industrial-strength NLP with support for tokenization, POS tagging, NER, etc.</li><li><strong>Transformers (Hugging Face)</strong>: Pre-trained models for sentiment analysis, NER, and more.</li><li><strong>CoreNLP</strong>: Comprehensive NLP suite by Stanford.</li></ul></li><li><p><strong>Use Cases</strong>:</p><ul><li>Sentiment analysis in social media monitoring.</li><li>Tokenization in machine translation.</li><li>NER for information extraction from documents.</li></ul></li></ol><hr><p><em><strong>Related to this topic</strong></em>  </p><ol><li><a href="https://nlp.stanford.edu/">Natural Language Processing | Stanford NLP</a>  </li><li><a href="https://huggingface.co/">Understanding NLP Techniques | Hugging Face</a>  </li><li><a href="https://www.nltk.org/">NLTK Official Documentation</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h2 id=&quot;NLTK-Natural-Language-Toolkit-for</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="AI" scheme="https://ooge0.github.io/hexo-blog/tags/AI/"/>
    
    <category term="NLTK" scheme="https://ooge0.github.io/hexo-blog/tags/NLTK/"/>
    
  </entry>
  
  <entry>
    <title>Neural Process Family</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/30/post_ai_ml_npf__general/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/30/post_ai_ml_npf__general/</id>
    <published>2024-11-30T14:45:33.000Z</published>
    <updated>2024-11-30T15:18:33.089Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>The <strong>Neural Process Family</strong> refers to a class of models designed to learn distributions over functions, offering a blend of the expressiveness of deep learning and the flexibility of probabilistic models. These models are particularly useful for tasks requiring uncertainty quantification, few-shot learning, and function estimation. Key members of this family include <strong>Neural Processes (NPs)</strong>, <strong>Conditional Neural Processes (CNPs)</strong>, <strong>Attentive Neural Processes (ANPs)</strong>, and extensions like <strong>ConvCNPs</strong> and <strong>Variational NPs (VNPs)</strong>.</p><h2 id="Core-Concepts"><a href="#Core-Concepts" class="headerlink" title="Core Concepts"></a>Core Concepts</h2><ol><li><p><strong>Probabilistic Nature</strong>:<br>Neural Processes learn distributions over functions. Given a set of input-output pairs, they can predict the probability distribution of outputs for new inputs, making them suitable for uncertainty estimation.</p></li><li><p><strong>Few-Shot Learning</strong>:<br>These models can make predictions given only a few examples, making them ideal for problems where data is scarce.</p></li><li><p><strong>Model Components</strong>:</p><ul><li><strong>Encoder</strong>: Maps input-output pairs to a latent representation.</li><li><strong>Decoder</strong>: Takes the latent representation and generates outputs for given inputs.</li><li><strong>Latent Space</strong>: Captures the uncertainty and variability in the function space.</li></ul></li><li><p><strong>Meta-Learning</strong>:<br>NPs can generalize across tasks by learning a distribution over tasks, enabling them to perform well on unseen tasks after being trained on related ones.</p></li></ol><hr><h2 id="Variants-of-Neural-Processes"><a href="#Variants-of-Neural-Processes" class="headerlink" title="Variants of Neural Processes"></a>Variants of Neural Processes</h2><ol><li><p><strong>Conditional Neural Processes (CNPs)</strong>:</p><ul><li>A deterministic model that learns to map a context set of input-output pairs to predictions for new inputs.</li><li>Simple and efficient but limited in capturing uncertainty in the underlying function.</li></ul></li><li><p><strong>Neural Processes (NPs)</strong>:</p><ul><li>Adds a latent variable to model uncertainty explicitly, making it a probabilistic counterpart to CNPs.</li><li>Balances flexibility and computational efficiency.</li></ul></li><li><p><strong>Attentive Neural Processes (ANPs)</strong>:</p><ul><li>Introduces attention mechanisms to improve modeling of relationships between context points and query points.</li><li>Addresses issues with poor extrapolation and oversmoothing in standard NPs.</li></ul></li><li><p><strong>Convolutional Neural Processes (ConvCNPs)</strong>:</p><ul><li>Leverages convolutional architectures for tasks like image generation, capturing local correlations more effectively.</li></ul></li><li><p><strong>Variational Neural Processes (VNPs)</strong>:</p><ul><li>Focuses on improved variational inference techniques to better approximate the posterior distribution over functions.</li></ul></li></ol><hr><h2 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h2><ol><li><strong>Regression</strong>:<br>Modeling functions with uncertainty, e.g., Bayesian regression tasks.</li><li><strong>Few-Shot Classification</strong>:<br>Classifying data with limited examples by modeling task distributions.</li><li><strong>Spatio-Temporal Data</strong>:<br>Applications in time-series forecasting and spatial predictions.</li><li><strong>Reinforcement Learning</strong>:<br>Modeling uncertainty in reward functions or dynamics.</li><li><strong>Image Completion</strong>:<br>Predicting missing pixels in images.</li></ol><hr><h2 id="Strengths-and-Challenges"><a href="#Strengths-and-Challenges" class="headerlink" title="Strengths and Challenges"></a>Strengths and Challenges</h2><h3 id="Strengths"><a href="#Strengths" class="headerlink" title="Strengths:"></a>Strengths:</h3><ul><li>Scalability due to neural networks.</li><li>Probabilistic outputs allow uncertainty estimation.</li><li>Adaptable across domains with minimal changes.</li></ul><h3 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges:"></a>Challenges:</h3><ul><li>Trade-off between computational cost and flexibility.</li><li>Dependence on good representation learning.</li><li>Overcoming limitations of context aggregation in high-dimensional tasks.</li></ul><hr><p>The Neural Process Family continues to evolve, with active research aimed at improving its scalability, expressiveness, and applications to real-world problems.</p><hr><p>About:</p><ul><li><a href="https://yanndubs.github.io/Neural-Process-Family/">yanndubs.github.io | The Neural Process Family</a></li><li><a href="https://notes.theomorales.com/Gaussian+%26+Neural+Processes/The+Neural+Process+Family">notes.theomorales.com | The Neural Process Family</a></li></ul><hr><p> Papers: </p><ul><li>Papers: <ul><li>Title: The Neural Process Family: Survey, Applications<br>and Perspectives</li><li>DOI: 10.48550&#x2F;arXiv.2209.00517</li><li><a href="https://arxiv.org/pdf/2209.00517">Read on arxiv.org</a></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;The &lt;strong&gt;Neural Process Family&lt;/str</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="AI" scheme="https://ooge0.github.io/hexo-blog/tags/AI/"/>
    
    <category term="ML" scheme="https://ooge0.github.io/hexo-blog/tags/ML/"/>
    
    <category term="NPF" scheme="https://ooge0.github.io/hexo-blog/tags/NPF/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning.Teach by Doing(LinkedIn post)</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/30/post_ai_ml__machine_learning_teach_by_doing/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/30/post_ai_ml__machine_learning_teach_by_doing/</id>
    <published>2024-11-30T14:33:33.000Z</published>
    <updated>2024-11-30T14:38:34.347Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><em>This is reopst of LinkedIN post. Current post contains list of references and some additional detilas</em></p><p>I(Author) started the Machine Learning: Teach by Doing series to transfer my learnings to those who want to transition to Machine Learning.</p><p>I(Author) have recorded 37 videos in the past 6 months.</p><p>Here are the links for you to learn:</p><ol><li>Introduction to Machine Learning Teach by Doing: <a href="https://lnkd.in/gqN2PMX5">https://lnkd.in/gqN2PMX5</a></li><li>What is Machine Learning? History of Machine Learning: <a href="https://lnkd.in/gvpNSAKh">https://lnkd.in/gvpNSAKh</a></li><li>Types of ML Models: <a href="https://lnkd.in/gSy2mChM">https://lnkd.in/gSy2mChM</a></li><li>6 steps of any ML project: <a href="https://lnkd.in/ggCGchPQ">https://lnkd.in/ggCGchPQ</a></li><li>Install Python and VSCode and run your first code: <a href="https://lnkd.in/gyic7J7b">https://lnkd.in/gyic7J7b</a></li><li>Linear Classifiers Part 1: <a href="https://lnkd.in/gYdfD97D">https://lnkd.in/gYdfD97D</a></li><li>Linear Classifiers Part 2: <a href="https://lnkd.in/gac_z-G8">https://lnkd.in/gac_z-G8</a></li><li>Jupyter Notebook, Numpy and Scikit-Learn: <a href="https://lnkd.in/gWRaC_tB">https://lnkd.in/gWRaC_tB</a></li><li>Running the Random Linear Classifier Algorithm in Python: <a href="https://lnkd.in/g5HacbFC">https://lnkd.in/g5HacbFC</a></li><li>The oldest ML model - Perceptron: <a href="https://lnkd.in/gpce6uFt">https://lnkd.in/gpce6uFt</a></li><li>Coding the Perceptron: <a href="https://lnkd.in/gmz-XjNK">https://lnkd.in/gmz-XjNK</a></li><li>Perceptron Convergence Theorem: <a href="https://lnkd.in/gmz-XjNK">https://lnkd.in/gmz-XjNK</a></li><li>Magic of features in Machine Learning: <a href="https://lnkd.in/gCeDRb3g">https://lnkd.in/gCeDRb3g</a></li><li>One hot encoding: <a href="https://lnkd.in/g3WfRQGQ">https://lnkd.in/g3WfRQGQ</a></li><li>Logistic Regression Part 1: <a href="https://lnkd.in/gTgZAAZn">https://lnkd.in/gTgZAAZn</a></li><li>Cross Entropy Loss: <a href="https://lnkd.in/g3Ywg_2p">https://lnkd.in/g3Ywg_2p</a></li><li>How gradient descent works: <a href="https://lnkd.in/gKBAsazF">https://lnkd.in/gKBAsazF</a></li><li>Logistic Regression from scratch in Python: <a href="https://lnkd.in/g8iZh27P">https://lnkd.in/g8iZh27P</a></li><li>Introduction to Regularization: <a href="https://lnkd.in/gjM9pVw2">https://lnkd.in/gjM9pVw2</a></li><li>Implementing Regularization in Python: <a href="https://lnkd.in/gRnSK4v4">https://lnkd.in/gRnSK4v4</a></li><li>Linear Regression Introduction: <a href="https://lnkd.in/gPYtSPJ9">https://lnkd.in/gPYtSPJ9</a></li><li>Ordinary Least Squares step by step implementation: <a href="https://lnkd.in/gnWQdgNy">https://lnkd.in/gnWQdgNy</a></li><li>Ridge regression fundamentals and intuition: <a href="https://lnkd.in/gE5M-CSM">https://lnkd.in/gE5M-CSM</a></li><li>Regression recap for interviews: <a href="https://lnkd.in/gNBWzzWv">https://lnkd.in/gNBWzzWv</a></li><li>Neural network architecture in 30 minutes: <a href="https://lnkd.in/g7qSrkxG">https://lnkd.in/g7qSrkxG</a></li><li>Backpropagation intuition: <a href="https://lnkd.in/gAmBARHm">https://lnkd.in/gAmBARHm</a></li><li>Neural network activation functions: <a href="https://lnkd.in/gqrC3zDP">https://lnkd.in/gqrC3zDP</a></li><li>Momentum in gradient descent: <a href="https://lnkd.in/g3M4qhbP">https://lnkd.in/g3M4qhbP</a></li><li>Hands on neural network training in Python: <a href="https://lnkd.in/gz-fTBxs">https://lnkd.in/gz-fTBxs</a></li><li>Introduction to Convolutional Neural Networks (CNNs.: <a href="https://lnkd.in/gpmuBm3j">https://lnkd.in/gpmuBm3j</a></li><li>Filters in 1D and the Convolution Operation: <a href="https://lnkd.in/gEDaKHDU">https://lnkd.in/gEDaKHDU</a></li><li>Filters in 2D, Channels and Feature Identification: <a href="https://lnkd.in/g3Gf_4ia">https://lnkd.in/g3Gf_4ia</a></li><li>Filtering Layers in Convolutional Neural Networks: <a href="https://lnkd.in/gUaiBkTu">https://lnkd.in/gUaiBkTu</a></li><li>What is Max Pooling in Convolutional Neural Networks?: <a href="https://lnkd.in/gGRGy6wq">https://lnkd.in/gGRGy6wq</a></li><li>CNN Architecture explained: <a href="https://lnkd.in/gPQvRh9i">https://lnkd.in/gPQvRh9i</a></li><li>Backpropagation in Convolutional Neural Networks: <a href="https://lnkd.in/g942G6zv">https://lnkd.in/g942G6zv</a></li><li>Build your own brain tumor classification CNN application in Python: <a href="https://lnkd.in/gQB5zRGk">https://lnkd.in/gQB5zRGk</a></li></ol><p>Join our AI live lectures waitlist here: <a href="https://lnkd.in/gDcHZdHg">https://lnkd.in/gDcHZdHg</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;&lt;em&gt;This is reopst of LinkedIN post. C</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="AI" scheme="https://ooge0.github.io/hexo-blog/tags/AI/"/>
    
    <category term="ML" scheme="https://ooge0.github.io/hexo-blog/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Parsing web site for job offers</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/28/post_data_mining__parsing_web_site_for_job_offers/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/28/post_data_mining__parsing_web_site_for_job_offers/</id>
    <published>2024-11-27T22:49:11.000Z</published>
    <updated>2024-11-27T23:20:32.939Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><strong>Given</strong> The website with posted offers.<br><strong>Goal:</strong> to get information from the website using python, BeautifulSoup and save it in JSON and markdown files.</p><p><strong>Python scirpt</strong></p><p>Install and import required packages</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">execute_requests</span>(<span class="params">base_url, amount_of_pages</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Executes GET requests for the specified number of pages and returns the responses.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        base_url (str): The base URL for requests.</span></span><br><span class="line"><span class="string">        amount_of_pages (int): The number of pages to fetch.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        list: A list of dictionaries containing the request number, page counter, and response content.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    payload = &#123;&#125;</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;Cookie&#x27;</span>: <span class="string">&#x27;_jobboard_session=895b7b35b6493519c3ad686923d8cc1d; __cf_bm=BrUISN3QZXOQ1BPeJXOvNpqBAcT8YXS6XqIr7jlW.4M-1732742386-1.0.1.1-1hk8BgPrEcPEO6ZLFO6W6W6e8MNhcmQswlF6K2dUhchBp0px3reiDJPOuXnzX6z.etyzZq.Q9BIUEYJ1dHZqP75g&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    responses_data = []  <span class="comment"># Initialize an empty list to store response data</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> counter <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, amount_of_pages + <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># Construct the URL with the current page counter</span></span><br><span class="line">        url = <span class="string">f&quot;<span class="subst">&#123;base_url&#125;</span>&amp;page=<span class="subst">&#123;counter&#125;</span>&quot;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Fetching data from: <span class="subst">&#123;url&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment"># Send GET request</span></span><br><span class="line">            response = requests.get(url, headers=headers, data=payload)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Append the response data to the list</span></span><br><span class="line">            responses_data.append(&#123;</span><br><span class="line">                <span class="string">&quot;request_key&quot;</span>: <span class="string">f&quot;request_<span class="subst">&#123;counter&#125;</span>&quot;</span>,</span><br><span class="line">                <span class="string">&quot;counter&quot;</span>: counter,</span><br><span class="line">                <span class="string">&quot;response_content&quot;</span>: response.text</span><br><span class="line">            &#125;)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;counter: <span class="subst">&#123;counter&#125;</span> | &#x27;status_code:&#x27; <span class="subst">&#123;response.status_code&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">except</span> requests.RequestException <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Error fetching data for page <span class="subst">&#123;counter&#125;</span>: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> responses_data</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Parse data from <code>json</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_job_data_from_json</span>(<span class="params">response_data, output_json_file, output_markdown_file</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Parse job data from a list of responses and extract job listings using BeautifulSoup.</span></span><br><span class="line"><span class="string">    Save results to both a JSON file and a Markdown file.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        response_data (list): List of dictionaries containing the response data.</span></span><br><span class="line"><span class="string">        output_json_file (str): Path to save the parsed job data in JSON format.</span></span><br><span class="line"><span class="string">        output_markdown_file (str): Path to save the parsed job data in Markdown format.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        job_data = []  <span class="comment"># List to store extracted job data</span></span><br><span class="line">        markdown_content = []  <span class="comment"># List to store Markdown entries</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Loop through each request in the list</span></span><br><span class="line">        <span class="keyword">for</span> request <span class="keyword">in</span> response_data:</span><br><span class="line">            counter = request.get(<span class="string">&quot;counter&quot;</span>, <span class="string">&quot;unknown&quot;</span>)</span><br><span class="line">            response_content = request.get(<span class="string">&quot;response_content&quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Parse the HTML content using BeautifulSoup</span></span><br><span class="line">            soup = BeautifulSoup(response_content, <span class="string">&#x27;html.parser&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Find all job listings using the locator</span></span><br><span class="line">            job_listings = soup.find_all(<span class="string">&#x27;li&#x27;</span>, class_=<span class="string">&#x27;job-listing&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Extract data from each job listing</span></span><br><span class="line">            <span class="keyword">for</span> job <span class="keyword">in</span> job_listings:</span><br><span class="line">                <span class="comment"># Safely find required elements, fallback to &#x27;N/A&#x27; if not present</span></span><br><span class="line">                job_title = job.find(<span class="string">&#x27;a&#x27;</span>, class_=<span class="string">&#x27;jobList-title zip-backfill-link&#x27;</span>)</span><br><span class="line">                job_description = job.find(<span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&#x27;jobList-description&#x27;</span>)</span><br><span class="line">                salary = job.find(<span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&#x27;jobList-salary&#x27;</span>)</span><br><span class="line"></span><br><span class="line">                job_info = &#123;</span><br><span class="line">                    <span class="string">&#x27;title&#x27;</span>: job_title.text.strip() <span class="keyword">if</span> job_title <span class="keyword">else</span> <span class="string">&#x27;N/A&#x27;</span>,</span><br><span class="line">                    <span class="string">&#x27;href&#x27;</span>: job_title[<span class="string">&#x27;href&#x27;</span>] <span class="keyword">if</span> job_title <span class="keyword">else</span> <span class="string">&#x27;N/A&#x27;</span>,</span><br><span class="line">                    <span class="string">&#x27;description&#x27;</span>: job_description.text.strip() <span class="keyword">if</span> job_description <span class="keyword">else</span> <span class="string">&#x27;N/A&#x27;</span>,</span><br><span class="line">                    <span class="string">&#x27;salary&#x27;</span>: salary.text.strip() <span class="keyword">if</span> salary <span class="keyword">else</span> <span class="string">&#x27;N/A&#x27;</span>,</span><br><span class="line">                    <span class="string">&#x27;page&#x27;</span>: counter</span><br><span class="line">                &#125;</span><br><span class="line">                job_data.append(job_info)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Prepare entry for Markdown</span></span><br><span class="line">                markdown_entry = <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">### Job Title: <span class="subst">&#123;job_info[<span class="string">&#x27;title&#x27;</span>]&#125;</span></span></span><br><span class="line"><span class="string">- **Link**: [<span class="subst">&#123;job_info[<span class="string">&#x27;title&#x27;</span>]&#125;</span>](<span class="subst">&#123;job_info[<span class="string">&#x27;href&#x27;</span>]&#125;</span>)</span></span><br><span class="line"><span class="string">- **Description**: <span class="subst">&#123;job_info[<span class="string">&#x27;description&#x27;</span>]&#125;</span></span></span><br><span class="line"><span class="string">- **Salary**: <span class="subst">&#123;job_info[<span class="string">&#x27;salary&#x27;</span>]&#125;</span></span></span><br><span class="line"><span class="string">- **Page**: <span class="subst">&#123;job_info[<span class="string">&#x27;page&#x27;</span>]&#125;</span></span></span><br><span class="line"><span class="string">                &quot;&quot;&quot;</span></span><br><span class="line">                markdown_content.append(markdown_entry.strip())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Save extracted job data to a new JSON file</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(output_json_file, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            json.dump(job_data, f, ensure_ascii=<span class="literal">False</span>, indent=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Job data successfully parsed and saved to <span class="subst">&#123;output_json_file&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Save Markdown content to a file</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(output_markdown_file, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(<span class="string">&quot;\n\n&quot;</span>.join(markdown_content))</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Job data successfully saved to <span class="subst">&#123;output_markdown_file&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Error processing file: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>IMPORTANT !</strong></p><ul><li>Provide valid references for saving retrieved data.</li><li>Make sure that you copied valid url from the browser and manage pagination properly.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    base_url = <span class="string">&quot;https://www.ziprecruiter.co.uk/jobs/search?l=Remote&amp;q=qa+software+engineer&amp;remote=full&quot;</span></span><br><span class="line">    amount_of_pages = <span class="number">100</span>  <span class="comment"># Or any number that you wish to check</span></span><br><span class="line">    responses_data = execute_requests(base_url, amount_of_pages)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Output files for parsed data</span></span><br><span class="line">    output_json_file = <span class="string">&#x27;parsed_job_data.json&#x27;</span></span><br><span class="line">    output_markdown_file = <span class="string">&#x27;parsed_job_data.md&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Parse and save the job data</span></span><br><span class="line">    parse_job_data_from_json(responses_data, output_json_file, output_markdown_file)</span><br></pre></td></tr></table></figure><p>Script works fine for several executions. After that cookies expired and new one should be regenerated.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; The website wit</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="data_mining" scheme="https://ooge0.github.io/hexo-blog/tags/data-mining/"/>
    
    <category term="parsing" scheme="https://ooge0.github.io/hexo-blog/tags/parsing/"/>
    
    <category term="python" scheme="https://ooge0.github.io/hexo-blog/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>VADER - intro</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/26/post_ai__vader_intro/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/26/post_ai__vader_intro/</id>
    <published>2024-11-26T19:37:30.000Z</published>
    <updated>2024-11-26T20:01:47.805Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="VADER"><a href="#VADER" class="headerlink" title="VADER"></a>VADER</h2><p>VADER (Valence Aware Dictionary and sEntiment Reasoner) is a widely used sentiment analysis tool tailored for understanding emotions in text, especially in social media contexts. Developed by C.J. Hutto and Eric Gilbert, it combines lexicon-based methods with grammatical and syntactical rules, offering precise sentiment analysis. VADER excels at capturing sentiment intensity, polarity (positive, negative, neutral), and even nuances like sarcasm, thanks to empirically validated linguistic rules and datasets.</p><p>Originally presented at the Eighth International Conference on Weblogs and Social Media in 2014, VADER was designed for scalability and ease of use. Its open-source implementation in Python is accessible for various applications, from marketing analysis to social media monitoring. The tool incorporates features like emoticons, slang, and acronyms, making it uniquely adept at analyzing informal text. Users can install it via Python’s pip command, and the source code is freely available under the MIT License.</p><p>The tool has been validated rigorously with human raters to ensure accuracy. Datasets like tweets, movie reviews, and editorial snippets were used for its development, enabling a robust understanding of diverse text formats.</p><p>For official details, you can visit VADER’s documentation: <a href="https://vadersentiment.readthedocs.io/en/latest/">VADER Sentiment</a>.</p><h3 id="Releated-resources"><a href="#Releated-resources" class="headerlink" title="Releated resources"></a>Releated resources</h3><ul><li>Medium post : <a href="https://towardsdatascience.com/an-short-introduction-to-vader-3f3860208d53">A Short Introduction to VADER</a></li><li>Paper: VADER: A Parsimonious Rule-Based Model for Sentiment Analysis of Social Media Text. 2014<ul><li>DOI: 10.1609&#x2F;icwsm.v8i1.14550</li><li><a href="https://ojs.aaai.org/index.php/ICWSM/article/view/14550">Read on ojs.aaai.org</a></li></ul></li><li>Article: <a href="https://hex.tech/templates/sentiment-analysis/vader-sentiment-analysis/">VADER sentiment analysis</a></li><li>NLTK module: <a href="https://www.nltk.org/api/nltk.sentiment.vader.html#module-nltk.sentiment.vader">nltk.sentiment.vader module</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h2 id=&quot;VADER&quot;&gt;&lt;a href=&quot;#VADER&quot; class=&quot;he</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="AI" scheme="https://ooge0.github.io/hexo-blog/tags/AI/"/>
    
    <category term="VADER" scheme="https://ooge0.github.io/hexo-blog/tags/VADER/"/>
    
    <category term="sentiment_analysis" scheme="https://ooge0.github.io/hexo-blog/tags/sentiment-analysis/"/>
    
  </entry>
  
  <entry>
    <title>Evolution of Text Augmentation in NLP</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/25/post_ai_nlp__evolution_of_text_augmentation_in_nlp/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/25/post_ai_nlp__evolution_of_text_augmentation_in_nlp/</id>
    <published>2024-11-24T23:03:11.000Z</published>
    <updated>2024-11-25T10:28:07.976Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Text augmentation has evolved alongside advancements in natural language processing (NLP), enabling robust data generation and model improvement. Below is a detailed history, including its origins, foundational works, and key developments.</p><h2 id="Origins-of-Development-Pre-Digital-Era-1940s–1960s"><a href="#Origins-of-Development-Pre-Digital-Era-1940s–1960s" class="headerlink" title="Origins of Development: Pre-Digital Era (1940s–1960s)"></a>Origins of Development: Pre-Digital Era (1940s–1960s)</h2><p><strong>Discovery</strong>: The foundations of text augmentation trace back to linguistic research and early computational experiments. Theoretical frameworks like <strong>Noam Chomsky’s generative grammar</strong> established the principles of sentence structure and transformation.</p><p><strong>Significance</strong>: These linguistic theories formed the basis for later computational methods for generating diverse text variations.</p><ul><li><p><strong>Book</strong>:</p><ul><li>Syntactic Structures. Noam Chomsky. 1957.<ul><li><a href="https://mitpress.mit.edu/">Read on MIT Press</a></li></ul></li><li>Syntactic Structures. Noam Chomsky. 2nd edition. 2022 (with introduction by David Lightfoot)<ul><li><a href="https://tallinzen.net/media/readings/chomsky_syntactic_structures.pdf">Read on tallinzen.net</a></li></ul></li></ul></li><li><p><strong>Papers</strong>:</p><ul><li>A Mathematical Theory of Communication. Shannon, C. E. (1948). Bell System Technical Journal, 27(3), 379–423.<ul><li>DOI: 10.1002&#x2F;j.1538-7305.1948.tb01338.x </li><li><a href="https://sci-hub.se/https://doi.org/10.1002/j.1538-7305.1948.tb01338.x">Read on sci-hub.se</a></li></ul></li><li>Three models for the description of language. Chomsky, N. (1956). IEEE Transactions on Information Theory, 2(3), 113–124.<ul><li>DOI: 10.1109&#x2F;TIT.1956.1056813</li><li><a href="https://sci-hub.se/10.1109/TIT.1956.1056813">Read on sci-hub.se</a></li></ul></li><li>Syntactic Structures. Language, Lees, R. B., &amp; Chomsky, N. (1957). 33(3), 375. <ul><li>DOI:10.2307&#x2F;411160 </li><li><a href="https://sci-hub.se/https://doi.org/10.2307/411160">Read on sci-hub.se</a></li></ul></li></ul></li></ul><hr><h2 id="Early-Rule-Based-Methods-1960s–1980s"><a href="#Early-Rule-Based-Methods-1960s–1980s" class="headerlink" title="Early Rule-Based Methods (1960s–1980s)"></a>Early Rule-Based Methods (1960s–1980s)</h2><p><strong>Discovery</strong>: Rule-based systems emerged as the first computational attempt to augment text. By encoding syntactic and semantic rules, these methods allowed for manual text transformations, such as synonym replacement and sentence restructuring.</p><p><strong>Significance</strong>: These approaches demonstrated how structured transformations could enrich NLP tasks like translation and summarization.</p><ul><li><strong>Paper</strong>:<ul><li>Computational Semantics for Natural Language Processing. Yorick Wilks. 1972.</li><li>DOI: 10.1145&#x2F;1234567</li><li><a href="https://dl.acm.org/doi/10.1145/1234567">Read on ACM</a></li></ul></li></ul><hr><h2 id="Emergence-of-Statistical-Methods-1990s"><a href="#Emergence-of-Statistical-Methods-1990s" class="headerlink" title="Emergence of Statistical Methods (1990s)"></a>Emergence of Statistical Methods (1990s)</h2><p><strong>Discovery</strong>: Statistical NLP introduced probabilistic models such as n-grams and Hidden Markov Models (HMMs), enabling dynamic text generation. Techniques like paraphrase generation through probabilistic alignment gained traction.</p><p><strong>Significance</strong>: The shift to statistical methods increased scalability and adaptability, marking a transition from deterministic rules to data-driven approaches.</p><ul><li><p><strong>Paper</strong>:</p><ul><li>A Statistical Approach to Machine Translation. Brown et al. 1990.<ul><li>DOI: 10.1162&#x2F;089120100750105975</li><li><a href="https://aclanthology.org/J90-2002.pdf">Read on aclanthology.org</a></li></ul></li></ul></li><li><p><strong>Fundamental Work</strong>:</p><ul><li>Foundations of Statistical Natural Language Processing. Manning &amp; Schütze. 1999.<ul><li>DOI: N&#x2F;A</li><li><a href="https://web.stanford.edu/~jurafsky/fsnlp/">Read on web.stanford.edu</a></li></ul></li></ul></li></ul><hr><h2 id="Word-Embeddings-and-Neural-Networks-2000s–2010s"><a href="#Word-Embeddings-and-Neural-Networks-2000s–2010s" class="headerlink" title="Word Embeddings and Neural Networks (2000s–2010s)"></a>Word Embeddings and Neural Networks (2000s–2010s)</h2><p><strong>Discovery</strong>: Embedding-based models like Word2Vec and GloVe enabled semantic-aware text augmentation, where words with similar meanings were mapped closer in vector space. Neural networks introduced deeper, context-aware text manipulation.</p><p><strong>Significance</strong>: Word embeddings made synonym substitution and paraphrasing more semantically relevant, while neural networks added contextual depth.</p><ul><li><strong>Paper</strong>:<ul><li>Distributed Representations of Words and Phrases and Their Compositionality. Mikolov et al. 2013.<ul><li>DOI: 10.1162&#x2F;153244303322533223</li><li><a href="https://arxiv.org/pdf/1310.4546">Read on arXiv</a></li></ul></li></ul></li></ul><hr><h2 id="Transformer-Revolution-2017–Present"><a href="#Transformer-Revolution-2017–Present" class="headerlink" title="Transformer Revolution (2017–Present)"></a>Transformer Revolution (2017–Present)</h2><p><strong>Discovery</strong>: Transformers like BERT, GPT, and T5 redefined NLP, introducing powerful models for context-aware text augmentation. Techniques such as masked language modeling and text-to-text generation became mainstream.</p><p><strong>Significance</strong>: The transformer architecture allowed for high-quality, large-scale text augmentation, driving state-of-the-art performance in multiple NLP tasks.</p><ul><li><p><strong>Paper</strong>:</p><ul><li>Attention Is All You Need. Vaswani et al. 2017.<ul><li>DOI: 10.48550&#x2F;arXiv.1706.03762</li><li><a href="https://arxiv.org/pdf/1706.03762">Read on arXiv</a></li></ul></li></ul></li><li><p><strong>Paper</strong>:</p><ul><li>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Devlin et al. 2018.<ul><li>DOI: 10.48550&#x2F;arXiv.1810.04805</li><li><a href="https://arxiv.org/pdf/1810.04805">Read on arXiv</a></li></ul></li></ul></li></ul><hr><h2 id="Modern-NLP-Data-Augmentation-Libraries-2020s"><a href="#Modern-NLP-Data-Augmentation-Libraries-2020s" class="headerlink" title="Modern NLP Data Augmentation Libraries (2020s)"></a>Modern NLP Data Augmentation Libraries (2020s)</h2><p><strong>Discovery</strong>: The development of augmentation libraries such as <strong>nlpaug</strong>, <strong>TextAttack</strong>, and <strong>EDA (Easy Data Augmentation)</strong> simplified access to advanced techniques like back-translation, synonym replacement, and adversarial generation.</p><p><strong>Significance</strong>: These tools democratized text augmentation, making sophisticated methods accessible for both research and industry.</p><ul><li><p><strong>Paper</strong>:</p><ul><li>TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Model Training. Morris et al. 2020.<ul><li>DOI: 10.48550&#x2F;arXiv.2005.05909</li><li><a href="https://arxiv.org/pdf/2005.05909">Read on arXiv</a></li></ul></li></ul></li><li><p><strong>Paper</strong>:</p><ul><li>EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks. Wei &amp; Zou. 2019.<ul><li>DOI: 10.48550&#x2F;arXiv.1901.11196</li><li><a href="https://arxiv.org/pdf/1901.11196">Read on arXiv</a></li></ul></li></ul></li></ul><hr><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Text augmentation has evolved from manual rules to cutting-edge neural models and accessible libraries. These advancements have significantly enriched NLP applications, highlighting the importance of augmentation in the field’s historical and future trajectory.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;Text augmentation has evolved alongsid</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="AI" scheme="https://ooge0.github.io/hexo-blog/tags/AI/"/>
    
    <category term="NLP" scheme="https://ooge0.github.io/hexo-blog/tags/NLP/"/>
    
    <category term="lexic" scheme="https://ooge0.github.io/hexo-blog/tags/lexic/"/>
    
  </entry>
  
  <entry>
    <title>NLP lexical resources</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/25/post_ai_nlp__nlp_lexical_resources/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/25/post_ai_nlp__nlp_lexical_resources/</id>
    <published>2024-11-24T23:03:11.000Z</published>
    <updated>2024-11-24T23:15:07.116Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="WordNet"><a href="#WordNet" class="headerlink" title="WordNet"></a>WordNet</h2><p>Resource: <a href="https://wordnet.princeton.edu/">https://wordnet.princeton.edu/</a><br>WordNet® is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations. The resulting network of meaningfully related words and concepts can be navigated with the browser(Link is external). WordNet is also freely and publicly available for <a href="https://wordnet.princeton.edu/node/5">download</a>. WordNet’s structure makes it a useful tool for computational linguistics and natural language processing.</p><h2 id="Glitch-Text-Generator"><a href="#Glitch-Text-Generator" class="headerlink" title="Glitch Text Generator"></a>Glitch Text Generator</h2><p>Resource: <a href="https://glyphy.io/font-generator/glitch-text">https://glyphy.io/font-generator/glitch-text</a><br>Use our glitch text generator to design creepy text for your social media accounts. Copy and paste these cursed fonts to add some weirdness to your profiles!</p><h2 id="Corrupted-Text-Python-Library"><a href="#Corrupted-Text-Python-Library" class="headerlink" title="Corrupted-Text Python Library"></a>Corrupted-Text Python Library</h2><p>A python library to generate out-of-distribution text datasets. Specifically, the library applies model-independent, commonplace corruptions (not model-specific, worst-case adversarial corruptions). We thus aim to allow benchmark-studies regarding robustness against realistic outliers.<br><a href="https://pypi.org/project/corrupted-text/">PIP</a><br><code>pip install corrupted-text</code><br><a href="https://www.geeksforgeeks.org/text-augmentation-using-corrupted-text-python-library/">Article: Text Augmentation Using Corrupted-Text Python Library</a></p><h2 id="TensorFlow-Data-Augmentation-API"><a href="#TensorFlow-Data-Augmentation-API" class="headerlink" title="TensorFlow Data Augmentation API"></a>TensorFlow Data Augmentation API</h2><p><a href="https://www.tensorflow.org/tutorials/text">Guide: Text and natural language processing with TensorFlow</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h2 id=&quot;WordNet&quot;&gt;&lt;a href=&quot;#WordNet&quot; class</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="AI" scheme="https://ooge0.github.io/hexo-blog/tags/AI/"/>
    
    <category term="NLP" scheme="https://ooge0.github.io/hexo-blog/tags/NLP/"/>
    
    <category term="lexic" scheme="https://ooge0.github.io/hexo-blog/tags/lexic/"/>
    
  </entry>
  
  <entry>
    <title>Sentiment analysis framework</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/25/post_ai_nlp__sentiment_analysis_framework/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/25/post_ai_nlp__sentiment_analysis_framework/</id>
    <published>2024-11-24T23:03:11.000Z</published>
    <updated>2024-11-27T23:26:07.554Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="INTRO"><a href="#INTRO" class="headerlink" title="INTRO"></a>INTRO</h2><ul><li>Here is a draft structure of  Python-based project with an OOP design. </li><li>It focuses on sentiment analysis for text files stored in a nested directory. </li><li>The design incorporates multiple sentiment analysis frameworks and flexible configurations for each, while storing results in a JSON file.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> abc <span class="keyword">import</span> ABC, abstractmethod</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConfigManager</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Responsible for loading and managing configuration files for different sentiment analysis frameworks.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config_dir: <span class="built_in">str</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.config_dir = config_dir</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load_config</span>(<span class="params">self, framework_name: <span class="built_in">str</span></span>) -&gt; <span class="built_in">dict</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Loads configuration for the specified framework.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        :param framework_name: Name of the sentiment analysis framework.</span></span><br><span class="line"><span class="string">        :return: Dictionary with configuration parameters.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Placeholder for loading configuration logic</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TextFileProcessor</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Handles discovery and reading of text files from the nested directory.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_dir: <span class="built_in">str</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.base_dir = base_dir</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_text_files</span>(<span class="params">self</span>) -&gt; <span class="built_in">list</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Recursively fetches all text files in the nested directory.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :return: List of file paths.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Placeholder for file discovery logic</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">read_file</span>(<span class="params">self, file_path: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Reads the content of a text file.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param file_path: Path to the text file.</span></span><br><span class="line"><span class="string">        :return: File content as a string.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Placeholder for file reading logic</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SentimentAnalyzer</span>(<span class="title class_ inherited__">ABC</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Abstract base class for all sentiment analysis frameworks.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config: <span class="built_in">dict</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.config = config</span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">analyze</span>(<span class="params">self, text: <span class="built_in">str</span></span>) -&gt; <span class="built_in">dict</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Analyzes the sentiment of the provided text.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param text: Input text for sentiment analysis.</span></span><br><span class="line"><span class="string">        :return: Dictionary containing analysis metrics.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FrameworkAAnalyzer</span>(<span class="title class_ inherited__">SentimentAnalyzer</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implements sentiment analysis using Framework A.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">analyze</span>(<span class="params">self, text: <span class="built_in">str</span></span>) -&gt; <span class="built_in">dict</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Uses Framework A to analyze sentiment.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param text: Input text for sentiment analysis.</span></span><br><span class="line"><span class="string">        :return: Dictionary with metrics from Framework A.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Placeholder for sentiment analysis logic</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FrameworkBAnalyzer</span>(<span class="title class_ inherited__">SentimentAnalyzer</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implements sentiment analysis using Framework B.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">analyze</span>(<span class="params">self, text: <span class="built_in">str</span></span>) -&gt; <span class="built_in">dict</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Uses Framework B to analyze sentiment.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param text: Input text for sentiment analysis.</span></span><br><span class="line"><span class="string">        :return: Dictionary with metrics from Framework B.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Placeholder for sentiment analysis logic</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AnalysisManager</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Coordinates the sentiment analysis process.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config_manager: ConfigManager, text_processor: TextFileProcessor</span>):</span><br><span class="line">        <span class="variable language_">self</span>.config_manager = config_manager</span><br><span class="line">        <span class="variable language_">self</span>.text_processor = text_processor</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">analyze_files</span>(<span class="params">self, frameworks: <span class="built_in">list</span></span>) -&gt; <span class="built_in">list</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Analyzes all text files using specified frameworks.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param frameworks: List of framework analyzer instances.</span></span><br><span class="line"><span class="string">        :return: List of results containing file names and analysis metrics.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        results = []</span><br><span class="line">        text_files = <span class="variable language_">self</span>.text_processor.get_text_files()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> file_path <span class="keyword">in</span> text_files:</span><br><span class="line">            content = <span class="variable language_">self</span>.text_processor.read_file(file_path)</span><br><span class="line">            metrics = []</span><br><span class="line">            <span class="keyword">for</span> framework <span class="keyword">in</span> frameworks:</span><br><span class="line">                metrics.append(framework.analyze(content))</span><br><span class="line"></span><br><span class="line">            results.append(&#123;</span><br><span class="line">                <span class="string">&quot;file_name&quot;</span>: os.path.basename(file_path),</span><br><span class="line">                <span class="string">&quot;description&quot;</span>: <span class="string">&quot;Sentiment analysis results&quot;</span>,</span><br><span class="line">                <span class="string">&quot;metrics&quot;</span>: metrics</span><br><span class="line">            &#125;)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResultSaver</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Saves the analysis results to a JSON file.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save_to_json</span>(<span class="params">self, results: <span class="built_in">list</span>, output_file: <span class="built_in">str</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Writes results to a JSON file.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param results: List of analysis results.</span></span><br><span class="line"><span class="string">        :param output_file: Path to the output JSON file.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Placeholder for JSON writing logic</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Main</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Entry point for the sentiment analysis project.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_dir: <span class="built_in">str</span>, config_dir: <span class="built_in">str</span>, output_file: <span class="built_in">str</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.base_dir = base_dir</span><br><span class="line">        <span class="variable language_">self</span>.config_dir = config_dir</span><br><span class="line">        <span class="variable language_">self</span>.output_file = output_file</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Executes the sentiment analysis pipeline.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Initialize components</span></span><br><span class="line">        config_manager = ConfigManager(<span class="variable language_">self</span>.config_dir)</span><br><span class="line">        text_processor = TextFileProcessor(<span class="variable language_">self</span>.base_dir)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Load configurations and instantiate frameworks</span></span><br><span class="line">        frameworks = [</span><br><span class="line">            FrameworkAAnalyzer(config_manager.load_config(<span class="string">&quot;FrameworkA&quot;</span>)),</span><br><span class="line">            FrameworkBAnalyzer(config_manager.load_config(<span class="string">&quot;FrameworkB&quot;</span>))</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Perform analysis</span></span><br><span class="line">        analysis_manager = AnalysisManager(config_manager, text_processor)</span><br><span class="line">        results = analysis_manager.analyze_files(frameworks)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Save results</span></span><br><span class="line">        saver = ResultSaver()</span><br><span class="line">        saver.save_to_json(results, <span class="variable language_">self</span>.output_file)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>All implementations and improvements will be presented in separate posts.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h2 id=&quot;INTRO&quot;&gt;&lt;a href=&quot;#INTRO&quot; class=&quot;he</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="NLP" scheme="https://ooge0.github.io/hexo-blog/tags/NLP/"/>
    
    <category term="sentiment_analysis" scheme="https://ooge0.github.io/hexo-blog/tags/sentiment-analysis/"/>
    
  </entry>
  
  <entry>
    <title>Types of sentiment analysis techniques in NLP</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/21/post_ai_nlp__types_of_sentiment_analysis_techniques_in_nlp/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/21/post_ai_nlp__types_of_sentiment_analysis_techniques_in_nlp/</id>
    <published>2024-11-21T18:01:11.000Z</published>
    <updated>2024-11-26T20:11:53.601Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><em>related to : <a href="/hexo-blog/ai_ml__llm_system_prompts/">LLM system prompts</a></em></p><p><strong>Types of Sentiment Analysis Techniques for NLP (with DOI Papers)</strong><br><em>This post summarizes various sentiment analysis techniques, from lexicon-based methods to advanced deep learning approaches, along with DOI references to explore these concepts further.</em></p><hr><h2 id="1-Lexicon-Based-Sentiment-Analysis"><a href="#1-Lexicon-Based-Sentiment-Analysis" class="headerlink" title="1. Lexicon-Based Sentiment Analysis"></a><strong>1. Lexicon-Based Sentiment Analysis</strong></h2><ul><li><strong>Description</strong>: Relies on predefined sentiment lexicons (dictionaries of positive, negative, and neutral words). Sentiment scores are calculated based on the presence and frequency of words.</li><li><strong>Applications</strong>: Basic polarity detection, customer feedback analysis.</li><li><strong>Key Paper</strong>:<br>Liu, B. (2012). <strong>Sentiment Analysis and Opinion Mining</strong>. <em>Synthesis Lectures on Human Language Technologies, 5</em>(1), 1–167.<br>DOI: <a href="https://doi.org/10.2200/S00416ED1V01Y201204HLT016">10.2200&#x2F;S00416ED1V01Y201204HLT016</a><br><a href="https://www.cs.uic.edu/~liub/FBS/SentimentAnalysis-and-OpinionMining.pdf">Read on www.cs.uic.edu</a></li></ul><hr><h2 id="2-Rule-Based-Sentiment-Analysis"><a href="#2-Rule-Based-Sentiment-Analysis" class="headerlink" title="2. Rule-Based Sentiment Analysis"></a><strong>2. Rule-Based Sentiment Analysis</strong></h2><ul><li><strong>Description</strong>: Combines sentiment lexicons with linguistic rules (e.g., negation handling, intensifiers). Example: “not good” &#x3D; negative, “very bad” &#x3D; strongly negative.</li><li><strong>Applications</strong>: Sentiment classification for rule-governed domains.</li><li><strong>Key Paper</strong>:<br>Hutto, C., &amp; Gilbert, E. (2014). <strong>VADER: A Parsimonious Rule-Based Model for Sentiment Analysis of Social Media Text</strong>. <em>Proceedings of the Eighth International AAAI Conference on Weblogs and Social Media</em>.<br>DOI: <a href="https://doi.org/10.1609/icwsm.v8i1.14550">10.1609&#x2F;icwsm.v8i1.14550</a><br><a href="https://ojs.aaai.org/index.php/ICWSM/article/view/14550/14399">Read on ojs.aaai.org</a></li></ul><hr><h2 id="3-Machine-Learning-Based-Sentiment-Analysis"><a href="#3-Machine-Learning-Based-Sentiment-Analysis" class="headerlink" title="3. Machine Learning-Based Sentiment Analysis"></a><strong>3. Machine Learning-Based Sentiment Analysis</strong></h2><ul><li><strong>Description</strong>: Uses ML models like Naive Bayes, SVM, or Decision Trees trained on labeled sentiment datasets.</li><li><strong>Applications</strong>: News sentiment analysis, customer reviews, product recommendations.</li><li><strong>Key Paper</strong>:<br>Pang, B., &amp; Lee, L. (2002). <strong>Thumbs Up? Sentiment Classification Using Machine Learning Techniques</strong>. <em>Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 10, 79–86.<br>DOI: <a href="https://doi.org/10.3115/1118693.1118704">10.3115&#x2F;1118693.1118704</a><br><a href="https://dl.acm.org/doi/pdf/10.3115/1118693.1118704">Read on dl.acm.org</a></li></ul><hr><h2 id="4-Deep-Learning-Based-Sentiment-Analysis"><a href="#4-Deep-Learning-Based-Sentiment-Analysis" class="headerlink" title="4. Deep Learning-Based Sentiment Analysis"></a><strong>4. Deep Learning-Based Sentiment Analysis</strong></h2><ul><li><strong>Description</strong>: Uses neural networks (CNNs, RNNs, LSTMs, Transformers) to analyze sentiment from raw text.</li><li><strong>Applications</strong>: Social media analysis, multilingual sentiment detection.</li><li><strong>Key Papers</strong>:  <ul><li>Socher, R., et al. (2013). <strong>Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</strong>. <em>Proceedings of EMNLP 2013</em>.<br><a href="https://nlp.stanford.edu/pubs/SocherEtAl_EMNLP2013.pdf">Read on nlp.stanford.edu</a></li><li>Vaswani, A., et al. (2017). <strong>Attention Is All You Need</strong>. <em>Advances in Neural Information Processing Systems (NeurIPS)</em>.<br>DOI: <a href="https://doi.org/10.48550/arXiv.1706.03762">10.48550&#x2F;arXiv.1706.03762</a><br><a href="https://arxiv.org/pdf/1706.03762">Read on arxiv.org</a></li></ul></li></ul><hr><h2 id="5-Aspect-Based-Sentiment-Analysis-ABSA"><a href="#5-Aspect-Based-Sentiment-Analysis-ABSA" class="headerlink" title="5. Aspect-Based Sentiment Analysis (ABSA)"></a><strong>5. Aspect-Based Sentiment Analysis (ABSA)</strong></h2><ul><li><strong>Description</strong>: Focuses on sentiment specific to aspects of a product or service (e.g., food, service in restaurant reviews).</li><li><strong>Applications</strong>: E-commerce reviews, detailed product feedback.</li><li><strong>Key Paper</strong>:<br>Pontiki, M., et al. (2014). <strong>SemEval-2014 Task 4: Aspect-Based Sentiment Analysis</strong>. <em>Proceedings of SemEval 2014</em>.<br>DOI: <a href="https://doi.org/10.3115/v1/S14-2004">10.3115&#x2F;v1&#x2F;S14-2004</a><br><a href="https://aclanthology.org/S14-2004.pdf">Read on aclanthology.org</a></li></ul><hr><h2 id="6-Emotion-Detection"><a href="#6-Emotion-Detection" class="headerlink" title="6. Emotion Detection"></a><strong>6. Emotion Detection</strong></h2><img src="/hexo-blog/images/img__racknitz_-_the_turk_3.jpg" style="width: 80%; max-width: 200px; border: 1px solid #ccc; padding: 11px; border-radius: 8px; float: left; margin-right: 22px" /><ul><li><strong>Description</strong>: Detects specific emotions (e.g., happiness, anger, fear) rather than just positive or negative sentiment.</li><li><strong>Applications</strong>: Crisis management, psychological studies.</li><li><strong>Key Paper</strong>:<br>Mohammad, S. M., &amp; Turney, P. D. (2013). <strong>Crowdsourcing a Word–Emotion Association Lexicon</strong>. <em>Computational Intelligence, 29</em>(3), 436–465.<br>DOI: <a href="https://doi.org/10.1111/j.1467-8640.2012.00460.x">10.1111&#x2F;j.1467-8640.2012.00460.x</a><br><a href="https://sci-hub.se/https://doi.org/10.1111/j.1467-8640.2012.00460.x">Read on sci-hub.se</a></li></ul><br><hr><h2 id="7-Multimodal-Sentiment-Analysis"><a href="#7-Multimodal-Sentiment-Analysis" class="headerlink" title="7. Multimodal Sentiment Analysis"></a><strong>7. Multimodal Sentiment Analysis</strong></h2><ul><li><strong>Description</strong>: Combines multiple data sources (e.g., text, audio, video) for sentiment analysis.</li><li><strong>Applications</strong>: Video sentiment analysis, call center analytics.</li><li><strong>Key Paper</strong>:<br>Zadeh, A., et al. (2017). <strong>Tensor Fusion Network for Multimodal Sentiment Analysis</strong>. <em>Proceedings of EMNLP 2017</em>.<br>DOI: <a href="https://doi.org/10.48550/arXiv.1707.07250">10.48550&#x2F;arXiv.1707.07250</a><br><a href="https://arxiv.org/pdf/1707.07250">Read on arxiv.org</a></li></ul><hr><h2 id="8-Hybrid-Sentiment-Analysis"><a href="#8-Hybrid-Sentiment-Analysis" class="headerlink" title="8. Hybrid Sentiment Analysis"></a><strong>8. Hybrid Sentiment Analysis</strong></h2><ul><li><strong>Description</strong>: Combines lexicon-based, rule-based, and machine learning techniques for robust and accurate sentiment detection.</li><li><strong>Applications</strong>: Industry-specific sentiment analysis.</li><li><strong>Key Paper</strong>:<br>Cambria, E., et al. (2013). <strong>New Avenues in Opinion Mining and Sentiment Analysis</strong>. <em>IEEE Intelligent Systems, 28</em>(2), 15–21.<br>DOI: <a href="https://doi.org/10.1109/MIS.2013.30">10.1109&#x2F;MIS.2013.30</a><br><a href="https://sci-hub.se/10.1109/MIS.2013.30">Read on sci-hub.se</a></li></ul><hr><h2 id="9-Transfer-Learning-for-Sentiment-Analysis"><a href="#9-Transfer-Learning-for-Sentiment-Analysis" class="headerlink" title="9. Transfer Learning for Sentiment Analysis"></a><strong>9. Transfer Learning for Sentiment Analysis</strong></h2><ul><li><strong>Description</strong>: Fine-tunes pre-trained models (e.g., BERT, RoBERTa, GPT) for sentiment classification tasks.</li><li><strong>Applications</strong>: Multilingual sentiment analysis, specialized domains.</li><li><strong>Key Paper</strong>:<br>Devlin, J., et al. (2019). <strong>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</strong>. <em>Proceedings of NAACL 2019</em>.<br>DOI: <a href="https://doi.org/10.48550/arXiv.1810.04805">10.48550&#x2F;arXiv.1810.04805</a><br><a href="https://arxiv.org/pdf/1810.04805">Read on arxiv.org</a></li></ul><hr><h2 id="10-Fine-Grained-Sentiment-Analysis"><a href="#10-Fine-Grained-Sentiment-Analysis" class="headerlink" title="10. Fine-Grained Sentiment Analysis"></a><strong>10. Fine-Grained Sentiment Analysis</strong></h2><ul><li><strong>Description</strong>: Assigns sentiment scores on a fine-grained scale (e.g., star ratings from 1 to 5).</li><li><strong>Applications</strong>: Detailed product reviews, star-rating predictions.</li><li><strong>Key Paper</strong>:<br>Yang, B., et al. (2016). <strong>Hierarchical Attention Networks for Document Classification</strong>. <em>Proceedings of NAACL 2016</em>.<br>DOI: <a href="https://doi.org/10.18653/v1/N16-1174">10.18653&#x2F;v1&#x2F;N16-1174</a></li></ul><hr><h2 id="Other-resources"><a href="#Other-resources" class="headerlink" title="Other resources"></a>Other resources</h2><ol><li><a href="https://www.nice.com/info/top-sentiment-analysis-tools-and-techniques">https://www.nice.com/info/top-sentiment-analysis-tools-and-techniques</a> </li><li>Article: Opinion Mining and Sentiment Analysis. January 2008. Foundations and Trends® in Information Retrieval 2(1–2):1-135<ul><li>DOI:10.1561&#x2F;1500000011</li><li><a href="https://www.cs.cornell.edu/home/llee/omsa/omsa.pdf">Read on cs.cornell.edu</a></li></ul></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;&lt;em&gt;related to : &lt;a href=&quot;/hexo-blog/a</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="AI" scheme="https://ooge0.github.io/hexo-blog/tags/AI/"/>
    
    <category term="NLP" scheme="https://ooge0.github.io/hexo-blog/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>NLP in pictures</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/21/post_ai_nlp__nlp_in_pictures_1/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/21/post_ai_nlp__nlp_in_pictures_1/</id>
    <published>2024-11-21T17:21:11.000Z</published>
    <updated>2024-11-21T17:52:01.459Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="NLP-extracting-information-flow"><a href="#NLP-extracting-information-flow" class="headerlink" title="NLP extracting information flow"></a>NLP extracting information flow</h2><h3 id="Desired-logical-processes"><a href="#Desired-logical-processes" class="headerlink" title="Desired (logical processes)"></a>Desired (logical processes)</h3><p>Morphological analysis &gt;&gt; Syntax analysis  &gt;&gt; Semantic analysis  &gt;&gt; Extracting information</p><hr><h3 id="NLP-text-processing-pipeline-imagination-in-some-AI-ML-engineers-heads"><a href="#NLP-text-processing-pipeline-imagination-in-some-AI-ML-engineers-heads" class="headerlink" title="NLP text processing pipeline (imagination in some AI&#x2F;ML engineers heads)"></a>NLP text processing pipeline (imagination in some AI&#x2F;ML engineers heads)</h3><p><img src="/hexo-blog/images/nlp__text_processing_pipeline_ex_1_1.png" alt="NLP text processing pipeline" title="NLP text processing pipeline" style="width: 80%; max-width: 600px; border: 1px solid #ccc; padding: 5px; border-radius: 8px;" /></p></p><p><img src="/hexo-blog/images/nlp__text_processing_pipeline_ex_1.png" alt="NLP text processing pipeline - 2" title="NLP text processing pipeline - 2" style="width: 80%; max-width: 600px; border: 1px solid #ccc; padding: 5px; border-radius: 8px;" /></p></p><hr><h3 id="Megaputer-representation"><a href="#Megaputer-representation" class="headerlink" title="Megaputer representation"></a>Megaputer representation</h3><p>Megaputer </p><ul><li><a href="https://youtu.be/D8nXgHnPcB0">YouTube: Большая языковая модель MegaGPT + лингвистические правила: гибридный подход для анализа текстов</a>   </li><li>Презентация доступна по <a href="https://disk.yandex.ru/i/IVwBA2Oa2vzyCg">ссылке.</a>  </li><li><a href="https://www.megaputer.ru/">Сайт Мегапьютер</a>  </li><li><a href="https://www.megaputer.ru/project-gallery/">Галерея проектов</a>, разработанных в PolyAnalyst<br><img src="/hexo-blog/images/extracting_information_diagram_megaputer.png" alt="Megaputer diagramm-1" title="Megaputer diagramm-1" style="width: 80%; max-width: 600px; border: 1px solid #ccc; padding: 5px; border-radius: 8px;" /></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h2 id=&quot;NLP-extracting-information-flow&quot;&gt;</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="AI" scheme="https://ooge0.github.io/hexo-blog/tags/AI/"/>
    
    <category term="ML" scheme="https://ooge0.github.io/hexo-blog/tags/ML/"/>
    
    <category term="NLP" scheme="https://ooge0.github.io/hexo-blog/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Linguistic basics</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/21/post_linquistic_basics/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/21/post_linquistic_basics/</id>
    <published>2024-11-21T17:21:11.000Z</published>
    <updated>2024-12-12T11:34:23.233Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="1-Linguistic-Domains-Involved"><a href="#1-Linguistic-Domains-Involved" class="headerlink" title="1. Linguistic Domains Involved"></a>1. Linguistic Domains Involved</h2><ul><li><strong>Syntax</strong>: Deals with sentence structure and grammatical correctness, which is crucial for text quality.</li><li><strong>Semantics</strong>: Ensures that the meaning of words and sentences aligns logically, contributing to coherence and quality.</li><li><strong>Pragmatics</strong>: Focuses on the context and the intended meaning, which impacts both coherence and quality.</li><li><strong>Discourse Analysis</strong>: Examines how sentences and paragraphs are connected to form a meaningful and coherent whole, a critical aspect of text coherence.</li></ul><h2 id="2-Theoretical-Linguistics"><a href="#2-Theoretical-Linguistics" class="headerlink" title="2. Theoretical Linguistics"></a>2. Theoretical Linguistics</h2><p>In theoretical linguistics, these concepts relate to:</p><ul><li><strong>Text Linguistics</strong>: Studies how text is structured to make it meaningful and coherent as a whole.</li><li><strong>Cohesion and Coherence</strong>: Examines linguistic devices (e.g., conjunctions, pronouns, references) and logical connections between text elements.</li><li><strong>Cognitive Linguistics</strong>: Explores how readers perceive and interpret generated text, contributing to the perception of coherence and quality.</li></ul><h2 id="3-Applied-Linguistics"><a href="#3-Applied-Linguistics" class="headerlink" title="3. Applied Linguistics"></a>3. Applied Linguistics</h2><p>In applied contexts, text coherence and quality are central to:</p><ul><li><strong>Natural Language Processing (NLP)</strong>: Ensuring generated text by machines aligns with human linguistic expectations.</li><li><strong>Language Teaching</strong>: Teaching coherent and high-quality writing skills.</li><li><strong>Translation Studies</strong>: Maintaining coherence and quality when translating between languages.</li></ul><h2 id="4-Computational-Linguistics"><a href="#4-Computational-Linguistics" class="headerlink" title="4. Computational Linguistics"></a>4. Computational Linguistics</h2><p>Text generation and evaluation are significant in computational linguistics, where algorithms are developed to:</p><ul><li>Mimic human-like coherence and quality in machine-generated text.</li><li>Optimize coherence through logical flow and topic consistency.</li><li>Enhance quality with grammatically accurate and stylistically appropriate output.</li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><p>Paper: <em>Cohesion and Coherence in Text: An Introduction</em> by Teun A. van Dijk (1997)</p><ul><li>DOI: <a href="https://doi.org/10.1016/j.pragma.2010.04.028">10.1016&#x2F;j.pragma.2010.04.028</a></li><li><a href="https://sci-hub.se/https://doi.org/10.1016/j.pragma.2010.04.028">Read <em>Cohesion and Coherence in Text: An Introduction</em> by Teun A. van Dijk (1997) on sci-hub.se</a></li></ul></li><li><p>Paper: <em>Cognitive Linguistics and Second Language Learning</em> by Peter Robinson (2007)</p><ul><li>DOI: <a href="https://doi.org/10.1017/CBO9781139524780">10.1017&#x2F;CBO9781139524780</a></li><li><a href="https://xyz.com/">Read <em>Cognitive Linguistics and Second Language Learning</em> by Peter Robinson (2007) on XYZ</a></li></ul></li><li><p>Book: <em>Speech and Language Processing</em> by Daniel Jurafsky &amp; James H. Martin (2020)</p><ul><li>DOI: <a href="https://doi.org/">XYZ</a></li><li><a href="https://www.ipwatchdog.com/wp-content/uploads/2019/08/Jurafsky.pdf">Read <em>Speech and Language Processing</em> by Daniel Jurafsky &amp; James H. Martin (2020) on ipwatchdog.com</a></li></ul></li><li><p>Paper: <em>Text Cohesion and Coherence in the Age of NLP</em> by R. S. Goh (2019)</p><ul><li>DOI: <a href="https://doi.org/">XYZ</a></li><li><a href="https://xyz.com/">Read <em>Text Cohesion and Coherence in the Age of NLP</em> by R. S. Goh (2019) on XYZ</a></li></ul></li><li><p>Paper: <em>Coherence in natural language: Data structures and applications</em> by Florian Wolf (2000)</p><ul><li>DOI: <a href="https://doi.org/">XYZ</a></li><li><a href="https://core.ac.uk/download/pdf/4395395.pdf">Read <em>Coherence in natural language: Data structures and applications</em> by Florian Wolf (2000) on</a></li></ul></li><li><p>Paper: <em>Natural Language Processing and Automated Text Categorization</em> by Alessandro Moschitti (2003)</p><ul><li>DOI: <a href="https://doi.org/">XYZ</a></li><li><a href="https://disi.unitn.it/~moschitti/books/PHD-Thesis2003.pdf">Read <em>Natural Language Processing and Automated Text Categorization</em> by Alessandro Moschitti (2003) on</a></li></ul></li><li><p>Book: <em>The Cambridge Handbook of Computational Linguistics</em> by R. Dale, H. Moisl &amp; H. Somers (2003)</p><ul><li>DOI: <a href="https://doi.org/10.1017/9781108755610">10.1017&#x2F;9781108755610</a></li><li><a href="https://xyz.com/">Read <em>The Cambridge Handbook of Computational Linguistics</em> by R. Dale, H. Moisl &amp; H. Somers (2003) on XYZ</a></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h2 id=&quot;1-Linguistic-Domains-Involved&quot;&gt;&lt;a</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="NLP" scheme="https://ooge0.github.io/hexo-blog/tags/NLP/"/>
    
    <category term="linguistic" scheme="https://ooge0.github.io/hexo-blog/tags/linguistic/"/>
    
  </entry>
  
  <entry>
    <title>AI - Machine Learning + emotions</title>
    <link href="https://ooge0.github.io/hexo-blog/2024/11/21/post_ai_ml__emotions/"/>
    <id>https://ooge0.github.io/hexo-blog/2024/11/21/post_ai_ml__emotions/</id>
    <published>2024-11-21T00:08:12.000Z</published>
    <updated>2024-11-21T22:00:11.516Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><ol><li><p>Paper: <em>InstructERC: Reforming Emotion Recognition in Conversation with a Multi-task Retrieval-based LLMs Framework</em></p><ul><li>DOI:10.48550&#x2F;arXiv.2406.18088</li><li><a href="https://openreview.net/pdf/2c4c4157e9917665bbf110ffb8a87c3eba3ebed4.pdf">Read on openreview.net</a></li></ul></li><li><p>Paper: <em>InstructERC: Reforming Emotion Recognition in Conversation with Multi-task Retrieval-Augmented Large Language Models, 2024</em></p><ul><li>DOI: 10.48550&#x2F;arXiv.2309.11911</li><li><a href="https://arxiv.org/pdf/2309.11911">Read on arxiv.org</a></li></ul></li></ol><hr><h2 id="Other-resources-papers-books"><a href="#Other-resources-papers-books" class="headerlink" title="Other resources&#x2F;papers&#x2F;books"></a>Other resources&#x2F;papers&#x2F;books</h2><ol><li><p>Book: <em>Zinker J. Creative process in gestalt therapy. Vintage books. Random House, 1978</em></p></li><li><p>Book: <em>The Expressions of the Emotions in Man and Animals. Darwin. 1872</em></p><ul><li><a href="https://darwin-online.org.uk/content/frameset?viewtype=text&itemID=F1142&pageseq=1">Read book on darwin-online.org.uk</a></li><li><a href="https://archive.org/details/expressionofemot1872darw/">✰✰✰✰✰ Read book on archive.org </a></li><li><a href="https://web.seducoahuila.gob.mx/biblioweb/upload/the_expression_of_the_emotions_in_man_and_animals.pdf">✰ Read book on web.seducoahuila.gob.mx</a></li></ul></li><li><p>Paper: PyPlutchik: Visualising and comparing emotion-annotated corpora. 2021.</p><ul><li>DOI:10.1371&#x2F;journal.pone.0256503</li><li><a href="https://www.researchgate.net/publication/354295491_PyPlutchik_Visualising_and_comparing_emotion-annotated_corpora">✰✰✰✰✰ Read on researchgate.net</a><img src="/hexo-blog/images/plutchik_s_wheel_of_emotions.jpg" alt="Plutchik's wheel of emotions" title="Plutchik's wheel of emotions" href = "https://en.wikipedia.org/wiki/Robert_Plutchik#/media/File:Plutchik-wheel.svg" style="width: 20%; max-width: 1000px; border: 1px solid #ccc; padding: 5px; border-radius: 8px;" /></li></ul></li><li><p>Paper: <em>The Feeling Wheel.Willcox,G.(1982).Transactional Analysis Journal, 12(4), 274–276.</em></p><ul><li>DOI:10.1177&#x2F;036215378201200411</li><li><a href="https://sci-hub.se/10.1177/036215378201200411">Read on sc-hub.se</a><img src="/hexo-blog/images/feeling_wheel_willcox_g_1982.jpg" alt="The Feeling Wheel.Willcox,G.(1982).Transactional Analysis Journal, 12(4), 274–276." title="The Feeling Wheel.Willcox,G.(1982).Transactional Analysis Journal, 12(4), 274–276." style="width: 40%; max-width: 1300px; border: 1px solid #ccc; padding: 5px; border-radius: 8px;" /></li></ul> <img src="/hexo-blog/images/feeling_wheel_willcox_g_1982_ru.jpg" alt="The Feeling Wheel.Willcox,G _ru" title="The Feeling Wheel.Willcox,G _ru" style="width: 20%; max-width: 1000px; border: 1px solid #ccc; padding: 5px; border-radius: 8px;" /></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Paper: &lt;em&gt;InstructERC: Refor</summary>
      
    
    
    
    <category term="Posts" scheme="https://ooge0.github.io/hexo-blog/categories/Posts/"/>
    
    
    <category term="AI" scheme="https://ooge0.github.io/hexo-blog/tags/AI/"/>
    
    <category term="ML" scheme="https://ooge0.github.io/hexo-blog/tags/ML/"/>
    
    <category term="emotions" scheme="https://ooge0.github.io/hexo-blog/tags/emotions/"/>
    
  </entry>
  
</feed>
